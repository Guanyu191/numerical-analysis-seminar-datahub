# 10-3-微分矩阵 (Differentiation matrices)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 从有限差分到矩阵算子**

在 **5-4-有限差分** 中，我们用有限差分把一组离散的函数值变成某个点处导数的估计. 类似于微积分里 "求导是把一个函数变成另一个函数" 的操作，我们也可以把 "某个点的差分公式" 推广成一个把离散函数映射到离散导数的操作.

**#2 有限差分的一阶导数矩阵**

先把区间 $[a,b]$ 等分成长度为 $h=(b-a)/n$ 的小段，得到节点

$$
x_i=a+ih,\qquad i=0,\dots,n.
$$

我们仍然从 0 开始编号. 目标是找到一个向量 $\mathbf{g}$ 使得对每个 $i=0,\dots,n$ 都有 $g_i\approx f'(x_i)$. 最直接的尝试是前向差分：对 $i=0,\dots,n-1$，

$$
g_i=\frac{f_{i+1}-f_i}{h},
$$

其中 $f_i=f(x_i)$. 但这会让 $g_n$ 无法定义，因为它会用到不存在的 $f_{n+1}$. 对 $g_n$，我们可以改用后向差分

$$
g_n=\frac{f_n-f_{n-1}}{h}.
$$

把所有节点上的函数值写成向量

$$
\mathbf{f}=
\begin{bmatrix}
f(x_0)\\
f(x_1)\\
\vdots\\
f(x_{n-1})\\
f(x_n)
\end{bmatrix},
$$

则上面的差分公式可以汇总成一个向量方程

$$
\begin{bmatrix}
f'(x_0)\\
f'(x_1)\\
\vdots\\
f'(x_{n-1})\\
f'(x_n)
\end{bmatrix}
\approx
\mathbf{D}_x\mathbf{f},
\qquad
\mathbf{D}_x=\frac{1}{h}
\begin{bmatrix}
-1 & 1\\
& -1 & 1\\
& & \ddots & \ddots\\
& & & -1 & 1\\
& & & -1 & 1
\end{bmatrix}.
$$

这里没有写出来的矩阵元素都是 0. 我们把 $\mathbf{D}_x$ 称为微分矩阵 (**differentiation matrix**). $\mathbf{D}_x$ 的每一行对应一个节点处的有限差分公式权重.

微分矩阵并不唯一，因为每一行都可以选用不同的差分公式. 但直觉上我们希望每一行尽量相似. 如果在内部节点使用二阶中心差分，在边界节点使用二阶单边差分，就得到二阶精度的一阶导数微分矩阵

$$
\mathbf{D}_x=\frac{1}{h}
\begin{bmatrix}
-\frac{3}{2} & 2 & -\frac{1}{2}\\
-\frac{1}{2} & 0 & \frac{1}{2}\\
& \ddots & \ddots & \ddots\\
& & -\frac{1}{2} & 0 & \frac{1}{2}\\
& & \frac{1}{2} & -2 & \frac{3}{2}
\end{bmatrix}.
$$

到目前为止，这些微分矩阵都是带状矩阵 (**banded matrix**)：非零元素只分布在主对角线附近的少数几条对角线上.

**#3 二阶导数矩阵**

同样地，我们也可以为二阶导数定义微分矩阵. 例如

$$
\begin{bmatrix}
f''(x_0)\\
f''(x_1)\\
f''(x_2)\\
\vdots\\
f''(x_{n-1})\\
f''(x_n)
\end{bmatrix}
\approx
\mathbf{D}_{xx}\mathbf{f},
\qquad
\mathbf{D}_{xx}=\frac{1}{h^2}
\begin{bmatrix}
2 & -5 & 4 & -1\\
1 & -2 & 1\\
& 1 & -2 & 1\\
& & \ddots & \ddots & \ddots\\
& & & 1 & -2 & 1\\
& & -1 & 4 & -5 & 2
\end{bmatrix}.
$$

和一阶导数一样，$\mathbf{D}_{xx}$ 的选择也不唯一，而且不必是某个 $\mathbf{D}_x$ 的平方. 把一阶导数矩阵平方当然是可行的，但会把非零元素放到离主对角线更远的位置，在很多场景里这并不必要.

**#4 二阶精度微分矩阵的实现与收敛**

> **Demo:** Second-order differentiation matrices
> We implement a routine that returns the nodes, the first-derivative matrix, and the second-derivative matrix, all with second-order accuracy.
>
> ```Python
> import numpy as np
>
> def diffmat2(n, xspan):
>     a, b = xspan
>     h = (b - a) / n
>     x = a + h * np.arange(n + 1)
>
>     D1 = np.zeros((n + 1, n + 1))
>     for i in range(1, n):
>         D1[i, i - 1] = -0.5 / h
>         D1[i, i + 1] = 0.5 / h
>     D1[0, 0:3] = np.array([-1.5, 2.0, -0.5]) / h
>     D1[n, n - 2 : n + 1] = np.array([0.5, -2.0, 1.5]) / h
>
>     D2 = np.zeros((n + 1, n + 1))
>     for i in range(1, n):
>         D2[i, i - 1] = 1.0 / h**2
>         D2[i, i] = -2.0 / h**2
>         D2[i, i + 1] = 1.0 / h**2
>     D2[0, 0:4] = np.array([2.0, -5.0, 4.0, -1.0]) / h**2
>     D2[n, n - 3 : n + 1] = np.array([-1.0, 4.0, -5.0, 2.0]) / h**2
> 
>     return x, D1, D2
> ```

> **Demo:** Testing finite differences on a small grid
> We apply the matrices to $f(x)=x+\exp(\sin(4x))$ over $[-1,1]$ and compare the discrete derivatives to the exact ones. The results show poor accuracy for this small value of $n$.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> f = lambda x: x + np.exp(np.sin(4 * x))
> dfdx = lambda x: 1 + 4 * np.exp(np.sin(4 * x)) * np.cos(4 * x)
> d2fdx2 = lambda x: 4 * np.exp(np.sin(4 * x)) * (4 * np.cos(4 * x) ** 2 - 4 * np.sin(4 * x))
>
> x, D1, D2 = diffmat2(18, (-1.0, 1.0))
> y = f(x)
> yx = D1 @ y
> yxx = D2 @ y
>
> fig, ax = plt.subplots(1, 2, figsize=(10, 4))
> xx = np.linspace(-1.0, 1.0, 600)
>
> ax[0].plot(xx, dfdx(xx), label="exact")
> ax[0].plot(x, yx, "o", ms=3, label="finite difference")
> ax[0].set_title("First derivative")
> ax[0].grid(True, alpha=0.3)
> ax[0].legend()
>
> ax[1].plot(xx, d2fdx2(xx), label="exact")
> ax[1].plot(x, yxx, "o", ms=3, label="finite difference")
> ax[1].set_title("Second derivative")
> ax[1].grid(True, alpha=0.3)
> ax[1].legend()
>
> plt.suptitle("Finite differences with n=18")
> plt.show()
> ```

> **Demo:** A finite-difference convergence experiment
> We test the matrices on $f(x)=x+\\exp(\\sin(4x))$ over $[-1,1]$, and estimate the convergence rate by log-log slopes of the maximum errors.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> f = lambda x: x + np.exp(np.sin(4 * x))
> dfdx = lambda x: 1 + 4 * np.exp(np.sin(4 * x)) * np.cos(4 * x)
> d2fdx2 = lambda x: 4 * np.exp(np.sin(4 * x)) * (4 * np.cos(4 * x) ** 2 - 4 * np.sin(4 * x))
>
> ns = np.array([20, 40, 80, 160, 320, 640])
> err1 = []
> err2 = []
> for n in ns:
>     x, D1, D2 = diffmat2(n, (-1.0, 1.0))
>     y = f(x)
>     err1.append(np.max(np.abs(dfdx(x) - D1 @ y)))
>     err2.append(np.max(np.abs(d2fdx2(x) - D2 @ y)))
>
> err1 = np.array(err1)
> err2 = np.array(err2)
> p1 = np.polyfit(np.log(ns), np.log(err1), 1)[0]
> p2 = np.polyfit(np.log(ns), np.log(err2), 1)[0]
> print("slope for f'  ", p1)
> print("slope for f'' ", p2)
>
> plt.loglog(ns, err1, "o-", label=r"max error of $f'$")
> plt.loglog(ns, err2, "o-", label=r"max error of $f''$")
>
> ref = err1[0] * (ns / ns[0]) ** (-2)
> plt.loglog(ns, ref, "--", color="gray", label="2nd order reference")
>
> plt.xlabel("n")
> plt.ylabel("max error")
> plt.title("Convergence of finite differences")
> plt.grid(True, which="both", alpha=0.3)
> plt.legend()
> plt.show()
> ```
>
> The errors decay roughly like $O(n^{-2})$, consistent with second-order accuracy.

**#5 谱微分与 Chebyshev 微分矩阵**

回忆有限差分公式的一种推导思路，可以分成三步：

1. 选定一个包含节点 $i$ 的索引集合 $S$.
2. 用 $S$ 中的节点做多项式插值.
3. 对插值多项式求导，并在节点 $i$ 处取值.

我们可以把这三步改成 "全局版本"：用全局插值 (多项式或三角插值) 来替代局部插值，从而每个节点都用全部节点一次性构造导数近似. 这类思路在第 9 章已经出现过. 在非周期情形里，为了稳定性，我们使用 Chebyshev 第二类点：

$$
x_k=-\cos\left(\frac{k\pi}{n}\right),\qquad k=0,\dots,n.
$$

对应的一阶导数 Chebyshev 微分矩阵是稠密矩阵 (**dense matrix**). 它的元素可以写成

> **Definition:** Chebyshev differentiation matrix
> Let $x_k=-\\cos(k\\pi/n)$ for $k=0,\\dots,n$. The Chebyshev differentiation matrix has entries
> $$
> D_{00} = -\\frac{2n^2+1}{6},\\qquad D_{nn}=\\frac{2n^2+1}{6},
> $$
> $$
> D_{ii} = -\\frac{x_i}{2(1-x_i^2)},\\quad i=j,\\quad i=1,\\dots,n-1,
> $$
> and for $i\\ne j$,
> $$
> D_{ij} = \\frac{c_i}{c_j}\\frac{(-1)^{i+j}}{x_i-x_j},
> $$
> where $c_0=c_n=2$ and $c_j=1$ for $j=1,\\dots,n-1$.

> **Note:** 这里的节点按 $x_0=-1$ 到 $x_n=1$ 的顺序编号 (即 $x_k=-\\cos(k\\pi/n)$). 因此端点对角元的符号与一些按 $x_k=\\cos(k\\pi/n)$ 编号的写法相反. 一个快速自检是：对常数向量 $\\mathbf{1}$ 应有 $\\mathbf{D}_x\\mathbf{1}=\\mathbf{0}$，对节点向量 $\\mathbf{x}=[x_0,\\dots,x_n]^{T}$ 应有 $\\mathbf{D}_x\\mathbf{x}=\\mathbf{1}$.

当我们已经有了 $\mathbf{D}_x$，一个最简单的二阶导数近似就是取平方 $\mathbf{D}_{xx}=\mathbf{D}_x^2$. 因为这里的矩阵本来就是稠密的，所以不再需要担心 "带宽" 的问题.

**#6 Chebyshev 微分矩阵的实现与谱收敛**

> **Demo:** Chebyshev differentiation matrices
> We compute $\mathbf{D}_x$ by the off-diagonal formula and then set diagonal entries so each row sums to zero (the "negative sum trick"). We also transplant the nodes from $[-1,1]$ to $[a,b]$ and apply the chain rule.
>
> ```Python
> import numpy as np
>
> def diffcheb(n, xspan):
>     k = np.arange(n + 1)
>     x = -np.cos(np.pi * k / n)  # nodes in [-1,1]
>
>     c = np.ones(n + 1)
>     c[0] = 2.0
>     c[-1] = 2.0
>     c = c * ((-1) ** k)  # endpoint factors and alternating signs
>
>     dX = x[:, None] - x[None, :]
>     D = (c[:, None] / c[None, :]) / (dX + np.eye(n + 1))
>     D = D - np.diag(D.sum(axis=1))  # diagonal entries
>
>     a, b = xspan
>     x = a + (b - a) * (x + 1.0) / 2.0
>     D1 = 2.0 * D / (b - a)  # chain rule
>     D2 = D1 @ D1
>     return x, D1, D2
> ```

> **Demo:** A 4x4 Chebyshev differentiation matrix
> ```Python
> x, D1, _ = diffcheb(3, (-1.0, 1.0))
> np.set_printoptions(precision=5, suppress=True)
> print(D1)
> ```

> **Demo:** Spectral convergence of Chebyshev derivatives
> We repeat the derivative experiment for $f(x)=x+\\exp(\\sin(4x))$. Since we expect a spectral convergence rate, we use a semi-log plot of the errors versus $n$.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> f = lambda x: x + np.exp(np.sin(4 * x))
> dfdx = lambda x: 1 + 4 * np.exp(np.sin(4 * x)) * np.cos(4 * x)
> d2fdx2 = lambda x: 4 * np.exp(np.sin(4 * x)) * (4 * np.cos(4 * x) ** 2 - 4 * np.sin(4 * x))
>
> ns = np.arange(5, 71, 5)
> err1 = []
> err2 = []
> for n in ns:
>     x, D1, D2 = diffcheb(n, (-1.0, 1.0))
>     y = f(x)
>     err1.append(np.max(np.abs(dfdx(x) - D1 @ y)))
>     err2.append(np.max(np.abs(d2fdx2(x) - D2 @ y)))
>
> plt.semilogy(ns, err1, "o-", label=r"max error of $f'$")
> plt.semilogy(ns, err2, "o-", label=r"max error of $f''$")
> plt.xlabel("n")
> plt.ylabel("max error")
> plt.title("Convergence of Chebyshev derivatives")
> plt.grid(True, alpha=0.3)
> plt.legend()
> plt.show()
> ```
>
> The errors decay rapidly until they reach a floor dominated by rounding.

前面在第 9 章我们已经见过：对解析函数，用 Chebyshev 节点做多项式插值可以获得谱收敛. 这里的现象与之对应：当插值逼近以谱速度收敛时，由插值导出的导数近似也会以相同的 "谱精度" 收敛.

> **Note:** 如果我们想利用带状结构高效计算有限差分的微分矩阵，通常需要把矩阵构造成 `scipy.sparse` 的稀疏格式，或使用三对角/五对角等专门结构来避免存储与乘法的 $O(n^2)$ 成本.
