# 3-3-QR 分解 (The QR factorization)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 正交、正交归一与 "避免消去"**

满足某种性质的向量集合，在理论与计算上都很有用.

> **Definition:** Orthogonal vectors
> Two vectors $\mathbf{u}$ and $\mathbf{v}$ in $\mathbb{R}^n$ are **orthogonal** if $\mathbf{u}^T\mathbf{v}=0$. We say that a collection of vectors $\mathbf{q}_1,\ldots,\mathbf{q}_k$ is orthogonal if
> $$
> i \neq j \quad \Rightarrow \quad \mathbf{q}_i^T\mathbf{q}_j = 0.
> $$
> If the condition above applies and also $\mathbf{q}_i^T\mathbf{q}_i=1$ for all $i=1,\ldots,k$, we say the vectors are **orthonormal**.

在二维和三维中，正交等价于 "互相垂直".

正交向量会显著简化内积的展开. 例如，若 $\mathbf{q}_1$ 与 $\mathbf{q}_2$ 正交，则

$$
\|\mathbf{q}_1-\mathbf{q}_2\|_2^2
=
(\mathbf{q}_1-\mathbf{q}_2)^T(\mathbf{q}_1-\mathbf{q}_2)
=
\mathbf{q}_1^T\mathbf{q}_1 - 2\mathbf{q}_1^T\mathbf{q}_2 + \mathbf{q}_2^T\mathbf{q}_2
=
\|\mathbf{q}_1\|_2^2 + \|\mathbf{q}_2\|_2^2.
$$

和本章其他部分一致，我们只使用 2-范数.

上面的恒等式是正交性在计算上很有用的关键：若 $\mathbf{q}_1,\mathbf{q}_2$ 正交，则同理可得
$$
\|\mathbf{q}_1\pm\mathbf{q}_2\|_2^2=\|\mathbf{q}_1\|_2^2+\|\mathbf{q}_2\|_2^2,
$$
因此向量的和或差的大小不会比原向量更小. 相比之下，即使 $\|\mathbf{x}\|$ 与 $\|\mathbf{y}\|$ 都很大，$\|\mathbf{x}-\mathbf{y}\|$ 也可能因为非正交而出现多维版本的 "相减消去".

> **Observation:** Addition and subtraction of vectors are guaranteed to be well conditioned when the vectors are orthogonal.

> **Note:** 这里的 "well conditioned" 指的是：向量加减的输出不会因为输入方向的特殊对齐而出现不成比例的相对放大.

**#2 正交矩阵、ONC 矩阵与 2-范数不变性**

关于正交向量的结论，用矩阵形式写出来通常更简洁. 令 $\mathbf{Q}$ 为 $n\times k$ 矩阵，其列向量 $\mathbf{q}_1,\ldots,\mathbf{q}_k$ 两两正交. 此时，"两两正交" 等价于 $\mathbf{Q}^T\mathbf{Q}$ 为对角矩阵，因为

$$
\mathbf{Q}^T \mathbf{Q}
=
\begin{bmatrix}
\mathbf{q}_1^T \\[1mm] \mathbf{q}_2^T \\ \vdots \\ \mathbf{q}_k^T
\end{bmatrix}
\begin{bmatrix}
\mathbf{q}_1 & \mathbf{q}_2 & \cdots & \mathbf{q}_k
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{q}_1^T\mathbf{q}_1 & \mathbf{q}_1^T\mathbf{q}_2 & \cdots & \mathbf{q}_1^T\mathbf{q}_k \\[1mm]
\mathbf{q}_2^T\mathbf{q}_1 & \mathbf{q}_2^T\mathbf{q}_2 & \cdots & \mathbf{q}_2^T\mathbf{q}_k \\
\vdots & \vdots & & \vdots \\
\mathbf{q}_k^T\mathbf{q}_1 & \mathbf{q}_k^T\mathbf{q}_2 & \cdots & \mathbf{q}_k^T\mathbf{q}_k
\end{bmatrix}.
$$

若列向量还是正交归一的，那么 $\mathbf{Q}^T\mathbf{Q}$ 就是 $k\times k$ 的单位矩阵. 这个性质非常重要，因此我们给它一个名字.

> **Definition:** ONC matrix
> An **ONC matrix** is one whose columns are an orthonormal set of vectors.

> **Theorem:** ONC matrix
> Suppose $\mathbf{Q}$ is a real $n\times k$ ONC matrix (matrix with orthonormal columns). Then:
> 1. $\mathbf{Q}^T\mathbf{Q} = \mathbf{I}$ ($k\times k$ identity).
> 2. $\| \mathbf{Q}\mathbf{x} \|_2 = \| \mathbf{x} \|_2$ for all $k$-vectors $\mathbf{x}$.
> 3. $\| \mathbf{Q} \|_2=1$.
>
> **Proof:** The first part is derived above. The second part follows a pattern that has become well established by now:
> $$
> \| \mathbf{Q}\mathbf{x} \|_2^2
> = (\mathbf{Q}\mathbf{x})^T(\mathbf{Q}\mathbf{x})
> = \mathbf{x}^T \mathbf{Q}^T \mathbf{Q} \mathbf{x}
> = \mathbf{x}^T \mathbf{I} \mathbf{x}
> = \| \mathbf{x} \|_2^2.
> $$
> The last part of the theorem is left to the exercises.

特别值得关注的是 "方阵" 的 ONC 矩阵.

> **Definition:** Orthogonal matrix
> An **orthogonal matrix** is a square matrix with orthonormal columns.

> **Note:** 容易混淆的一点是：如果一个方阵的列向量 "两两正交但未归一"，它不满足 $\mathbf{Q}^T\mathbf{Q}=\mathbf{I}$，因此不叫 orthogonal matrix. orthogonal matrix 的条件更强：列向量必须正交归一.

正交矩阵的性质比一般 ONC 矩阵更强.

> **Theorem:** Orthogonal matrix
> Suppose $\mathbf{Q}$ is an $n\times n$ real orthogonal matrix. Then:
> 1. $\mathbf{Q}^T = \mathbf{Q}^{-1}$.
> 2. $\mathbf{Q}^T$ is also an orthogonal matrix.
> 3. $\kappa(\mathbf{Q})=1$ in the 2-norm.
> 4. For any other $n\times n$ matrix $\mathbf{A}$, $\| \mathbf{A}\mathbf{Q} \|_2=\| \mathbf{A} \|_2$.
> 5. If $\mathbf{U}$ is another $n\times n$ orthogonal matrix, then $\mathbf{Q}\mathbf{U}$ is also orthogonal.
>
> **Proof:** Since $\mathbf{Q}$ is an ONC matrix, $\mathbf{Q}^T\mathbf{Q}=\mathbf{I}$. All three matrices are $n\times n$, so $\mathbf{Q}^{-1}=\mathbf{Q}^T$. The proofs of the other statements are left to the exercises.

**#3 QR 分解与薄 QR 分解**

现在我们引入一种重要的矩阵分解：**QR factorization**. 我们会看到，QR 分解在 "线性最小二乘" 中扮演的角色，类似于 LU 分解在线性方程组中扮演的角色.

> **Theorem:** QR factorization
> Every real $m\times n$ matrix $\mathbf{A}$ ($m\ge n$) can be written as $\mathbf{A}=\mathbf{Q}\mathbf{R}$, where $\mathbf{Q}$ is an $m\times m$ orthogonal matrix and $\mathbf{R}$ is an $m\times n$ upper triangular matrix.

在很多线性代数教材中，QR 分解会通过 **Gram-Schmidt orthogonalization** 推导出来. 但 Gram-Schmidt 在数值上是不稳定的. 在后面的 **3-4-计算 QR 分解** 里，我们会用另一种构造来替代它.

当 $m$ 远大于 $n$ 时，QR 分解可以进一步压缩成更高效的形式. 在乘积

$$
\mathbf{A}
=
\begin{bmatrix}
\mathbf{q}_1 & \mathbf{q}_2 & \cdots & \mathbf{q}_m
\end{bmatrix}
\begin{bmatrix}
r_{11} & r_{12} & \cdots & r_{1n} \\
0 & r_{22} & \cdots & r_{2n} \\
\vdots &  & \ddots & \vdots\\
0 & 0 & \cdots & r_{nn} \\
0 & 0 & \cdots & 0 \\
\vdots & \vdots &  & \vdots \\
0 & 0 & \cdots & 0
\end{bmatrix}
$$

里，$\mathbf{q}_{n+1},\ldots,\mathbf{q}_m$ 总是被乘上 0. 如果删掉它们，$\mathbf{A}$ 的信息不会丢失，因此我们得到等价的分解

$$
\mathbf{A}
=
\begin{bmatrix}
\mathbf{q}_1 & \mathbf{q}_2 & \cdots & \mathbf{q}_n
\end{bmatrix}
\begin{bmatrix}
r_{11} & r_{12} & \cdots & r_{1n} \\
0 & r_{22} & \cdots & r_{2n} \\
\vdots &  & \ddots & \vdots\\
0 & 0 & \cdots & r_{nn}
\end{bmatrix}
=
\hat{\mathbf{Q}}\,\hat{\mathbf{R}}.
$$

> **Definition:** Thin QR factorization
> The thin QR factorization is $\mathbf{A} = \hat{\mathbf{Q}} \hat{\mathbf{R}}$, where $\hat{\mathbf{Q}}$ is $m\times n$ and ONC, and $\hat{\mathbf{R}}$ is $n\times n$ and upper triangular.

> **Demo:** Computing full and thin QR in NumPy.
> ```Python
> import numpy as np
>
> rng = np.random.default_rng(0)
> A = rng.integers(1, 10, size=(6, 4)).astype(float)
>
> # Full QR: Q is m x m, R is m x n.
> Q, R = np.linalg.qr(A, mode="complete")
> print(Q.shape, R.shape)
> print(np.linalg.norm(A - Q @ R))
> print(np.linalg.norm(Q.T @ Q - np.eye(Q.shape[1])))
>
> # Thin QR: Qhat is m x n, Rhat is n x n.
> Qhat, Rhat = np.linalg.qr(A, mode="reduced")
> print(Qhat.shape, Rhat.shape)
> print(np.linalg.norm(A - Qhat @ Rhat))
> print(np.linalg.norm(Qhat.T @ Qhat - np.eye(Rhat.shape[0])))
> ```
> In exact arithmetic, both residual norms are zero, and both `Q.T@Q` and `Qhat.T@Qhat` equal the identity.

> **Note:** 在 Python / NumPy 里，`np.linalg.qr` 可以直接返回 full 或 thin 形式. 这里的 "thin" 也常被称为 "economy-size" QR.

**#4 用薄 QR 分解解线性最小二乘**

把薄 QR 分解代入 **3-2-正规方程** 的正规方程 $(\mathbf{A}^T\mathbf{A})\mathbf{x}=\mathbf{A}^T\mathbf{b}$，可以大幅简化表达式：

$$
\begin{aligned}
\mathbf{A}^T\mathbf{A}\mathbf{x} &= \mathbf{A}^T\mathbf{b},\\
\hat{\mathbf{R}}^T\hat{\mathbf{Q}}^T\hat{\mathbf{Q}}\hat{\mathbf{R}}\mathbf{x} &= \hat{\mathbf{R}}^T\hat{\mathbf{Q}}^T\mathbf{b},\\
\hat{\mathbf{R}}^T\hat{\mathbf{R}}\mathbf{x} &= \hat{\mathbf{R}}^T\hat{\mathbf{Q}}^T\mathbf{b}.
\end{aligned}
$$

为了让正规方程 "良定"，我们要求 $\mathbf{A}$ 不秩亏. **3-2-正规方程** 已经说明：这等价于 $\mathbf{A}$ 的列向量线性无关. 在该条件下，$\hat{\mathbf{R}}$ 可逆，因此也能消去上式左端的 $\hat{\mathbf{R}}^T$，得到

$$
\hat{\mathbf{R}}\mathbf{x}=\hat{\mathbf{Q}}^T\mathbf{b}.
$$

> **Algorithm:** Solution of linear least squares by thin QR
> 1. Compute the thin QR factorization $\hat{\mathbf{Q}}\hat{\mathbf{R}}=\mathbf{A}$.
> 2. Compute $\mathbf{z} = \hat{\mathbf{Q}}^T\mathbf{b}$.
> 3. Solve the $n\times n$ linear system $\hat{\mathbf{R}}\mathbf{x} = \mathbf{z}$ for $\mathbf{x}$.

上面算法可以写成一个很短的函数.

> **Function:** lsqrfact
> **Linear least-squares solution by QR factorization**
> ```Python
> import numpy as np
>
> def lsqrfact(A, b):
>     """
>     Solve a linear least-squares problem by QR factorization.
>     Returns the minimizer of ||b - A x||_2.
>     """
>     Q, R = np.linalg.qr(A, mode="reduced")   # A = Q R, with Q ONC and R upper triangular
>     z = Q.T @ b
>     x = np.linalg.solve(R, z)
>     return x
> ```

用 QR 分解来解最小二乘，通常比直接构造并求解正规方程更稳定.

> **Demo:** QR is stable on an ill-conditioned least-squares problem.
> ```Python
> import numpy as np
>
> t = np.linspace(0.0, 3.0, 400)
> A = np.column_stack([
>     np.sin(t) ** 2,
>     np.cos((1.0 + 1e-7) * t) ** 2,
>     np.ones_like(t),
> ])
>
> x_true = np.array([1.0, 2.0, 1.0])
> b = A @ x_true  # exact residual is zero
>
> kappa = np.linalg.cond(A)
> x_qr = lsqrfact(A, b)
> relerr = np.linalg.norm(x_qr - x_true) / np.linalg.norm(x_true)
>
> print("cond(A) =", kappa)
> print("relative error (QR) =", relerr)
> print("kappa * eps =", kappa * np.finfo(float).eps)
> ```
> Typically, the QR-based solution has relative error on the order of `kappa * eps`.

> **Note:** 这类实验里，误差通常只有在 `kappa * eps` 的量级以内才算 "符合预期". 如果误差明显更大，常见原因是 `A` 进一步变得更病态，或问题本身出现了秩亏.
