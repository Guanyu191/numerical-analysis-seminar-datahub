# 3-2-正规方程 (The normal equations)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 正规方程与最小二乘解**

我们现在要解 **3-1-用函数拟合数据** 里提出的线性最小二乘问题：给定 $\mathbf{A}\in\mathbb{R}^{m\times n}$ 与 $\mathbf{b}\in\mathbb{R}^{m}$ (且 $m>n$)，找到 $\mathbf{x}\in\mathbb{R}^{n}$ 使得

$$
\|\mathbf{b}-\mathbf{A}\mathbf{x}\|_2
$$

尽可能小.

这里有一个简洁的刻画. 在证明里，我们会用到下面这个代数恒等式：对任意向量 $\mathbf{u},\mathbf{v}$，

$$
\begin{aligned}
(\mathbf{u}+\mathbf{v})^{T}(\mathbf{u}+\mathbf{v})
&=\mathbf{u}^{T}\mathbf{u}+\mathbf{u}^{T}\mathbf{v}+\mathbf{v}^{T}\mathbf{u}+\mathbf{v}^{T}\mathbf{v}\\
&=\mathbf{u}^{T}\mathbf{u}+2\mathbf{v}^{T}\mathbf{u}+\mathbf{v}^{T}\mathbf{v}.
\end{aligned}
$$

> **Theorem:** If $\mathbf{x}$ satisfies $\mathbf{A}^{T}(\mathbf{A}\mathbf{x}-\mathbf{b})=\mathbf{0}$, then $\mathbf{x}$ solves the linear least-squares problem, i.e., minimizes $\|\mathbf{b}-\mathbf{A}\mathbf{x}\|_2$.
>
> **Proof:** Let $\mathbf{z}$ be any vector in $\mathbb{R}^{n}$. Write
> $$
> \mathbf{b}-\mathbf{A}\mathbf{z}
> =
> (\mathbf{b}-\mathbf{A}\mathbf{x})
> +
> \mathbf{A}(\mathbf{x}-\mathbf{z}).
> $$
> Using the identity above with $\mathbf{u}=\mathbf{b}-\mathbf{A}\mathbf{x}$ and $\mathbf{v}=\mathbf{A}(\mathbf{x}-\mathbf{z})$, we obtain
> $$
> \begin{aligned}
> \|\mathbf{b}-\mathbf{A}\mathbf{z}\|_2^2
> &=
> \|\mathbf{b}-\mathbf{A}\mathbf{x}\|_2^2
> +2(\mathbf{x}-\mathbf{z})^{T}\mathbf{A}^{T}(\mathbf{b}-\mathbf{A}\mathbf{x})
> +\|\mathbf{A}(\mathbf{x}-\mathbf{z})\|_2^2\\
> &=
> \|\mathbf{b}-\mathbf{A}\mathbf{x}\|_2^2
> +\|\mathbf{A}(\mathbf{x}-\mathbf{z})\|_2^2\\
> &\ge
> \|\mathbf{b}-\mathbf{A}\mathbf{x}\|_2^2,
> \end{aligned}
> $$
> because $\mathbf{A}^{T}(\mathbf{A}\mathbf{x}-\mathbf{b})=\mathbf{0}$ implies $\mathbf{A}^{T}(\mathbf{b}-\mathbf{A}\mathbf{x})=\mathbf{0}$. Hence $\mathbf{x}$ is a minimizer.

上面的条件 $\mathbf{A}^{T}(\mathbf{A}\mathbf{x}-\mathbf{b})=\mathbf{0}$，就是所谓的 **normal equations**.

> **Definition:** Normal equations
> Given $\mathbf{A}\in\mathbb{R}^{m\times n}$ and $\mathbf{b}\in\mathbb{R}^{m}$, the normal equations for the linear least-squares problem are
> $$
> \mathbf{A}^{T}(\mathbf{A}\mathbf{x}-\mathbf{b})=\mathbf{0},
> $$
> or equivalently,
> $$
> (\mathbf{A}^{T}\mathbf{A})\mathbf{x}=\mathbf{A}^{T}\mathbf{b}.
> $$

**#2 几何解释**

正规方程有一个很直观的几何解释. $\mathbf{A}\mathbf{x}$ 一定落在 $\mathbf{A}$ 的列空间里. 在所有这种向量里，离 $\mathbf{b}$ 最近的那个，会让残差向量 $\mathbf{A}\mathbf{x}-\mathbf{b}$ 与列空间正交.

因此，对任意 $\mathbf{z}$，我们都应有

$$
(\mathbf{A}\mathbf{z})^{T}(\mathbf{A}\mathbf{x}-\mathbf{b})=0.
$$

把 $\mathbf{z}$ 换成标准基向量，就能得到等价的矩阵形式：$\mathbf{A}^{T}(\mathbf{A}\mathbf{x}-\mathbf{b})=\mathbf{0}$.

**#3 伪逆与 $\mathbf{A}^{T}\mathbf{A}$ 的性质**

把正规方程写成

$$
(\mathbf{A}^{T}\mathbf{A})\mathbf{x}=\mathbf{A}^{T}\mathbf{b},
$$

我们就把过定问题转成了一个 $n\times n$ 的方阵线性系统.

当 $\mathbf{A}$ 的列向量线性无关时，$\mathbf{A}^{T}\mathbf{A}$ 可逆. 这时我们可以把解写成一个更紧凑的形式.

> **Definition:** Pseudoinverse
> Let $\mathbf{A}\in\mathbb{R}^{m\times n}$ with $m>n$. If $\mathbf{A}^{T}\mathbf{A}$ is nonsingular, then the pseudoinverse of $\mathbf{A}$ is the $n\times m$ matrix
> $$
> \mathbf{A}^{+}=(\mathbf{A}^{T}\mathbf{A})^{-1}\mathbf{A}^{T}.
> $$

用这个记号，线性最小二乘问题 $\mathbf{A}\mathbf{x}\approx\mathbf{b}$ 的解可以写成

$$
\mathbf{x}=\mathbf{A}^{+}\mathbf{b}.
$$

从计算角度看，我们很少会直接去算 $\mathbf{A}^{+}$ (就像我们很少直接去算矩阵逆一样). 在 Python 里也可以用 `np.linalg.pinv` 直接计算伪逆，但和显式求逆一样，这在实践中很少必要. 更常见的做法是把正规方程当作线性系统来解.

矩阵 $\mathbf{A}^{T}\mathbf{A}$ 还有一些重要的结构性质.

> **Theorem:** For any real $m\times n$ matrix $\mathbf{A}$ with $m\ge n$, the following are true.
> 1. $\mathbf{A}^{T}\mathbf{A}$ is symmetric.
> 2. $\mathbf{A}^{T}\mathbf{A}$ is singular if and only if the columns of $\mathbf{A}$ are linearly dependent. (Equivalently, if and only if the rank of $\mathbf{A}$ is less than $n$.)
> 3. If $\mathbf{A}^{T}\mathbf{A}$ is nonsingular, then it is positive definite.

**#4 实现：用正规方程解最小二乘**

直接根据正规方程求解最小二乘问题的步骤可以概括为：

> **Algorithm:** Solution of linear least squares by the normal equations
> 1. Compute $\mathbf{N}=\mathbf{A}^{T}\mathbf{A}$.
> 2. Compute $\mathbf{z}=\mathbf{A}^{T}\mathbf{b}$.
> 3. Solve the $n\times n$ linear system $\mathbf{N}\mathbf{x}=\mathbf{z}$ for $\mathbf{x}$.

在这一步里，我们可以利用 **2-9-利用矩阵结构** 里的 Cholesky 分解：当 $\mathbf{N}$ 对称正定时，用 Cholesky 分解来解线性系统会更合适.

> **Demo:** Solving least squares via the normal equations (Cholesky).
> ```Python
> import numpy as np
>
> def lsnormal(A, b):
>     # Solve min ||b - A x||_2 via the normal equations.
>     N = A.T @ A
>     z = A.T @ b
>
>     # Cholesky: N = R^T R, where R is upper triangular.
>     R = np.linalg.cholesky(N).T
>
>     # Solve R^T w = z, then R x = w.
>     w = np.linalg.solve(R.T, z)
>     x = np.linalg.solve(R, w)
>     return x
> ```
> This approach is simple, but it can be unstable when `A` is ill-conditioned (see the next section).

这一算法的主要计算量来自上面第 1 步和第 3 步，总 flop 数大约为

$$
mn^2+\frac{1}{3}n^3.
$$

**#5 条件数与稳定性**

在实际计算中，我们经常直接用线性最小二乘求解器 (例如 Python 的 `np.linalg.lstsq`). 这类求解器通常不会通过正规方程来计算解，因为正规方程可能不稳定.

线性最小二乘问题的条件性，刻画的是解 $\mathbf{x}$ 对数据 $\mathbf{A},\mathbf{b}$ 的敏感性. 为了讨论条件数，我们先把 "矩阵条件数" 的定义推广到非方阵.

> **Definition:** Matrix condition number (rectangular case)
> Let $\mathbf{A}\in\mathbb{R}^{m\times n}$ with $m>n$. Define
> $$
> \kappa(\mathbf{A})=\|\mathbf{A}\|_2\cdot\|\mathbf{A}^{+}\|_2.
> $$
> If the rank of $\mathbf{A}$ is less than $n$ (i.e., if it has linearly dependent columns), then $\kappa(\mathbf{A})=\infty$.

如果最小残差 $\|\mathbf{b}-\mathbf{A}\mathbf{x}\|_2$ 相对较小，那么线性最小二乘问题的条件性会接近 $\kappa(\mathbf{A})$.

但作为算法，正规方程会先构造 $n\times n$ 的线性系统 $(\mathbf{A}^{T}\mathbf{A})\mathbf{x}=\mathbf{A}^{T}\mathbf{b}$. 当我们解这个系统时，数据扰动会被放大到大约 $\kappa(\mathbf{A}^{T}\mathbf{A})$ 的量级.

下面的恒等式说明：正规方程会把条件数平方化，这正是它不稳定的根源.

> **Theorem:** Condition number in the normal equations
> Let $\mathbf{A}\in\mathbb{R}^{m\times n}$ with $m>n$. Then
> $$
> \kappa(\mathbf{A}^{T}\mathbf{A})=\kappa(\mathbf{A})^2.
> $$

当 $\kappa(\mathbf{A})$ 已经很大时，$\kappa(\mathbf{A})^2$ 往往会把正规方程推到不可接受的误差放大水平.

> **Demo:** Normal equations can lose accuracy for ill-conditioned `A`.
> ```Python
> import numpy as np
>
> t = np.linspace(0, 3, 400)
> A = np.column_stack([
>     np.sin(t) ** 2,
>     np.cos((1 + 1e-7) * t) ** 2,
>     np.ones_like(t),
> ])
>
> kappa = np.linalg.cond(A)
> print("cond(A) =", kappa)
>
> x = np.array([1.0, 2.0, 1.0])
> b = A @ x  # exact residual is zero
>
> # A stable least-squares solve.
> x_bs, *_ = np.linalg.lstsq(A, b, rcond=None)
> observed_error = np.linalg.norm(x_bs - x) / np.linalg.norm(x)
> error_bound = kappa * np.finfo(float).eps
> print("observed_error (lstsq) =", observed_error)
> print("kappa * eps =", error_bound)
>
> # Solve via normal equations.
> N = A.T @ A
> x_ne = np.linalg.solve(N, A.T @ b)
> observed_err = np.linalg.norm(x_ne - x) / np.linalg.norm(x)
> digits = -np.log10(observed_err)
> print("observed_error (normal eq) =", observed_err)
> print("digits =", digits)
> ```
> A typical outcome is that `lstsq` achieves an error well below `kappa * eps`, while solving the normal equations can lose most digits (the printed `digits` is often around 1-2 here).
