# 9-4-正交多项式 (Orthogonal polynomials)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 从数据拟合到函数拟合**

插值不是用多项式做全局逼近的唯一方式. 在 **3-1-用函数拟合数据** 中，我们用线性最小二乘问题找过 "最贴近数据" 的多项式. 这个想法也可以推广到 "拟合一个连续函数".

> **Demo:** Least-squares fit of $\exp(x)$
> We approximate $\exp(x)$ on $[-1,1]$ by a best-fitting straight line, using least squares on sampled points, and observe that the fitted coefficients approach a limit as we increase the number of samples.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def fit_line(n):
>     t = np.linspace(-1.0, 1.0, n)
>     y = np.exp(t)
>     V = np.column_stack([np.ones_like(t), t])  # [1, x]
>     c, *_ = np.linalg.lstsq(V, y, rcond=None)
>     return c  # intercept, slope
>
> # Compare fits from different sample sizes.
> c20 = fit_line(20)
> c200 = fit_line(200)
>
> xs = np.linspace(-1.0, 1.0, 400)
> plt.plot(xs, np.exp(xs), label="function")
> plt.plot(xs, c20[0] + c20[1] * xs, label="linear fit for 20 points")
> plt.plot(xs, c200[0] + c200[1] * xs, label="linear fit for 200 points")
> plt.title("Least-squares fit of exp(x)")
> plt.xlabel("x")
> plt.ylabel("value")
> plt.grid(True, alpha=0.3)
> plt.legend()
> plt.show()
>
> # Track the fitted coefficients as the number of samples increases.
> ns = list(range(40, 401, 60))
> rows = []
> for n in ns:
>     c = fit_line(n)
>     rows.append((n, c[0], c[1]))
>
> print("n   intercept    slope")
> for n, a, b in rows:
>     print(f"{n:<3d} {a:>10.5f} {b:>10.5f}")
> ```
>
> The fitted line changes little once the number of sample points is large, suggesting a limiting continuous least-squares fit.

**#2 函数的内积与 2-范数**

我们把从有限维向量空间里熟悉的定义，推广到函数空间. 离散求和的连续版本是积分，因此自然会得到下面的定义.

> **Definition:** Inner product of functions
> Let $S$ be the set of continuous real-valued functions on the interval $[-1,1]$. The inner product of any functions $f$ and $g$ in $S$ is the real scalar
> $$
> \langle f,g\rangle=\int_{-1}^{1} f(x)g(x)\,dx.
> $$
> With this inner product, $S$ is an inner product space. The $2$-norm of a function $f\in S$ is
> $$
> \|f\|_2=\sqrt{\langle f,f\rangle}.
> $$
> Functions $f$ and $g$ in $S$ are orthogonal if $\langle f,g\rangle=0$.

**#3 准矩阵 (Quasimatrices) 与 Gram 矩阵**

如果我们把函数当作 "向量"，那么什么对象扮演 "矩阵" 的角色？矩阵最核心的用途之一是描述线性组合. 在 **3-1-用函数拟合数据** 中，Vandermonde 型系统 $\mathbf{V}\mathbf{c}\approx \mathbf{y}$ 可以理解为：用基函数 $1,x,\dots,x^n$ 的线性组合去逼近数据.

对函数拟合，我们同样希望把 "一组函数的线性组合" 写成类似矩阵-向量乘法的形式. 这会导向 **准矩阵 (quasimatrix)** 的概念.

> **Definition:** Quasimatrix and Gram matrix
> Given functions $f_1,\dots,f_n$ in inner product space $S$, define the quasimatrix
> $$
> \mathbf{F}=[f_1,\ f_2,\ \dots,\ f_n].
> $$
> For a vector $\mathbf{z}\in\mathbb{R}^n$, define the quasimatrix-vector product
> $$
> \mathbf{F}\mathbf{z}=z_1 f_1(x)+z_2 f_2(x)+\cdots+z_n f_n(x).
> $$
> For another function $g\in S$, define the adjoint product
> $$
> \mathbf{F}^{T}g=
> \begin{bmatrix}
> \langle f_1,g\rangle\\
> \vdots\\
> \langle f_n,g\rangle
> \end{bmatrix}.
> $$
> Finally, define the Gram matrix
> $$
> \mathbf{F}^{T}\mathbf{F}=\bigl[\langle f_i,f_j\rangle\bigr]_{i,j=1,\dots,n}.
> $$

我们把其他涉及准矩阵的表达式都视为未定义. 一个有用的直觉是：把 $\mathbf{F}$ 当作一个 "$\infty\times n$ 的矩阵" 来想. 这样 $\mathbf{F}\mathbf{z}$ 是一个函数 (可类比为 $\infty\times 1$ )，$\mathbf{F}^{T}g$ 是一个向量 ($n\times 1$)，$\mathbf{F}^{T}\mathbf{F}$ 是一个矩阵 ($n\times n$). 当 "无限维" 参与到乘法里时，求和会被积分替代.

> **Example:** A simple quasimatrix
> Let $\mathbf{F}=[\cos(\pi x)\ \ \sin(\pi x)]$. Then
> $$
> \mathbf{F}
> \begin{bmatrix}
> -2\\
> 1
> \end{bmatrix}
> =-2\cos(\pi x)+\sin(\pi x),
> $$
> $$
> \mathbf{F}^{T}x=
> \begin{bmatrix}
> \int_{-1}^{1} x\cos(\pi x)\,dx\\
> \int_{-1}^{1} x\sin(\pi x)\,dx
> \end{bmatrix}
> =
> \begin{bmatrix}
> 0\\
> 2/\pi
> \end{bmatrix},
> $$
> and
> $$
> \mathbf{F}^{T}\mathbf{F}=
> \begin{bmatrix}
> \int_{-1}^{1}\cos^2(\pi x)\,dx & \int_{-1}^{1}\cos(\pi x)\sin(\pi x)\,dx\\
> \int_{-1}^{1}\cos(\pi x)\sin(\pi x)\,dx & \int_{-1}^{1}\sin^2(\pi x)\,dx
> \end{bmatrix}
> =
> \mathbf{I}.
> $$

**#4 函数最小二乘与正规方程**

离散的线性最小二乘问题可以用 **3-2-正规方程** 来刻画. 对准矩阵，完全类似的结论成立.

> **Theorem:** Normal equations (continuous form)
> Given functions $f_1,\dots,f_n$ and $y$ in an inner product space $S$, the least-squares problem
> $$
> \operatorname*{argmin}_{\mathbf{c}\in\mathbb{R}^n}\ \|\mathbf{F}\mathbf{c}-y\|_2
> $$
> has the solution
> $$
> \mathbf{c}=(\mathbf{F}^{T}\mathbf{F})^{-1}\mathbf{F}^{T}y,
> $$
> where $\mathbf{F}$ is the quasimatrix $[f_1,\dots,f_n]$.

> **Note:** 这里不需要重复证明，因为证明过程和离散正规方程完全一致：核心只用到了线性组合与内积的性质.

> **Example:** Revisiting the $\exp(x)$ fit via integrals
> Using the Vandermonde quasimatrix $\mathbf{V}=[1\ \ x]$, we have
> $$
> \mathbf{V}^{T}e^{x}=
> \begin{bmatrix}
> \langle 1,e^{x}\rangle\\
> \langle x,e^{x}\rangle
> \end{bmatrix}
> =
> \begin{bmatrix}
> \int_{-1}^{1} e^{x}\,dx\\
> \int_{-1}^{1} x e^{x}\,dx
> \end{bmatrix}
> =
> \begin{bmatrix}
> e-e^{-1}\\
> 2e^{-1}
> \end{bmatrix},
> $$
> and
> $$
> \mathbf{V}^{T}\mathbf{V}=
> \begin{bmatrix}
> \langle 1,1\rangle & \langle 1,x\rangle\\
> \langle x,1\rangle & \langle x,x\rangle
> \end{bmatrix}
> =
> \begin{bmatrix}
> 2 & 0\\
> 0 & 2/3
> \end{bmatrix}.
> $$
> Therefore
> $$
> \mathbf{c}=
> \begin{bmatrix}
> 2 & 0\\
> 0 & 2/3
> \end{bmatrix}^{-1}
> \begin{bmatrix}
> e-e^{-1}\\
> 2e^{-1}
> \end{bmatrix}
> =
> \begin{bmatrix}
> \sinh(1)\\
> 3e^{-1}
> \end{bmatrix}
> \approx
> \begin{bmatrix}
> 1.175201\\
> 1.103638
> \end{bmatrix}.
> $$
>
> If we extend $\mathbf{V}$ by an additional column $x^2$, then we need
> $$
> \int_{-1}^{1} x^2 e^{x}\,dx = e-5e^{-1},
> \qquad
> \int_{-1}^{1} x^4\,dx = 2/5,
> $$
> which leads to a quadratic best fit with coefficients
> $$
> \mathbf{c}\approx
> \begin{bmatrix}
> 0.9962940\\
> 1.103638\\
> 0.5367215
> \end{bmatrix}.

**#5 Legendre 多项式**

如果 $\mathbf{F}^{T}\mathbf{F}$ 是对角矩阵，那么正规方程会大幅简化. 这等价于列函数彼此正交. 对单项式 $x^j$ 这并不成立，但确实存在满足正交性的多项式族.

在后面，我们把 $\mathcal{P}_n$ 记作次数不超过 $n$ 的多项式集合.

> **Definition:** Legendre polynomials
> The Legendre polynomials are
> $$
> P_0(x)=1,\qquad
> P_1(x)=x,\qquad
> P_k(x)=\frac{2k-1}{k}xP_{k-1}(x)-\frac{k-1}{k}P_{k-2}(x),
> \quad k=2,3,\dots
> $$

> **Theorem:** Properties of Legendre polynomials
> 1. The degree of $P_k$ is $k$.
> 2. $P_0,\dots,P_n$ form a basis for $\mathcal{P}_n$.
> 3. The Legendre polynomials are mutually orthogonal. More specifically, the Gram matrix is given by
> $$
> \langle P_i,P_j\rangle =
> \begin{cases}
> 0, & i\ne j,\\
> \alpha_i^2=(i+\tfrac{1}{2})^{-1}, & i=j.
> \end{cases}
> $$

现在定义准矩阵

$$
\mathbf{L}_n(x)=\bigl[\alpha_0^{-1}P_0\ \ \alpha_1^{-1}P_1\ \ \cdots\ \ \alpha_n^{-1}P_n\bigr].
$$

于是 $\mathbf{L}_n^{T}\mathbf{L}_n=\mathbf{I}$，正规方程就会相应简化. 把定义展开，我们得到最小二乘意义下的多项式逼近：

$$
\mathbf{L}_n(\mathbf{L}_n^{T}f)=\sum_{k=0}^{n} c_k P_k(x),
\qquad
c_k=\frac{1}{\alpha_k^2}\langle P_k,f\rangle.
$$

**#6 Chebyshev 多项式**

内积并不一定要使用常数权重. 我们可以更一般地定义

$$
\langle f,g\rangle=\int_{-1}^{1} f(x)g(x)w(x)\,dx,
$$

其中 $w(x)>0$ 称为 weight function (权重函数). 一个重要特例是 Chebyshev 权重：

$$
\langle f,g\rangle=\int_{-1}^{1} \frac{f(x)g(x)}{\sqrt{1-x^2}}\,dx.
$$

> **Definition:** Chebyshev polynomials
> The Chebyshev polynomials are defined by
> $$
> T_0(x)=1,\qquad
> T_1(x)=x,\qquad
> T_k(x)=2xT_{k-1}(x)-T_{k-2}(x),\quad k=2,3,\dots
> $$
> Chebyshev polynomials also have a starting alternative form,
> $$
> T_k(x)=\cos(k\theta),\qquad \theta=\arccos(x).
> $$

Legendre 的那些性质在 Chebyshev 情形下也成立，只是正交性是针对上面的 Chebyshev 加权内积. 其 Gram 矩阵是

$$
\langle T_i,T_j\rangle=
\begin{cases}
0, & i\ne j,\\
\gamma_0^2=\pi, & i=j=0,\\
\gamma_i^2=\pi/2, & i=j>0.
\end{cases}
$$

因此，Chebyshev 最小二乘解与 Legendre 最小二乘解并不相同：它们都是在找某种范数意义下 "最近的逼近"，但使用的范数不同.

**#7 正交多项式的根，以及最小二乘与插值的对比**

只用正交性条件，就能推出许多有趣性质. 下面这个结果在后面的谱方法里会用到.

> **Theorem:** Roots of Legendre polynomials
> All $n$ roots of the Legendre polynomial $P_n(x)$ are simple and real, and they lie in the open interval $(-1,1)$.

这个结论对其他权重函数下的正交多项式族也成立. Chebyshev 情形的一个特殊之处是：由于 $T_k(x)=\cos(k\arccos x)$，$T_n$ 的根可以显式写出：

$$
t_k=\cos\left(\frac{2k-1}{2n}\pi\right),
\qquad k=1,\dots,n.
$$

它们称为第一类 Chebyshev 点 (Chebyshev points of the first kind). 第一类与第二类的主要区别是：第二类点包含端点 $\pm 1$，而第一类点不包含. 这两类点都适合做多项式插值，并能给出谱收敛.

最后总结一下最小二乘与插值的差异：两者都可以看作把函数投影到多项式空间 $\mathcal{P}_n$ 上.

- 最小二乘与内积、正交性紧密相连，因此用 2-范数 (可能带权) 来分析很自然. 常数权重是最简单的情形，所以 Legendre 多项式经常用在最小二乘里.
- 插值没有那么直接的内积或 2-范数解释. 对插值，更合适的分析往往用到复平面，并且 max-范数是自然的选择. 在这种语境下，Chebyshev 多项式通常最方便使用.

