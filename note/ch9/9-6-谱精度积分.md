# 9-6-谱精度积分 (Spectrally accurate integration)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 回到通用求积公式**

在 **5-6-数值积分** 中，我们推导过二阶、四阶及更高阶的数值积分方法. 它们都可以写成一个统一形式：

$$
\int_{-1}^{1} f(x)\,dx \approx \sum_{k=0}^{n} w_k f(t_k),
$$

其中 $t_0,\dots,t_n\in[-1,1]$ 是节点，$w_0,\dots,w_n$ 是权重. (本节统一以 $[-1,1]$ 作为积分区间；一般区间 $[a,b]$ 可以通过变量代换处理.) 节点与权重不依赖于被积函数 $f$，它们决定了公式的实现方式与性质.

推导某个具体方法的套路是：先插值 $f$，再对插值多项式积分. 例如在等距节点上做分段线性插值，会得到梯形公式. 当我们用具有谱精度的全局逼近去近似 $f$ 时，相应的积分公式也会是谱精度的.

**#2 周期函数与梯形公式的谱精度**

对于定义在 $[-1,1]$ 上的周期函数，一个最自然的插值函数是 **9-5-三角插值** 中的三角多项式. 利用三角基函数的性质，可以得到

$$
\int_{-1}^{1} \sum_{k=-n}^{n} y_k \tau_k(x)\,dx
=
\sum_{k=-n}^{n} y_k\left[\int_{-1}^{1}\tau_k(x)\,dx\right]
=
\frac{2}{2n+1}\sum_{k=-n}^{n} y_k.
$$

也就是说：对周期函数，梯形求积公式是谱精度的.

> **Observation:** Trapezoid rule is spectrally accurate for periodic functions
> The trapezoid integration formula is spectrally accurate for periodic functions.

> **Demo:** Perimeter of an ellipse by the trapezoid rule
> We compute the perimeter of an ellipse with semi-axes 1 and 1/2 using a periodic arc-length integrand and the trapezoid rule.
>
> ```Python
> import numpy as np
>
> f = lambda t: np.pi * np.sqrt(np.cos(np.pi * t)**2 + 0.25 * np.sin(np.pi * t)**2)
>
> for n in range(4, 49, 4):
>     h = 2.0 / n
>     t = h * np.arange(0, n) - 1.0
>     S = h * np.sum(f(t))
>     print(f"n = {n:2d}, value = {S:.15f}")
> ```
>
> The approximations typically gain about one digit of accuracy for each constant increment of `n`, consistent with spectral convergence.

**#3 Clenshaw-Curtis 积分**

当 $f$ 光滑但不周期时，我们可以在 Chebyshev 第二类点上做全局多项式插值，然后把插值多项式积分. 由此得到的方法称为 Clenshaw-Curtis 积分.

Clenshaw-Curtis 的节点是

$$
t_k=-\cos\left(\frac{k\pi}{n}\right),
\qquad k=0,\dots,n,
$$

与通用求积公式一致. 关键是如何找到权重. 对 Lagrange 形式的插值多项式

$$
p(x)=\sum_{k=0}^{n} f(t_k)\ell_k(x),
$$

积分后有

$$
I=\int_{-1}^{1} f(x)\,dx
\approx
\int_{-1}^{1} p(x)\,dx
=
\sum_{k=0}^{n} f(t_k)\int_{-1}^{1}\ell_k(x)\,dx
=
\sum_{k=0}^{n} w_k f(t_k),
$$

其中

$$
w_k=\int_{-1}^{1}\ell_k(x)\,dx.
$$

对 $n$ 为偶数时，权重可以写成显式公式：

$$
w_k=
\begin{cases}
\dfrac{1}{n^2-1}, & k=0 \text{ or } k=n,\\[6pt]
\dfrac{4}{n}\displaystyle\sum_{j=0}^{n/2}\frac{\cos(2\pi jk/n)}{\gamma_j(1-4j^2)}, & k=1,\dots,n-1,
\end{cases}
$$

其中

$$
\gamma_j=
\begin{cases}
2, & j=0 \text{ or } j=n/2,\\
1, & j=1,2,\dots,n/2-1.
\end{cases}
$$

对奇数 $n$ 有不同公式. 另外需要注意：权重也依赖于 $n$，例如 $n=4$ 时的 $w_2$ 与 $n=10$ 时的 $w_2$ 不是同一个数. 但我们从始至终都不需要显式构造插值多项式本身.

> **Demo:** Clenshaw-Curtis integration (even n)
> We implement the even-`n` Clenshaw-Curtis weights and compute the integral by a vector inner product.
>
> ```Python
> import numpy as np
>
> def ccint(f, n):
>     if n % 2 != 0:
>         raise ValueError("n must be even.")
>     theta = np.pi * np.arange(0, n + 1) / n
>     x = -np.cos(theta)  # Chebyshev second-kind points
>
>     w = np.empty(n + 1, dtype=float)
>     w[0] = 1.0 / (n * n - 1.0)
>     w[-1] = w[0]
>
>     # Interior weights (vectorized form of the cosine-series formula).
>     j = np.arange(1, n)
>     theta_j = theta[j]
>     s = np.zeros_like(theta_j)
>     for k in range(1, n // 2):
>         s += np.cos(2.0 * k * theta_j) / (4.0 * k * k - 1.0)
>     v = 1.0 - 2.0 * s - np.cos(n * theta_j) / (n * n - 1.0)
>     w[j] = 2.0 * v / n
>
>     I = np.dot(w, f(x))
>     return I, x, w
> ```
>
> This code mirrors the structure of the textbook function: precompute nodes/weights, then take a dot product with sampled values.

**#4 Gauss-Legendre 积分**

现在重新看通用公式

$$
\int_{-1}^{1} f(x)\,dx \approx \sum_{k=1}^{n} w_k f(t_k)=:Q_n[f],
$$

这里把求和下标从 $k=1$ 开始只是为了后续记号方便.

插值推导促使我们选择 Chebyshev 节点，但对于 "求积分" 这个任务来说，它们未必是最理想的节点. 因此我们改用一个最优性准则来选节点与权重.

> **Definition:** Degree of an integration formula
> The degree of integration formula $Q_n$ is the maximum value of $d$ such that
> $$
> Q_n[p]=\int_{-1}^{1} p(x)\,dx
> $$
> for all $p\in \mathcal{P}_d$.

直觉上，有 $n$ 个节点与 $n$ 个权重可选，我们或许能达到 $d=2n-1$. 这个直觉确实是正确的：我们想找到节点 $t_k$ 与权重 $w_k$，使得对任意 $p\in\mathcal{P}_{2n-1}$ 都有

$$
\int_{-1}^{1} p(x)\,dx = Q_n[p]=\sum_{k=1}^{n} w_k p(t_k).
$$

如果这些条件满足，方法称为 Gauss-Legendre 积分 (简称 Gauss 积分). 因为求积公式是线性的，只要它对单项式 $1,x,x^2,\dots,x^{2n-1}$ 都精确，就足够了.

> **Example:** The two-point Gaussian formula
> For $n=2$, requiring exactness on monomials up to degree $2n-1=3$ gives
> $$
> \begin{aligned}
> 2 &= w_1+w_2,\\
> 0 &= w_1 t_1 + w_2 t_2,\\
> \frac{2}{3} &= w_1 t_1^2 + w_2 t_2^2,\\
> 0 &= w_1 t_1^3 + w_2 t_2^3.
> \end{aligned}
> $$
> Solving yields
> $$
> w_1=w_2=1,\qquad
> t_1=-\frac{1}{\sqrt{3}},\qquad
> t_2=\frac{1}{\sqrt{3}}.
> $$

把这个过程推广到一般 $n$ 会很快变得难以手算，因为条件是非线性的. 但存在一个更优雅的结论.

> **Theorem:** Gauss nodes are Legendre roots
> The roots of the Legendre polynomial $P_n(x)$ are the nodes of an $n$-point Gaussian integration formula.

由 **9-4-正交多项式** 中关于 Legendre 根的性质可知：这些根两两不同，且都位于 $(-1,1)$ 内，这一点与 "积分只依赖区间内部信息" 的直觉一致. 虽然节点没有显式公式，但我们可以用快速算法计算它们，并按需得到求积权重. 一个经典方法是把节点与权重写成一个三对角矩阵的特征值问题.

> **Demo:** Gauss-Legendre integration via an eigenvalue problem
> We compute nodes and weights from a symmetric tridiagonal matrix and then integrate by a dot product.
>
> ```Python
> import numpy as np
> import numpy.linalg as npla
>
> def glint(f, n):
>     # Nodes and weights from a tridiagonal eigenvalue problem.
>     k = np.arange(1, n)
>     beta = 0.5 / np.sqrt(1.0 - (2.0 * k) ** (-2))
>     T = np.diag(beta, 1) + np.diag(beta, -1)
>     lam, V = npla.eigh(T)
>     p = np.argsort(lam)
>     x = lam[p]                # nodes
>     w = 2.0 * (V[0, p] ** 2)   # weights
>     I = np.dot(w, f(x))
>     return I, x, w
> ```
>
> This method is practical up to moderately large `n` and is a standard way to generate Gauss-Legendre rules.

**#5 两种谱方法的对比**

Clenshaw-Curtis 与 Gauss-Legendre 都是谱精度方法. Clenshaw-Curtis 用 $n+1$ 个点，求积公式的 degree 是 $n$；而 Gauss-Legendre 用 $n$ 个点，degree 是 $2n-1$. 因此，从这个角度看，Gauss-Legendre 可能 "收敛快两倍"，也就是在误差上可能大致表现为 Clenshaw-Curtis 的平方. 但完整的故事并不总是这么简单.

> **Demo:** Spectral integration (CC vs GL)
> We compare the two methods on smooth integrands and plot errors versus number of nodes.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def f1(x):
>     return 1.0 / (1.0 + 4.0 * x * x)
>
> exact1 = np.arctan(2.0)
>
> ms = list(range(5, 102, 4))  # number of nodes
> err_cc = []
> err_gl = []
>
> for m in ms:
>     n_cc = m - 1  # even, so that CC uses n_cc+1 = m nodes
>     Icc, xcc, wcc = ccint(f1, n_cc)
>     Igl, xgl, wgl = glint(f1, m)
>     err_cc.append(abs(Icc - exact1))
>     err_gl.append(abs(Igl - exact1))
>
> plt.semilogy(ms, err_cc, marker="o", label="CC")
> plt.semilogy(ms, err_gl, marker="o", label="GL")
> plt.title("Spectral integration")
> plt.xlabel("number of nodes")
> plt.ylabel("error")
> plt.grid(True, which="both", alpha=0.3)
> plt.legend()
> plt.show()
> ```
>
> Gauss-Legendre often converges faster, but the observed rate can be less than a clean factor of two.

> **Note:** 和 **5-7-自适应积分** 一样，选方法往往取决于问题. 一个经验法则是：误差容忍度较大时，自适应低阶方法可能很合适；当我们追求高精度时，谱方法往往更高效.
