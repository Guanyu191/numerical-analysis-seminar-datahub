# 8-1-稀疏性与结构 (Sparsity and structure)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 稀疏矩阵与结构零**

非常大的矩阵如果不是稀疏的，通常无法完整放进计算机的主存. 稀疏矩阵的关键特征是存在结构零 (structural zeros)：即某些元素我们事先就知道它们严格等于 0. 例如，在 **7-1-从矩阵到洞见** 里，图的邻接矩阵在 "不存在链接" 的位置上就是 0.

为了高效地存储和运算，稀疏矩阵不会被表示成 "把所有元素都存下来" 的二维数组. 常见做法是只记录非零元素的位置与数值. 在很多直觉性的讨论里，我们可以把稀疏矩阵想象成存储了所有非零位置的三元组 $(i,j,A_{ij})$ 的集合.

**#2 稀疏矩阵计算**

多数真实应用中的图并不会接近完全图：对 $n$ 个节点而言，边的数量往往远少于可能的最大值 $n^2$. 因此它们的邻接矩阵大部分元素为 0，应当用稀疏形式存储.

我们可以用 "密度" 来描述一个稀疏矩阵的稀疏程度：密度定义为非零元个数除以总元素个数.

> **Demo:** Sparse storage and sparse matvec
> We compare the storage and speed of sparse vs dense matrix-vector multiplication.
>
> ```Python
> import numpy as np
> import time
> import scipy.sparse as sp
>
> rng = np.random.default_rng(0)
>
> n = 2790
> density = 1.1e-3
>
> # A sparse adjacency-like matrix with 0/1 entries.
> A = sp.random(
>     n, n, density=density, format="csr",
>     data_rvs=lambda k: np.ones(k, dtype=float),
>     random_state=rng,
> )
> A.data[:] = 1.0
> A.eliminate_zeros()
>
> nnz = A.nnz
> dens = nnz / (n * n)
> sparse_bytes = A.data.nbytes + A.indices.nbytes + A.indptr.nbytes
> dense_bytes = n * n * 8  # float64
> print("density =", dens)
> print("sparse bytes (approx) =", sparse_bytes)
> print("dense bytes (theory)  =", dense_bytes)
> print("dense/sparse ~", dense_bytes / sparse_bytes)
>
> x = rng.standard_normal(n)
>
> # Warm up.
> _ = A @ x
>
> t0 = time.perf_counter()
> for _ in range(300):
>     _ = A @ x
> t_sparse = time.perf_counter() - t0
>
> F = A.toarray()
> _ = F @ x
> t0 = time.perf_counter()
> for _ in range(300):
>     _ = F @ x
> t_dense = time.perf_counter() - t0
>
> print("sparse matvec time:", t_sparse)
> print("dense  matvec time:", t_dense)
> ```
>
> The storage savings can be dramatic, and sparse matvec is often much faster because operations with structural zeros are skipped.

当矩阵操作的输入是稀疏矩阵时，很多算术运算 (例如加减乘幂) 会自动尊重并利用稀疏性. 但要注意：某些矩阵运算会显著减少稀疏程度，这种现象称为 **fill-in**.

> **Demo:** Fill-in in powers of an adjacency matrix
> We build a small-world style sparse adjacency matrix and visualize how nonzeros spread in $\mathbf{A}^k$.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
> import scipy.sparse as sp
>
> rng = np.random.default_rng(0)
> n = 200
> k_nei = 4          # each node connects to 2*k_nei neighbors on a ring
> p_rewire = 0.02    # a few long-range links
>
> rows, cols = [], []
> for i in range(n):
>     for d in range(1, k_nei + 1):
>         rows += [i, i]
>         cols += [(i + d) % n, (i - d) % n]
>
> # Random extra directed links.
> extra = int(p_rewire * n * n)
> rows += rng.integers(0, n, size=extra).tolist()
> cols += rng.integers(0, n, size=extra).tolist()
>
> data = np.ones(len(rows), dtype=float)
> A = sp.csr_matrix((data, (rows, cols)), shape=(n, n))
> A.eliminate_zeros()
>
> plt.figure(figsize=(8, 3))
> plt.subplot(1, 3, 1)
> plt.spy(A, markersize=2)
> plt.title("A")
> plt.subplot(1, 3, 2)
> plt.spy(A @ A, markersize=2)
> plt.title("A^2")
> plt.subplot(1, 3, 3)
> plt.spy((A @ A) @ A, markersize=2)
> plt.title("A^3")
> plt.tight_layout()
> plt.show()
> ```
>
> The number of nonzeros typically increases with $k$, illustrating fill-in.

**#3 带状矩阵**

一种特别重要的稀疏矩阵是带状矩阵 (banded matrix). 回忆：如果 $\mathbf{A}$ 的上带宽为 $p$，则当 $j-i>p$ 时必有 $A_{ij}=0$. 如果下带宽为 $q$，则当 $i-j>q$ 时必有 $A_{ij}=0$. 总带宽为 $p+q+1$.

带状矩阵常出现在 "每个元素只与少数邻居直接相互作用" 的问题里. 在不做主元交换 (pivoting) 的情况下，LU 分解会保留带宽结构. 但如果引入选主元，带宽可能被扩展甚至被破坏.

> **Demo:** Bandedness and pivoting
> We construct a banded matrix, compute an LU factorization without pivoting, and compare it with a pivoted LU.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
> import scipy.sparse as sp
> import scipy.linalg as la
>
> n = 50
> diags = [
>     50.0 * np.ones(n - 3),          # offset -3
>     np.ones(n),                      # offset 0
>     -np.arange(1, n, dtype=float),   # offset +1
>     0.1 * np.ones(n - 5),            # offset +5
> ]
> offsets = [-3, 0, 1, 5]
> A = sp.diags(diags, offsets, shape=(n, n), format="csr").toarray()
>
> def lu_nopivot(A):
>     A = A.copy().astype(float)
>     n = A.shape[0]
>     L = np.eye(n)
>     U = np.zeros_like(A)
>     for k in range(n):
>         U[k, k:] = A[k, k:]
>         piv = U[k, k]
>         if abs(piv) < 1e-14:
>             raise ValueError("zero pivot encountered")
>         for i in range(k + 1, n):
>             if A[i, k] != 0.0:
>                 L[i, k] = A[i, k] / piv
>                 A[i, k:] -= L[i, k] * U[k, k:]
>     return L, U
>
> L0, U0 = lu_nopivot(A)
> P, L1, U1 = la.lu(A)  # partial pivoting
>
> plt.figure(figsize=(8, 4))
> plt.subplot(2, 2, 1)
> plt.spy(A, markersize=2)
> plt.title("A")
> plt.subplot(2, 2, 2)
> plt.spy(L0, markersize=2)
> plt.title("L (no pivot)")
> plt.subplot(2, 2, 3)
> plt.spy(U0, markersize=2)
> plt.title("U (no pivot)")
> plt.subplot(2, 2, 4)
> plt.spy(U1, markersize=2)
> plt.title("U (pivoted)")
> plt.tight_layout()
> plt.show()
> ```
>
> Without pivoting, bandedness is largely preserved. Pivoting can introduce extra nonzeros and may destroy the band structure.

**#4 线性系统与特征值**

如果给定一个稀疏矩阵，用于解线性系统的实现通常会自动选择稀疏友好的分解方式 (例如稀疏 Cholesky 或带主元的稀疏 LU). 这会使得求解时间明显低于一般情形的 $O(n^3)$.

对于非常大的矩阵，我们也往往不会尝试求出全部特征值与特征向量. 更常见的需求是：只找少数几个满足某种准则的特征值，例如模长最大的几个，最靠左/最靠右的几个，或距离某个给定复数最近的几个. 后续章节会围绕这种目标展开 (例如 **8-4-Krylov-子空间**).

> **Demo:** Selected eigenvalues and sparse linear solves
> We compute a few extremal eigenvalues of a large sparse symmetric matrix, and compare sparse vs dense linear solves on a moderate size.
>
> ```Python
> import numpy as np
> import time
> import scipy.sparse as sp
> import scipy.sparse.linalg as spla
>
> rng = np.random.default_rng(0)
>
> # Sparse eigenvalues (symmetric).
> n = 2000
> density = 4e-4
> R = sp.random(n, n, density=density, format="csr", random_state=rng)
> A = (R + R.T) * 0.5
> A = A + sp.eye(n)  # shift to make it more likely nonsingular
>
> # Find 5 eigenvalues of largest magnitude.
> lam_max, _ = spla.eigsh(A, k=5, which="LM")
> print("largest-magnitude eigenvalues:", np.sort(lam_max)[::-1])
>
> # Find 5 eigenvalues closest to sigma=1 via shift-invert.
> lam_near, _ = spla.eigsh(A, k=5, sigma=1.0, which="LM")
> print("eigenvalues near 1:", np.sort(lam_near))
>
> # Sparse linear solve vs dense (use a smaller n for dense).
> n2 = 600
> R2 = sp.random(n2, n2, density=1e-3, format="csr", random_state=rng)
> B = (R2 + R2.T) * 0.5 + 2.0 * sp.eye(n2)  # SPD-ish
> x_true = 1.0 / np.arange(1, n2 + 1)
> b = B @ x_true
>
> t0 = time.perf_counter()
> x_sparse = spla.spsolve(B, b)
> t_sparse = time.perf_counter() - t0
>
> Bd = B.toarray()
> t0 = time.perf_counter()
> x_dense = np.linalg.solve(Bd, b)
> t_dense = time.perf_counter() - t0
>
> print("sparse solve time:", t_sparse, "  error:", np.linalg.norm(x_sparse - x_true))
> print("dense  solve time:", t_dense, "  error:", np.linalg.norm(x_dense - x_true))
> ```
>
> The time for sparse solves is not easy to predict from $n$ alone, but it is often orders of magnitude smaller than the dense version when sparsity is effectively exploited.

