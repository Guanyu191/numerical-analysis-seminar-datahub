# 8-3-反迭代 (Inverse iteration)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 从幂迭代到反迭代**

幂迭代只能找到主导特征值. 下面我们展示：只要我们对某个目标特征值有一个足够好的初始估计，就能把幂迭代改造成用来找到 "任意" 特征值. 需要的工具只是一些简单的线性代数.

> **Theorem:** Shifts, inverses, and eigenvalues
> Let $\mathbf{A}$ be an $n\times n$ matrix with eigenvalues $\lambda_1,\dots,\lambda_n$ (possibly with repeats), and let $s$ be a complex scalar. Then:
> 1. The eigenvalues of the matrix $\mathbf{A}-s\mathbf{I}$ are $\lambda_1-s,\dots,\lambda_n-s$.
> 2. If $s$ is not an eigenvalue of $\mathbf{A}$, the eigenvalues of the matrix $(\mathbf{A}-s\mathbf{I})^{-1}$ are $(\lambda_1-s)^{-1},\dots,(\lambda_n-s)^{-1}$.
> 3. The eigenvectors associated with the eigenvalues in the first two parts are the same as those of $\mathbf{A}$.

先看第 2 条在 $s=0$ 时的含义. 假设 $\mathbf{A}$ 的特征值按模长排序为

$$
|\lambda_n|\ge |\lambda_{n-1}|\ge \cdots > |\lambda_1|.
$$

那么 $\mathbf{A}^{-1}$ 的特征值模长会反过来：

$$
|\lambda_1^{-1}|>|\lambda_2^{-1}|\ge \cdots \ge |\lambda_n^{-1}|.
$$

也就是说，如果我们对 $\mathbf{A}^{-1}$ 做幂迭代，会收敛到 $\lambda_1^{-1}$，从而得到 $\mathbf{A}$ 的 "最靠近 0 的特征值" $\lambda_1$.

对一般的 $s\ne 0$，如果我们把特征值按与 $s$ 的距离排序：

$$
|\lambda_n-s|\ge \cdots \ge |\lambda_2-s|>|\lambda_1-s|,
$$

那么 $(\mathbf{A}-s\mathbf{I})^{-1}$ 的主导特征值就是 $(\lambda_1-s)^{-1}$. 因此对 $(\mathbf{A}-s\mathbf{I})^{-1}$ 做幂迭代，会收敛到 $\lambda_1$ (即离 $s$ 最近的那个特征值).

**#2 反迭代算法**

如果我们直接照搬幂迭代的更新，会出现一步

$$
\mathbf{y}_k=(\mathbf{A}-s\mathbf{I})^{-1}\mathbf{x}_k.
$$

和以往一样，我们不希望显式计算逆矩阵. 应当把这一步改写为解线性系统：

$$
(\mathbf{A}-s\mathbf{I})\mathbf{y}_k=\mathbf{x}_k.
$$

> **Algorithm:** Inverse iteration
> Given matrix $\mathbf{A}$ and shift $s$:
> 1. Choose $\mathbf{x}_1$.
> 2. For $k=1,2,\dots$,
>    a. Solve for $\mathbf{y}_k$ in
>    $$
>    (\mathbf{A}-s\mathbf{I})\mathbf{y}_k=\mathbf{x}_k.
>    $$
>    b. Find $m$ such that $|y_{k,m}|=\|\mathbf{y}_k\|_{\infty}$.
>    c. Set $\alpha_k=\frac{1}{y_{k,m}}$ and $\beta_k=s+\frac{x_{k,m}}{y_{k,m}}$.
>    d. Set $\mathbf{x}_{k+1}=\alpha_k\mathbf{y}_k$.

在幂迭代里，我们用 $y_{k,m}/x_{k,m}$ 来估计主导特征值. 在反迭代里，这个比值估计的是 $(\lambda_1-s)^{-1}$，因此要先取倒数并加回 $s$，才得到 $\lambda_1$ 的估计 $\beta_k$.

每次反迭代都需要解一个系数矩阵为 $\mathbf{B}=\mathbf{A}-s\mathbf{I}$ 的线性系统. 这里我们先用 LU 分解来实现，并期待它能工作. 由于 $\mathbf{B}$ 在迭代过程中保持不变，我们可以只做一次分解，然后在每次迭代中复用.

> **Function:** inviter
> **Shifted inverse iteration for the closest eigenvalue**
> ```Python
> import numpy as np
> import scipy.linalg as la
>
> def inviter(A, s, numiter, seed=0):
>     A = np.asarray(A)
>     n = A.shape[0]
>     rng = np.random.default_rng(seed)
>     x = rng.standard_normal(n)
>     x = x / np.linalg.norm(x, ord=np.inf)
>
>     beta = np.empty(numiter, dtype=complex if np.iscomplexobj(A) else float)
>
>     B = A - s * np.eye(n)
>     lu, piv = la.lu_factor(B)
>
>     for k in range(numiter):
>         y = la.lu_solve((lu, piv), x)
>         m = int(np.argmax(np.abs(y)))
>         beta[k] = x[m] / y[m] + s
>         x = y / y[m]
>
>     return beta, x
> ```

**#3 收敛性**

反迭代的收敛性可以直接借用幂迭代的结论，只需把矩阵 $\mathbf{A}$ 换成 $(\mathbf{A}-s\mathbf{I})^{-1}$. 设特征值按距离 $s$ 的远近排序为

$$
|\lambda_n-s|\ge \cdots \ge |\lambda_2-s|>|\lambda_1-s|.
$$

那么反迭代的线性收敛因子为

$$
\frac{\lambda_1-s}{\lambda_2-s},
\qquad (k\to\infty).
$$

因此当 $s$ 越靠近目标特征值 $\lambda_1$，并且 $s$ 与其他特征值保持相对更远的距离时，收敛会越快.

> **Demo:** Convergence of inverse iteration
> We reuse the same upper-triangular matrix as in the power iteration demo, pick a shift $s=0.7$, and observe linear convergence to the eigenvalue closest to $s$.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> lam = np.array([1.0, -0.75, 0.6, -0.4, 0.0])
> A = np.triu(np.ones((5, 5)), 1) + np.diag(lam)
>
> s = 0.7
> beta, x = inviter(A, s, 30, seed=1)
> eig_est = beta[-1]
> print("eig_est =", eig_est)
>
> err = np.abs(eig_est - beta)
> plt.semilogy(np.arange(len(err) - 1), err[:-1], "o-", ms=3)
> plt.xlabel("k")
> plt.ylabel(r"$|\\lambda-\\beta_k|$")
> plt.title("Convergence of inverse iteration")
> plt.grid(True, which="both", alpha=0.3)
> plt.show()
>
> # Compare observed rate to theory after sorting by distance to s.
> obs = err[22] / err[21]
> order = np.argsort(np.abs(lam - s))
> lam_sorted = lam[order]
> theory = (lam_sorted[0] - s) / (lam_sorted[1] - s)
> print("observed_rate  =", obs)
> print("theoretical_rate =", theory)
> ```

**#4 动态更新 shift**

在反迭代里存在一个明显的正反馈机会：收敛率随着 $s$ 更接近真实特征值而变好，而迭代本身又在给出越来越好的特征值估计. 因此我们可以在每次迭代后更新 shift，令

$$
s \leftarrow \beta_k.
$$

这样收敛会显著加速. 粗略地说：如果当前误差是 $\epsilon=O(|\lambda_1-s|)$，那么下一步误差会再乘一个因子 $O(|\lambda_1-s|)$，从而变成 $O(\epsilon^2)$，也就是二次收敛.

但代价是：线性系统的系数矩阵 $\mathbf{A}-s\mathbf{I}$ 会随迭代变化，无法再只做一次 LU 分解. 在很多情形下，更快的收敛会让这个代价仍然值得.

> **Note:** 实务里，比起单纯的幂迭代/反迭代，更常用的是基于本章后续数学结构的 Krylov 子空间方法 (以及 `eigs` 这类实现). 但反迭代仍然很有用：它常用于把一个特征值估计 "转化" 为一个对应的特征向量估计.

