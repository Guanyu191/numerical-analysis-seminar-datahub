# 8-2-幂迭代 (Power iteration)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 只用矩阵-向量乘法能做什么**

既然对稀疏矩阵而言，矩阵-向量乘法通常很快，那么我们就来看看：如果我们手里只有这一种操作，能做到哪些事情.

> **Demo:** Repeated matvec reveals an eigenvector
> We choose a random matrix and repeatedly apply matrix-vector multiplication. After many repetitions, the vector tends to align with a dominant eigenvector.
>
> ```Python
> import numpy as np
>
> rng = np.random.default_rng(0)
>
> A = rng.integers(1, 10, size=(5, 5)).astype(float)
> A = A / A.sum(axis=0, keepdims=True)  # make columns sum to 1 (a small "cheat" for a neat story)
>
> x = rng.standard_normal(5)
> for _ in range(8):
>     x = A @ x
>
> # Compare x and A@x after repeated multiplication.
> print(np.column_stack([x, A @ x]))
> ```
>
> We often observe that $A\mathbf{x}\approx \mathbf{x}$ after enough repetitions.

上面的 Demo 里有一点 "作弊"：把矩阵按列归一化，使得更容易出现一个主导特征值为 1 的情形. 但它揭示了一个更一般的事实.

**#2 主导特征值 (dominant eigenvalue)**

对矩阵幂的分析在可对角化情形下最直接. 设 $\mathbf{A}$ 是可对角化的 $n\times n$ 矩阵，特征值为 $\lambda_1,\dots,\lambda_n$，对应线性无关的特征向量为 $\mathbf{v}_1,\dots,\mathbf{v}_n$. 进一步假设

$$
|\lambda_1|>|\lambda_2|\ge |\lambda_3|\ge \cdots \ge |\lambda_n|.
$$

在这个条件下，我们称 $\lambda_1$ 为主导特征值 (dominant eigenvalue). 上面的 Demo 就是这种情形，并且还让 $\lambda_1=1$.

现在取任意向量 $\mathbf{x}$，令 $k$ 为正整数. 由 **7-2-特征值分解** 里的矩阵幂公式

$$
\mathbf{A}^k\mathbf{x}=\mathbf{V}\mathbf{D}^k\mathbf{V}^{-1}\mathbf{x}.
$$

令 $\mathbf{z}=\mathbf{V}^{-1}\mathbf{x}$，并记 $\mathbf{D}$ 为特征值对角矩阵，则

$$
\mathbf{A}^k\mathbf{x}
=
\mathbf{V}\mathbf{D}^k\mathbf{z}
=
\mathbf{V}
\begin{bmatrix}
\lambda_1^k z_1\\
\lambda_2^k z_2\\
\vdots\\
\lambda_n^k z_n
\end{bmatrix}
=
\lambda_1^k
\left[
z_1\mathbf{v}_1
z_2\left(\frac{\lambda_2}{\lambda_1}\right)^k\mathbf{v}_2
\cdots
z_n\left(\frac{\lambda_n}{\lambda_1}\right)^k\mathbf{v}_n
\right].
$$

因为 $|\lambda_j/\lambda_1|<1$ (对 $j\ge 2$)，所以只要 $z_1\ne 0$，当 $k\to\infty$ 时，$\mathbf{A}^k\mathbf{x}$ 最终会趋向于主导特征向量方向 (只差一个标量因子).

> **Note:** 算法视角里，$\mathbf{A}^k\mathbf{x}$ 应该理解为 $k$ 次重复应用 $\mathbf{A}$：$\mathbf{A}(\cdots(\mathbf{A}(\mathbf{A}\mathbf{x}))\cdots)$. 这样做可以最大化利用稀疏性. 如果先显式形成 $\mathbf{A}^k$，稀疏性往往会被幂运算破坏 (见 **8-1-稀疏性与结构** 里关于 fill-in 的例子).

**#3 幂迭代算法**

但要把上面的现象变成算法，还差一个技术细节：除非 $|\lambda_1|=1$，否则 $\lambda_1^k$ 会让 $\|\mathbf{A}^k\mathbf{x}\|$ 很快变得极大或极小. 同时，我们也不知道 $\lambda_1$，因此不能直接用 $\lambda_1^k$ 去做归一化.

解决方式是：每次做完一次矩阵-向量乘法后，就对向量做一次重归一化. 教材用 $\infty$-范数归一化，并且用某个分量的比值作为特征值估计.

> **Algorithm:** Power iteration
> Given matrix $\mathbf{A}$:
> 1. Choose $\mathbf{x}_1$.
> 2. For $k=1,2,\dots$,
>    a. Set $\mathbf{y}_k=\mathbf{A}\mathbf{x}_k$.
>    b. Find $m$ such that $|y_{k,m}|=\|\mathbf{y}_k\|_{\infty}$.
>    c. Set $\alpha_k=\frac{1}{y_{k,m}}$ and $\beta_k=\frac{y_{k,m}}{x_{k,m}}$.
>    d. Set $\mathbf{x}_{k+1}=\alpha_k\mathbf{y}_k$.

由定义，$\|\mathbf{x}_{k+1}\|_{\infty}=1$. 同时我们可以写出

$$
\mathbf{x}_k=(\alpha_1\alpha_2\cdots\alpha_k)\mathbf{A}^k\mathbf{x}_1.
$$

这说明幂迭代只是在 $\mathbf{A}^k\mathbf{x}_1$ 的基础上加入了归一化因子，从而让数值规模保持稳定.

当 $\mathbf{x}_k$ 已经接近 $\mathbf{A}$ 的主导特征向量时，$\mathbf{A}\mathbf{x}_k$ 会接近 $\lambda_1\mathbf{x}_k$，因此我们可以用

$$
\beta_k=\frac{y_{k,m}}{x_{k,m}}
$$

作为特征值 $\lambda_1$ 的估计.

> **Function:** poweriter
> **Power iteration to find a dominant eigenvalue**
> ```Python
> import numpy as np
>
> def poweriter(A, numiter, seed=0):
>     A = np.asarray(A)
>     n = A.shape[0]
>     rng = np.random.default_rng(seed)
>     x = rng.standard_normal(n)
>     x = x / np.linalg.norm(x, ord=np.inf)
>     beta = np.empty(numiter, dtype=complex if np.iscomplexobj(A) else float)
>
>     for k in range(numiter):
>         y = A @ x
>         m = int(np.argmax(np.abs(y)))
>         beta[k] = y[m] / x[m]
>         x = y / y[m]  # makes ||x||_inf = 1
>
>     return beta, x
> ```

上面的实现里，$\mathbf{A}$ 的唯一用途就是计算矩阵-向量乘积 $\mathbf{A}\mathbf{x}$. 因此只要我们能高效做 matvec，就能高效做幂迭代.

**#4 收敛性**

为了更细致地分析 $\beta_k$ 的误差，我们把 $\beta_k$ 的分子与分母分别展开，会出现一项形如

$$
r_2^k b_2 + \cdots + r_n^k b_n
$$

其中 $r_j=\lambda_j/\lambda_1$. 这时再引入一个额外假设

$$
|\lambda_2|>|\lambda_3|\ge \cdots \ge |\lambda_n|,
$$

它不是严格必要，但能让讨论更简洁，因为它保证上式主要由 $r_2^k$ 主导.

对大 $k$ 做几何级数展开后，可以得到 $\beta_k$ 的线性收敛行为：误差以比例因子 $r_2=\lambda_2/\lambda_1$ 衰减 (可能带符号振荡).

> **Observation:** Linear convergence of eigenvalue estimates
> The error in the power iteration eigenvalue estimates $\beta_k$ is reduced asymptotically by a constant factor $\lambda_2/\lambda_1$ at each iteration.

> **Demo:** Observing the convergence factor
> We run power iteration on a small matrix with prescribed eigenvalues and check that the asymptotic error ratio matches $\lambda_2/\lambda_1$.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> lam = np.array([1.0, -0.75, 0.6, -0.4, 0.0])
> A = np.triu(np.ones((5, 5)), 1) + np.diag(lam)  # upper triangular with these eigenvalues
>
> beta, x = poweriter(A, 60, seed=1)
> eig_est = beta[-1]
> print("eig_est =", eig_est)
>
> err = 1.0 - beta
> plt.semilogy(np.arange(len(err)), np.abs(err), "o-", ms=3)
> plt.xlabel("k")
> plt.ylabel(r"$|\\lambda_1 - \\beta_k|$")
> plt.title("Convergence of power iteration")
> plt.grid(True, which="both", alpha=0.3)
> plt.show()
>
> theory = lam[1] / lam[0]
> observed = err[40] / err[39]
> print("theory   =", theory)
> print("observed =", observed)
> ```
>
> The estimates can oscillate around the exact value when the dominant ratio is negative.

> **Note:** 实务里我们往往不知道精确的 $\lambda_1$. 这时通常会用最终的估计值 $\beta_{K}$ 来近似 $\lambda_1$，并据此画出 "相对收敛" 的曲线，但在接近机器精度时会出现参考值不够准导致的尾部偏差.
