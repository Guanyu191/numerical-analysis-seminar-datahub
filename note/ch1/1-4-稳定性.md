# 1-4-稳定性 (Stability)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记比较特殊，主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 算法的稳定性**

当我们用计算机算法求解一个数学问题，却发现结果误差很大时，第一反应往往是：问题本身是不是 **ill-conditioned**. 但误差也可能来自算法本身. 当某个算法的结果误差大到连问题的条件数都解释不了时，我们称这个算法是 **unstable** 的.

**#2 二次方程求根的例子**

在 **1-2-问题与条件数** 里，我们看到：二次多项式求根是否病态，关键取决于两根相对它们自身大小是否过近. 因此对下面这个多项式，

$$
p(x)=(x-10^6)(x-10^{-6})=x^2-(10^6+10^{-6})x+1,
$$

求根是一个条件良好的问题. 一个看起来很自然的算法，是直接使用熟悉的求根公式：对 $ax^2+bx+c$，其根为

$$
x_1=\frac{-b+\sqrt{b^2-4ac}}{2a},\qquad x_2=\frac{-b-\sqrt{b^2-4ac}}{2a}.
$$

> **Demo:** Quadratic formula can lose accuracy.
> ```Python
> import numpy as np
>
> a = 1.0
> b = -(1e6 + 1e-6)
> c = 1.0
>
> x1 = (-b + np.sqrt(b * b - 4 * a * c)) / (2 * a)
> x2 = (-b - np.sqrt(b * b - 4 * a * c)) / (2 * a)
> print("x1 =", x1)
> print("x2 =", x2)
>
> error = abs(1e-6 - x2) / 1e-6
> accurate_digits = -np.log10(error)
> print("accurate_digits =", accurate_digits)
> ```
> You should see that `x1` is correct to all stored digits, but `x2` has only about 5 accurate digits.

> **Note:** 科学记数法在 Python 里也常写成 `1.23e4` (而不是 `1.23*10**4`). 另外，浮点数的默认打印格式可能会省略十进制展开末尾的 0，这不等价于丢失了内部存储的二进制有效位.

上面的输出里，较大的根 $x_1$ 非常准确，但较小的根 $x_2$ 明显丢失了有效数字. 这可以用 "算法路径" 上的条件数来解释.

把 $x_1$ 的计算拆成一串初等步骤：

$$
u_1=b^2,\quad
u_2=u_1-4,\quad
u_3=\sqrt{u_2},\quad
u_4=u_3-b,\quad
u_5=u_4/2,
$$

最终 $u_5=x_1$. 对每一步的条件数做一个数量级估计，可以得到类似下面的结果：

$$
\begin{array}{lll}
\hline
\text{Calculation} & \text{Result} & \kappa \\
\hline
u_1=b^2 & 1.000000000002000\times 10^{12} & 2 \\
u_2=u_1-4 & 9.999999999980000\times 10^{11} & \approx 1.00 \\
u_3=\sqrt{u_2} & 999999.9999990000 & 1/2 \\
u_4=u_3-b & 2000000 & \approx 0.500 \\
u_5=u_4/2 & 1000000 & 1 \\
\hline
\end{array}
$$

> **Note:** 这里的思路是：把算法写成一串初等函数的复合，然后用 **1-2-问题与条件数** 中的 "条件数链式法则" 把各步的相对放大倍数乘起来. 当每一步都没有出现大的放大倍数时，我们不该期待出现远超条件数解释范围的误差增长.

但如果我们用同样的求根公式去计算 "坏根" $x_2$，那么倒数第二步会变成

$$
u_4=(-u_3)-b,
$$

这时 $u_4$ 会经历明显的 **subtractive cancellation**，并且对应的相对条件数将变得非常大 (数量级可到 $5\times 10^{11}$). 因此我们会预期丢失大约 11 位十进制有效数字，这正是上面 Demo 里观察到的现象.

这个例子说明：即使问题本身条件良好，某条具体的计算路径仍可能因为一次关键的消去而变得 **unstable**. 这里 "不稳定" 的来源不是数据敏感性，而是我们选择的计算路径.

我们可以换一条避免消去的路径. 对二次方程的两个根，总有

$$
x_1x_2=\frac{c}{a}.
$$

因此一旦我们先用求根公式得到了那个更可靠的根 $r$，就可以用

$$
\frac{c}{ar}
$$

来计算另一个根. 这个计算只包含乘法与除法，通常不会引入类似的数值麻烦.

> **Demo:** A numerically safer path for the second root.
> ```Python
> import numpy as np
>
> a = 1.0
> b = -(1e6 + 1e-6)
> c = 1.0
>
> x1 = (-b + np.sqrt(b * b - 4 * a * c)) / (2 * a)
> x2 = c / (a * x1)
> print("x1 =", x1)
> print("x2 =", x2)
>
> relerr = abs(x2 - 1e-6) / 1e-6
> print("relative error =", relerr)
> ```
> You should see `x2 = 1.0e-6` and `relative error = 0.0`.

两种算法在实数与精确算术下是等价的. 但在浮点运算里，每一步都会发生舍入扰动，再加上条件数的链式放大，最终结果可能会极其依赖具体的计算路径.

> **Observation:** The sensitivity of a problem $f(x)$ is governed only by $\kappa_f$, but the sensitivity of an algorithm depends on the condition numbers of all of its individual steps.

> **Note:** 这个案例里，"问题的条件数" 不大，但某条计算路径的中间步骤出现了巨大的局部放大倍数，于是算法误差远超我们从条件数能预期到的水平. 因此判断数值可靠性时，我们既要看问题的条件数，也要看算法路径上是否出现 subtractive cancellation 等放大机制.

**#3 不稳定性的一个实用判断**

看起来我们需要一路追踪每个中间量的条件数，这似乎很复杂. 但多数我们习以为常的初等运算 (例如 **1-2-问题与条件数** 中表格里的那些) 在大多数情况下都表现良好. 例外往往发生在 $|f(x)|$ 远小于 $|x|$ 的场景里，但这并不必然意味着麻烦；最常见的罪魁祸首仍是简单的 **subtractive cancellation**.

一个更实用的判断方式是：如果一个算法算出来的结果，比问题条件数所能解释的程度差很多，我们就应怀疑它是 **unstable** 的. 实践中，我们常把算法跑在一些答案已知的测试问题上，或与已知可靠的程序交叉验证，以尽早暴露潜在的不稳定性. 在后续章节里，我们会看到不同类型问题里不稳定性会以哪些具体方式出现.

**#4 反向误差 (Backward error)**

当问题 $f(x)$ 本身 **ill-conditioned** 时，仅仅把数据 $x$ 舍入为浮点数，就可能让结果发生很大变化. 为了客观地衡量算法的水平，我们不该期待任何算法 $\tilde f$ 都能满足 "正向误差很小" 这样的要求 (即 $\tilde f(x)\approx f(x)$).

我们可以改用另一种误差刻画方式：与其只问 "我们是否得到了几乎正确的答案？"，不如问 "我们是否回答了几乎正确的问题？". 这会引出 **backward error** 的概念.

> **Note:** 直觉上，前者更关注结果是否正确，后者更关注算法过程是否稳定. 我们希望算法的结果 $\tilde{y} = \tilde{f}(x)$，相比基于理论得到的相同结果 $\tilde{y} = f(\tilde{x})$，其输入是相差不大的，即 $x \approx \tilde{x}$. 

> **Note:** 这里的 "几乎正确的问题" 指的是：是否存在一个和 $x$ 很接近的 $\tilde x$，使得当前的计算结果 $\tilde y$ 恰好满足 $\tilde y=f(\tilde x)$. 也就是说，反向误差关注的是 "把输入改动多少" 才能解释输出，而不是直接比较输出差值.

> **Definition:** Backward error.
> Let $\tilde f$ be an algorithm for the problem $f$. Let $y=f(x)$ be an exact result and $\tilde y=\tilde f(x)$ be its approximation by the algorithm. If there is a value $\tilde x$ such that $f(\tilde x)=\tilde y$, then the relative backward error in $\tilde y$ is
> $$
> \frac{|\tilde x-x|}{|x|}.
> $$
> The absolute backward error is $|\tilde x-x|$.

反向误差衡量的是：我们需要把原始数据改动多少，才能让算法输出变成某个 "精确问题" 的精确解. 直观上，它把误差从 "输出端" 转回关注到 "输入端".

> **Note:** 反向误差并不是在说输出误差小，而是在说 "存在一个很近的输入 $\tilde x$，使得当前输出是 $f(\tilde x)$ 的精确值". 当问题病态时，正向误差可能无法避免，但小反向误差依然有意义，因为它说明算法并没有把数据扭曲到非常离谱的程度.

> **Demo:** Forward error can be misleading for multiple roots.
> ```Python
> import numpy as np
>
> np.set_printoptions(precision=16)
>
> # Construct a polynomial with known roots, including a double root at x = 1.
> r = np.array([-2.0, -1.0, 1.0, 1.0, 3.0, 6.0])
> p_desc = np.poly(r)            # coefficients in descending powers
> r_tilde = np.sort(np.roots(p_desc))
>
> print("computed roots =", r_tilde)
> print("root errors:")
> print(np.abs(r - r_tilde) / r)
>
> # Backward error: compare coefficients of the polynomial implied by the computed roots.
> c = p_desc[::-1]               # ascending powers: constant term first
> p_tilde_desc = np.poly(r_tilde)
> c_tilde = p_tilde_desc[::-1]
>
> print("coefficient errors:")
> print(np.abs(c - c_tilde) / c)
> ```
> You should see forward errors close to machine epsilon except near the double root at `x = 1`, while the coefficient errors remain close to machine epsilon.

这里出现的现象并不意外：重根附近的求根问题条件数很大，因此正向误差可能会被放大. 但从反向误差的角度看，即使某些根在相对意义下偏离较多，它们仍可能是某个 "系数非常接近原始系数" 的多项式的精确根.

在病态问题里，我们能期待的最好结果往往就是小的反向误差. 不展开形式化细节的话，我们只需要记住：如果一个算法总能产生小的反向误差，那么它就是稳定的. 但反过来并不总成立：有些稳定算法也可能产生很大的反向误差.

> **Example:** One stable algorithm that is not backward stable is floating-point evaluation for our old friend, $f(x)=x+1$. If $|x|<\epsilon_{\rm mach}/2$, then the computed result is $\tilde f(x)=1$, since there are no floating-point numbers between $1$ and $1+\epsilon_{\rm mach}$. Hence the only possible choice for a real number $\tilde x$ satisfying the backward error relation is $\tilde x=0$. But then $|\tilde x-x|/|x|=1$, which indicates 100% backward error.
