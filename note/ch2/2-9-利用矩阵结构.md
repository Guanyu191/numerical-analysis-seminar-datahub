# 2-9-利用矩阵结构 (Exploiting matrix structure)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 引入矩阵的结构**

很多计算问题的矩阵并不是 "完全一般" 的，它们往往有某些结构或性质，可以让我们更快或更准确地求解.

例如，一个 $n\times n$ 矩阵 $\mathbf{A}$ 若满足

$$
\lvert A_{ii}\rvert > \sum_{\substack{j=1\\ j\ne i}}^{n} \lvert A_{ij}\rvert, \quad \text{for each } i=1,\dots,n
$$

则称 $\mathbf{A}$ 是对角占优 (**diagonally dominant**) 的. 对角占优矩阵能保证可逆，并且在数值上不需要行主元就能稳定地做 LU 分解.

这一节我们会看三类常见结构：**带状矩阵**、**对称矩阵**、**对称正定矩阵**.

**#2 带状矩阵**

> **Definition:** **Bandwidth.**
> A matrix $\mathbf{A}$ has upper bandwidth $b_u$ if $j-i>b_u$ implies $A_{ij}=0$, and lower bandwidth $b_\ell$ if $i-j>b_\ell$ implies $A_{ij}=0$. We say the total bandwidth is $b_u+b_\ell+1$. When $b_u=b_\ell=1$, we have the important case of a tridiagonal matrix.

> **Note:** 我们可以理解为 "允许出现非零元的对角线 **条数**"：上带宽 $b_u$ 表示主对角线上方最多有 $b_u$ 条可能非零的对角线，下带宽 $b_\ell$ 表示主对角线下方最多有 $b_\ell$ 条可能非零的对角线. 例如：
> $$
> \begin{align*}
> & \text{(1) } b_u=b_\ell=1\ \text{(三对角矩阵)}:\quad
> \begin{bmatrix}
> \ast & \ast\\
> \ast & \ast & \ast\\
>  & \ddots & \ddots & \ddots\\
>  &  & \ast & \ast & \ast\\
>  &  &  & \ast & \ast
> \end{bmatrix},\\
> 
> & \text{(2) } b_u=1,\ b_\ell=0\ \text{(上双对角矩阵)}:\quad
> \begin{bmatrix}
> \ast & \ast\\
>  & \ast & \ast\\
>  &  & \ddots & \ddots\\
>  &  &  & \ast & \ast\\
>  &  &  &  & \ast
> \end{bmatrix},\\
> 
> & \text{(3) } b_u=b_\ell=2\ \text{(五对角矩阵)}:\quad
> \begin{bmatrix}
> \ast & \ast & \ast\\
> \ast & \ast & \ast & \ast\\
> \ast & \ast & \ast & \ast & \ast\\
>  & \ddots & \ddots & \ddots & \ddots & \ddots\\
>  &  & \ast & \ast & \ast & \ast & \ast\\
>  &  &  & \ast & \ast & \ast & \ast\\
>  &  &  &  & \ast & \ast & \ast
> \end{bmatrix}.
> \end{align*}
> $$

下面的 Demo 构造了一个小的三对角矩阵，并观察它的不带主元 LU 分解.

> **Demo:** LU factorization preserves bandwidth when pivoting is not used.
> ```Python
> import numpy as np
>
> def lufact(A):
>     A = np.array(A, dtype=float, copy=True)
>     n = A.shape[0]
>     L = np.eye(n)
>     U = np.zeros((n, n), dtype=float)
>     Ak = A.copy()
>     for k in range(n - 1):
>         U[k, :] = Ak[k, :]
>         L[:, k] = Ak[:, k] / U[k, k]
>         Ak = Ak - np.outer(L[:, k], U[k, :])
>     U[-1, -1] = Ak[-1, -1]
>     return L, U
>
> # A 6x6 tridiagonal example.
> main = np.array([2, 2, 0, 2, 1, 2], dtype=float)
> sub = np.array([4, 3, 2, 1, 0], dtype=float)
> sup = -np.ones(5, dtype=float)
> A = np.diag(main) + np.diag(sup, 1) + np.diag(sub, -1)
> print("A =\n", A)
>
> # Extract diagonals: k=0 main, k=1 super, k=-1 sub.
> print("diag(A,0) =", np.diag(A, 0))
> print("diag(A,-1) =", np.diag(A, -1))
>
> L, U = lufact(A)
> print("L =\n", L)
> print("U =\n", U)
> ```
> The nonzeros of `L` and `U` stay within the original bandwidth.

> **Note:** 在这个三对角例子里，上下带宽都是 1. 当我们不做行主元时，每一步消元都只会用到 pivot 行附近的少量元素，因此 **$\mathbf{L},\mathbf{U}$ 也会保留同样的带宽结构**. 对三对角矩阵，典型的形状是
> $$
> \mathbf{L}=
> \begin{bmatrix}
> 1\\
> \ell_{21} & 1\\
>  & \ell_{32} & 1\\
>  &  & \ddots & \ddots\\
>  &  &  & \ell_{n,n-1} & 1
> \end{bmatrix},
> \qquad
> \mathbf{U}=
> \begin{bmatrix}
> u_{11} & u_{12}\\
>  & u_{22} & u_{23}\\
>  &  & \ddots & \ddots\\
>  &  &  & u_{n-1,n-1} & u_{n-1,n}\\
>  &  &  &  & u_{nn}
> \end{bmatrix}.
> $$
> 如果做行主元，行交换可能把带宽打乱，并引入额外的非零元，因此 "利用带宽" 往往需要额外的结构性假设或更专门的算法.

保留带宽意味着我们不必做 $O(n^3)$ 规模的运算. 当我们在实现上跳过那些结构性为 0 的位置时，LU 分解与三角代回的开销会显著下降.

> **Observation:** The number of flops needed by LU factorization without pivoting is $O(b_u b_\ell n)$ when the upper and lower bandwidths are $b_u$ and $b_\ell$.

> **Note:** 一个粗糙的估算方式是：在第 $k$ 步消元时，非零元只可能出现在 pivot 附近的一个带状区域内. 具体地，每一列最多只有 $b_\ell$ 个位置 (在 pivot 下方)、每一行最多只有 $b_u$ 个位置 (在 pivot 右侧) 会更新. 因此每步大约更新 $O(b_\ell b_u)$ 个矩阵元素，做 $n$ 步就得到总 flops 为 $O(b_\ell b_u n)$.

如果我们想进一步利用稀疏性，最关键的不是改 "算法公式"，而是改 "存储方式与实现方式". 我们要用稀疏矩阵类型来避免存储和运算那些为 0 的位置. 这一点会在更靠后的章节详细展开 (Chapter 7 & 8).

> **Demo:** Dense vs sparse factorization for a tridiagonal matrix (SciPy).
> ```Python
> import time
> import numpy as np
> from scipy.linalg import lu_factor
> from scipy.sparse import diags
> from scipy.sparse.linalg import splu
>
> n = 1000
> sub = np.ones(n - 1, dtype=float)
> main = np.arange(1, n + 1, dtype=float)
> sup = np.arange(n - 1, 0, -1, dtype=float)
>
> # Dense representation loses sparsity.
> A_dense = np.diag(main) + np.diag(sup, 1) + np.diag(sub, -1)
>
> t0 = time.perf_counter()
> lu_factor(A_dense)
> t1 = time.perf_counter()
>
> # Sparse representation keeps only nonzeros.
> A_sparse = diags([sub, main, sup], [-1, 0, 1], format="csc")
> t2 = time.perf_counter()
> splu(A_sparse)
> t3 = time.perf_counter()
>
> print("dense lu_factor:", t1 - t0)
> print("sparse splu:", t3 - t2)
> print("dense bytes ~", A_dense.nbytes)
> print("sparse nnz =", A_sparse.nnz)
> ```
> Sparse factorization can be dramatically faster and use far less memory.

> **Note:** 三对角的 $n\times n$ 矩阵，非零元个数是 $3n-2$，但稠密存储会占用 $n^2$ 个元素. 因此当 $n$ 很大时，能否把 "结构性为 0 的位置" 从存储与运算里排除，往往会带来数量级上的差异.

**#3 对称矩阵与 $\mathbf{L}\mathbf{D}\mathbf{L}^{T}$**

> **Definition:** **Symmetric matrix.**
> A square matrix $\mathbf{A}$ satisfying $\mathbf{A}^{T} = \mathbf{A}$ is called symmetric.

对称矩阵在应用中很常见. 当矩阵具有对称性，我们往往希望算法既能利用它，也能在计算过程中保持它.

普通 LU 分解通过把 $\mathbf{L}$ 的对角线固定为 1 来保证分解的唯一性，但于对称矩阵而言，这会破坏分解的对称性. 因此我们将分解的目标修改为

$$
\mathbf{A}=\mathbf{L}\mathbf{D}\mathbf{L}^{T},
$$

其中 $\mathbf{L}$ 是单位下三角矩阵，$\mathbf{D}$ 是对角矩阵.

推导这个分解时，一个关键代数工具是 "带对角权重的外积求和".

> **Theorem:** **Linear combination of outer products.**
> Let $\mathbf{D}$ be an $n\times n$ diagonal matrix with diagonal elements $d_1,d_2,\dots,d_n$, and suppose $\mathbf{A}$ and $\mathbf{B}$ are $n\times n$ as well. Write the columns of $\mathbf{A}$ as $\mathbf{a}_1,\dots,\mathbf{a}_n$ and the rows of $\mathbf{B}$ as $\mathbf{b}_1^{T},\dots,\mathbf{b}_n^{T}$. Then
> $$
> \mathbf{A}\mathbf{D}\mathbf{B}=\sum_{k=1}^{n} d_k\,\mathbf{a}_k\mathbf{b}_k^{T}.
> $$

下面的 Demo 用一个 $4\times 4$ 对称矩阵把 $\mathbf{L}\mathbf{D}\mathbf{L}^{T}$ 的外积消去过程写出来.

> **Demo:** A worked $\mathbf{L}\mathbf{D}\mathbf{L}^{T}$ factorization by successive outer products.
> ```Python
> import numpy as np
>
> A = np.array(
>     [
>         [2, 4, 4, 2],
>         [4, 5, 8, -5],
>         [4, 8, 6, 2],
>         [2, -5, 2, -26],
>     ],
>     dtype=float,
> )
>
> n = A.shape[0]
> L = np.eye(n)
> d = np.zeros(n, dtype=float)
> Ak = A.copy()
>
> for k in range(n):
>     d[k] = Ak[k, k]
>     L[:, k] = Ak[:, k] / d[k]
>     Ak = Ak - d[k] * np.outer(L[:, k], L[:, k])
>
> D = np.diag(d)
> print("d =", d)
> print("L =\n", L)
> print("||A - L D L^T||_inf =", np.linalg.norm(A - L @ D @ L.T, np.inf))
> ```
> When the process succeeds, the residual `A - L D L^T` is small.

> **Note:** 这个 Demo 的每一步都可以直接写成外积消去. 令初始矩阵 $\mathbf{A}_1=\mathbf{A}$. 对 $k=1,\dots,n$：
> 
> - 取 $d_k=(\mathbf{A}_k)_{kk}$.
> - 定义 $\boldsymbol{\ell}_k=\mathbf{A}_k\mathbf{e}_k/d_k$ (`l_k = A_k[:,k] / d[k]`)，使 $(\boldsymbol{\ell}_k)_k=1$.
> - 做一次对称的外积更新：
> $$
> \mathbf{A}_{k+1}=\mathbf{A}_k-d_k\,\boldsymbol{\ell}_k\boldsymbol{\ell}_k^{T}.
> $$
> 
> 最后把所有 $d_k$ 放进对角矩阵 $\mathbf{D}=\mathrm{diag}(d_1,\dots,d_n)$，把所有 $\boldsymbol{\ell}_k$ 放进 $\mathbf{L}=[\boldsymbol{\ell}_1,\dots,\boldsymbol{\ell}_n]$，就得到 $\mathbf{A}=\mathbf{L}\mathbf{D}\mathbf{L}^{T}$. 因为每一步减去的是对称矩阵 $d_k\,\boldsymbol{\ell}_k\boldsymbol{\ell}_k^{T}$，所以如果 $\mathbf{A}_k$ 对称，那么 $\mathbf{A}_{k+1}$ 仍然对称.

由于对称性，分解过程中上三角的运算是下三角的镜像，因此对称 LU 的 flops 量级大约是普通 LU 的一半.

> **Observation:** $\mathbf{L}\mathbf{D}\mathbf{L}^{T}$ factorization on an $n\times n$ symmetric matrix, when successful, takes $\sim \frac{1}{3}n^3$ flops.

不过，和普通 LU 类似，不带主元的 $\mathbf{L}\mathbf{D}\mathbf{L}^{T}$ 也可能不稳定，甚至不一定存在. 我们接下来把注意力收束到一个更友好的子类：对称正定矩阵.

**#4 对称正定矩阵与 Cholesky**

对称正定的核心是 **二次型严格为正**. 对任意非零 $\mathbf{x}\in\mathbb{R}^n$，数量 $\mathbf{x}^{T}\mathbf{A}\mathbf{x}$ 是一个 $1\times n$、$n\times n$、$n\times 1$ 矩阵相乘得到的标量，称为二次型 (**quadratic form**). 它也可以展开为

$$
\mathbf{x}^{T}\mathbf{A}\mathbf{x}=\sum_{i=1}^{n}\sum_{j=1}^{n} A_{ij}x_ix_j.
$$

> **Definition:** **Symmetric positive definite matrix.**
> A real $n\times n$ matrix $\mathbf{A}$ is called a symmetric positive definite matrix (or **SPD** matrix) if it is symmetric and, for all nonzero $\mathbf{x}\in\mathbb{R}^n$,
> $$
> \mathbf{x}^{T}\mathbf{A}\mathbf{x} > 0.
> $$

> **Note:** 正定性通常很难直接按定义去检验，不过它有一些等价刻画. 例如，一个对称矩阵正定 $\Leftrightarrow$ 特征值都是实的正数. 另外，SPD 矩阵有许多好性质在实际应用中能够用得上，并且对于实际应用中的矩阵而言，我们大都可以从理论上保证它们是 SPD 矩阵.

我们来看看正定性对 $\mathbf{L}\mathbf{D}\mathbf{L}^{T}$ 分解意味着什么. 若 $\mathbf{A}=\mathbf{L}\mathbf{D}\mathbf{L}^{T}$，则对任意非零 $\mathbf{x}$，

$$
0 < \mathbf{x}^{T}\mathbf{A}\mathbf{x}
=\mathbf{x}^{T}\mathbf{L}\mathbf{D}\mathbf{L}^{T}\mathbf{x}
=\mathbf{z}^{T}\mathbf{D}\mathbf{z},\qquad \mathbf{z}=\mathbf{L}^{T}\mathbf{x}.
$$

注意 $\mathbf{L}$ 是单位下三角，因此可逆，于是每个 $\mathbf{z}\in\mathbb{R}^n$ 都能写成 $\mathbf{z}=\mathbf{L}^{T}\mathbf{x}$. 由 $\mathbf{x}$ 的任意性，不妨取 $\mathbf{z}=\mathbf{e}_k$ ($k=1,\dots,n$)，就得到 $D_{kk}>0$ 对所有 $k$ 都成立.

这就允许我们对 $\mathbf{D}$ 做逐元素开方，得到一种 **"平方根"** 形式. 若

$$
\mathbf{D}=\mathrm{diag}(D_{11},\dots,D_{nn}),
\qquad
\mathbf{D}^{1/2}=\mathrm{diag}(\sqrt{D_{11}},\dots,\sqrt{D_{nn}}),
$$

则 $\mathbf{D}=(\mathbf{D}^{1/2})^2$，从而

$$
\mathbf{A}=\mathbf{L}\mathbf{D}^{1/2}\mathbf{D}^{1/2}\mathbf{L}^{T}
=(\mathbf{D}^{1/2}\mathbf{L}^{T})^T (\mathbf{D}^{1/2}\mathbf{L}^{T})
=\mathbf{R}^{T}\mathbf{R},
\qquad
\mathbf{R}:=\mathbf{D}^{1/2}\mathbf{L}^{T}.
$$

这里 $\mathbf{R}$ 是上三角矩阵，并且对角元为正. **Cholesky 分解是专门对 SPD 矩阵的 $\mathbf{L}\mathbf{D}\mathbf{L}^{T}$ 分解.**

> **Theorem:** **Cholesky factorization.**
> Any SPD matrix $\mathbf{A}$ may be factored as
> $$
> \mathbf{A}=\mathbf{R}^{T}\mathbf{R},
> $$
> where $\mathbf{R}$ is an upper triangular matrix with positive diagonal elements. This is called the Cholesky factorization.

虽然不带主元的 $\mathbf{L}\mathbf{D}\mathbf{L}^{T}$ 分解一般不稳定，甚至不一定存在，但在 **SPD** 情形可以证明 **三角分解存在** 且稳定.

> **Observation:** Cholesky factorization of an $n\times n$ SPD matrix takes $\sim \frac{1}{3}n^3$ flops.

Cholesky 分解的速度和稳定性使它成为求解 SPD 线性系统的首选直接法. 另外，Cholesky 算法当且仅当矩阵 $\mathbf{A}$ 不正定时会失败 (具体表现为对负数开平方或除以 0)，因此我们还可以用来检验一个对称矩阵的正定性.

随机生成的矩阵几乎不可能对称，不过我们可以用 $\mathbf{B}=\mathbf{A}+\mathbf{A}^{T}$ 或 $\mathbf{C}=\mathbf{A}^{T}\mathbf{A}$ 使 **对称化**.

> **Demo:** Cholesky succeeds for SPD matrices and fails otherwise.
> ```Python
> import numpy as np
> from scipy.linalg import cholesky
>
> rng = np.random.default_rng(0)
> A = rng.integers(1, 10, size=(4, 4)).astype(float)
>
> # Symmetrize, but this is unlikely to be positive definite.
> B = A + A.T
> try:
>     cholesky(B, lower=False)
>     print("unexpected: B is SPD")
> except Exception as e:
>     print("Cholesky failed as expected:", type(e).__name__)
>
> # Manufacture an SPD matrix.
> C = A.T @ A
> R = cholesky(C, lower=False)
> rel = np.linalg.norm(R.T @ R - C, 2) / np.linalg.norm(C, 2)
> print("relative factorization residual =", rel)
> ```
> The factorization residual is at the level of roundoff.

> **Note:** 上面先构造了对称矩阵 $\mathbf{B}=\mathbf{A}+\mathbf{A}^{T}$，它虽然对称，但通常不是正定的，因此 Cholesky 往往会失败. 然后构造 $\mathbf{C}=\mathbf{A}^{T}\mathbf{A}$，它通常是正定的，因为随机数矩阵 $\mathbf{A}$ 一般满秩，只要 $\mathbf{A}$ 满秩，$\mathbf{C}$ 就是对称正定矩阵，Cholesky 就能成功，并给出分解 $\mathbf{C}=\mathbf{R}^{T}\mathbf{R}$. 代码最后用相对残差 $\|\mathbf{R}^{T}\mathbf{R}-\mathbf{C}\|/\|\mathbf{C}\|$ 验证分解误差处于舍入误差尺度.
> 
> **Note:** 理论上，$\mathbf{C}=\mathbf{A}^{T}\mathbf{A}$ 只有在 $\mathbf{A}$ 满秩时才是 SPD，若出现 Cholesky 失败，可以重新生成 $\mathbf{A}$，或改用 $\mathbf{C}=\mathbf{A}^{T}\mathbf{A}+\delta\mathbf{I}$ (例如 $\delta=10^{-12}$) 来保证正定.
