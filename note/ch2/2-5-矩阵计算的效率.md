# 2-5-矩阵计算的效率 (Efficiency of matrix computations)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 渐近分析**

如果我们想精确预测某个算法在某台机器、某种实现方式、某种语言环境下的运行时间，现实中会非常困难. 更可行的方案是：当问题规模增长，预测算法用时如何随规模缩放.

以线性系统 $\mathbf{A}\mathbf{x}=\mathbf{b}$ 为例，规模参数通常是 $n$，也就是未知量个数与方程条数. 因为运行时间的表达式只能近似地表示，所以我们通常只保留当 $n\to\infty$ 时占主导的那一项. 下面引入一些专门的记号.

> **Definition:** **Asymptotic notation.**
> Let $f(n)$ and $g(n)$ be positive-valued functions. We say $f(n) = O(g(n))$ (read "f is big-O of g") as $n \to \infty$ if $f(n)/g(n)$ is bounded above as $n \to \infty$.
> We say $f(n) \sim g(n)$ (read "f is asymptotic to g") as $n \to \infty$ if $f(n)/g(n) \to 1$ as $n \to \infty$.

> **Note:** 从定义的角度来说，这里的 $O(g)$ 既包括 $g$ 的 **"同阶量"** ($f/g \to c, \,(0 < |c|< \infty)$)，也包括 $g$ 的 "低阶量" ($f/g \to 0$)，但我们习惯上当作同阶量. 其中，同阶量的一个特殊情况是 $f/g \to 1, \,(f \sim g)$，我们也将 $f$ 称作 $g$ 的 "等价量" .

> **Note:** 更严格地说，$O(g)$ 与 $\sim g$ 都是函数集合，且 $\sim g$ 是 $O(g)$ 的子集. 但习惯上我们写 $f=O(g)$ 而不是 $f\in O(g)$.

一个直接推论是：如果 $f\sim g$，那么一定有 $f=O(g)$.

> **Example:** Consider the functions $f(n) = a_1 n^3 + b_1 n^2 + c_1 n$ and $g(n) = a_2 n^3$ in the limit $n\to\infty$. Then
> $$
> \lim_{n\to\infty}\frac{f(n)}{g(n)}
> =
> \lim_{n\to\infty}\frac{a_1 + b_1 n^{-1} + c_1 n^{-2}}{a_2}
> =
> \frac{a_1}{a_2}.
> $$
> Since $a_1/a_2$ is a constant, $f(n)=O(g(n))$; if $a_1=a_2$, then $f\sim g$.

> **Example:** Consider $f(n)=\sin(1/n)$, $g(n)=1/n$ and $h(n)=1/n^2$. For large $n$, Taylor's theorem with remainder implies that
> $$
> f(n)=\frac{1}{n}-\frac{\cos(1/\xi)}{6n^3},
> $$
> where $n<\xi<\infty$. But
> $$
> \lim_{n\to\infty}\frac{f}{g}
> =
> \lim_{n\to\infty}\left(1-\frac{\cos(1/\xi)}{6n^2}\right)
> =1,
> $$
> and so $f\sim g$. On the other hand,
> $$
> \lim_{n\to\infty}\frac{f}{h}
> =
> \lim_{n\to\infty}\left(n-\frac{\cos(1/\xi)}{6n}\right)
> =\infty,
> $$
> so we cannot say that $f = O(h)$. A consideration of $h/f$ will show that $h = O(f)$, however.

使用渐近记号时，一个约定俗成的习惯是尽量写得 **"尽可能具体"**. 例如 $n^2+n=O(n^{10})$ 虽然是真的，但通常我们会写 $n^2+n=O(n^2)$，因为它提供的信息更多.

渐近记号还可以用来快速简化求和. 有一个很常用的套路是

$$
\sum_{k=1}^{n} k \sim \frac{n^2}{2}=O(n^2),\qquad
\sum_{k=1}^{n} k^2 \sim \frac{n^3}{3}=O(n^3),\qquad
\sum_{k=1}^{n} k^p \sim \frac{n^{p+1}}{p+1}=O(n^{p+1}).
$$

这些公式在形式上很像 $\int_0^n x^p\,\mathrm{d}x$.

> **Example:** 
> $$
> \sum_{k=1}^{n-1} (4k^2+3)
> =
> 4\sum_{k=1}^{n-1} k^2 + 3\sum_{k=1}^{n-1} 1
> \sim 4\left(\frac{(n-1)^3}{3}\right)+3(n-1)
> = \frac{4}{3}(n^3-3n^2+3n-1)+3n-3
> \sim \frac{4}{3}n^3.
> $$

**#2 Flop 计数**

在数值线性代数里，一个传统的复杂度估计方式是数 **floating-point operations (浮点数运算)**，简称 **flops**. 在这里我们把每一次标量加、减、乘、除、以及平方根都算作 1 个 flop. 给定一个算法，我们把这些标量 flops 加起来，忽略其他一切开销.

> **Demo:** Flop count for a naive matrix-vector product.
> ```Python
> import numpy as np
>
> n = 6
> rng = np.random.default_rng(0)
> A = rng.standard_normal((n, n))
> x = rng.standard_normal(n)
>
> y = np.zeros(n)
> for i in range(n):
>     for j in range(n):
>         y[i] += A[i, j] * x[j]   # 1 multiply + 1 add
> ```
> The nested loops do `2*n*n` flops, so matrix-vector multiplication is `O(n^2)` in the general case.

> **Note:** 这里我们按 "一次标量乘法/加法算 1 个 flop" 来计数：双重循环一共做了 $n^2$ 次乘法与 $n^2$ 次加法，因此是 $2n^2$ 个 flops. 如果把每一行看成一个长度为 $n$ 的点积来数，则每行约 $(2n-1)$ 个 flops，总计约 $2n^2-n$. 常数差异不影响 $O(n^2)$ 的结论.

因为矩阵 $\mathbf{A}$ 有 $n^2$ 个元素，而且一般情形下它们都要参与运算，所以很难期待把矩阵-向量乘法做得比 $O(n^2)$ 更低.

除了数 flops，我们也可以直接做简单的计时实验，用来观察 "矩阵规模增长时，计算用时如何变化". 下面给出一个最小可复现的计时脚本：对一组不同的 $n$，重复计算 `A @ x` 若干次并记录时间.

> **Demo:** Timing matrix-vector multiplication and checking $O(n^2)$ growth.
> ```Python
> import time
> import numpy as np
>
> rng = np.random.default_rng(0)
>
> n_values = [400, 800, 1200, 1600, 2000]
> times = []
> for n in n_values:
>     A = rng.standard_normal((n, n))
>     x = rng.standard_normal(n)
>
>     # Repeat to avoid timer-resolution issues.
>     t0 = time.perf_counter()
>     for _ in range(50):
>         A @ x
>     t1 = time.perf_counter()
>     times.append(t1 - t0)
>
> for n, t in zip(n_values, times):
>     print(n, t)
> ```
> The exact timings depend on the machine, but the trend should be close to quadratic for large `n`.

> **Note:** 如果 $t(n)\approx Cn^2$，那么把 $n$ 翻倍时，时间大约会变成原来的 $2^2=4$ 倍.

如果一个算法的运行时间满足 $t(n)=O(n^p)$，那么当 $n$ 足够大时我们可以用 $t\approx Cn^p$ 近似. 对两边取对数，

$$
t\approx Cn^p
\quad\Longleftrightarrow\quad
\log t \approx p\log n + \log C.
$$

因此，在 log-log 坐标上，$\log t$ 对 $\log n$ 应该接近一条斜率为 $p$ 的直线. 这解释了为什么 **log-log 图** 经常被用来 "读出" **渐近阶**.

**#3 解线性系统的复杂度**

回顾用 LU 分解求解 $\mathbf{A}\mathbf{x}=\mathbf{b}$ 的流程 (见 **2-4-LU 分解**)：

1. 分解 $\mathbf{A}=\mathbf{L}\mathbf{U}$.
2. 解下三角系统 $\mathbf{L}\mathbf{z}=\mathbf{b}$ (前代).
3. 解上三角系统 $\mathbf{U}\mathbf{x}=\mathbf{z}$ (回代).

第二与第三步分别是前代与回代 (见 **2-3-线性方程组**). 在 flop 计数的意义下，前代或回代的主开销来自每一步计算一个点积. 把每一层循环的 flops 相加，并用前面的求和渐近公式简化，就得到 $\sim n^2$ 的量级结论.

> **Lemma:** Solving a triangular $n\times n$ system by forward or backward substitution takes $\sim n^2$ flops asymptotically.

> **Note:** 以前代为例，第 $i$ 步需要一个长度为 $(i-1)$ 的点积，约 $2(i-1)$ 个 flops，再加 1 次减法与 1 次除法. 把 $i=1,\dots,n$ 的代价相加，主导项来自 $\sum_{i=1}^{n}2(i-1)\sim n^2$，因此计算量是 $O(n^2)$.

接下来估计 LU 分解本身的 flops. 外积消去的实现可以避免对已知为 0 的位置做乘加. 设 $k$ 是消元步数，从 $k=1$ 到 $k=n-1$，每一步会更新一个大小约为 $(n-k+1)\times(n-k+1)$ 的右下子块.

> **Observation:** The index set $\{k,k+1,\dots,n\}$, with $k\le n$, has $n-k+1$ elements.

在第 $k$ 步中：

- 计算 $\mathbf{L}$ 的第 $k$ 列 (对一个长度为 $n-k+1$ 的向量做标量除法) 需要 $n-k+1$ 个 flops.
- 用外积更新右下子块：外积本身需要 $(n-k+1)^2$ 次乘法；再做一次矩阵相减又需要 $(n-k+1)^2$ 次减法，因此合计 $2(n-k+1)^2$ 个 flops.

因此，LU 分解的总 flops 可以写成

$$
\sum_{k=1}^{n-1} \Bigl((n-k+1) + 2(n-k+1)^2\Bigr),
$$

其主导项来自平方和，因此渐近量级是

$$
\sim \frac{2}{3}n^3,
$$

由前面那几个很像 $\int_0^n x^p\,\mathrm{d}x$ 的式子可以直接看出来.

> **Note:** 如果详细计算的话. 令 $m=n-k+1$，则 $m$ 从 $n$ 递减到 2. 因此
> $$
> \sum_{k=1}^{n-1} (n-k+1)=\sum_{m=2}^{n} m = \frac{n(n+1)}{2}-1,\\
> \sum_{k=1}^{n-1} (n-k+1)^2=\sum_{m=2}^{n} m^2 = \frac{n(n+1)(2n+1)}{6}-1.
> $$
> 把它们代回去，就得到总 flops 等于 $\frac{2}{3}n^3+O(n^2)$.

> **Demo:** Timing LU factorization and comparing to $O(n^3)$.
> ```Python
> import time
> import numpy as np
> from scipy.linalg import lu_factor
>
> rng = np.random.default_rng(0)
> n_values = [200, 400, 600, 800, 1000]
> times = []
> for n in n_values:
>     A = rng.standard_normal((n, n))
>     t0 = time.perf_counter()
>     for _ in range(10):
>         lu_factor(A)
>     t1 = time.perf_counter()
>     times.append(t1 - t0)
>
> for n, t in zip(n_values, times):
>     print(n, t)
> ```
> The log-log slope of `time` versus `n` should trend toward 3.

> **Note:** 如果时间确实接近 $Cn^3$，那么把 $n$ 翻倍时，时间大约会变成原来的 $2^3=8$ 倍. 这也是用少量数据估计阶数的一个方法.

在实践中，flops 并不是唯一占用时间的因素. 但把 flops 当成性能指标是一种很有用的简化. 我们会把它作为后续章节默认的复杂度估计方式：一般的 LU 分解在实际计算时间上大致是 $O(n^3)$ 级别.

这个增长率虽然远好于 $O(2^n)$，但也意味着当 $n$ 大到一定程度 (例如上万级) 时，我们就需要利用更多结构、稀疏性，或采用其他方法，而不是依赖一般的 LU 分解.
