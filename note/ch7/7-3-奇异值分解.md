# 7-3-奇异值分解 (Singular value decomposition)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 奇异值分解的定义**

我们现在引入另一个与特征值分解同等基础的重要分解：奇异值分解 (SVD).

> **Definition:** Singular value decomposition (SVD)
> The **singular value decomposition** of an $m\times n$ matrix $\mathbf{A}$ is
> $$
> \mathbf{A}=\mathbf{U}\mathbf{S}\mathbf{V}^{*},
> $$
> where $\mathbf{U}\in\mathbb{C}^{m\times m}$ and $\mathbf{V}\in\mathbb{C}^{n\times n}$ are unitary and $\mathbf{S}\in\mathbb{R}^{m\times n}$ is real and diagonal with nonnegative elements.
>
> The columns of $\mathbf{U}$ and $\mathbf{V}$ are called left and right singular vectors, respectively. The diagonal elements of $\mathbf{S}$, written $\sigma_1,\dots,\sigma_r$, for $r=\min\{m,n\}$, are called the singular values of $\mathbf{A}$ and are ordered so that
> $$
> \sigma_1\ge \sigma_2 \ge \cdots \ge \sigma_r \ge 0.
> $$
> We call $\sigma_1$ the principal singular value and $\mathbf{u}_1$ and $\mathbf{v}_1$ the principal singular vectors.

每个 $m\times n$ 矩阵都有 SVD. 奇异值是唯一的，但奇异向量不唯一. 如果矩阵是实的，那么上式里的 $\mathbf{U}$ 与 $\mathbf{V}$ 可以取为实正交矩阵.

> **Example:** A simple SVD
> It is easy to check that
> $$
> \begin{bmatrix}3\\4\end{bmatrix}
> =
> \left(\frac{1}{5}\begin{bmatrix}3\\4\end{bmatrix}\right)
> \cdot [5]\cdot [1]
> $$
> meets all the requirements of an SVD. Interpreted as a matrix, the vector $[3,4]$ has the lone singular value $5$.

> **Example:** $\mathbf{A}$ and $\mathbf{A}^{T}$ share singular values (real case)
> Suppose $\mathbf{A}$ is a real matrix and that $\mathbf{A}=\mathbf{U}\mathbf{S}\mathbf{V}^{T}$ is an SVD. Then $\mathbf{A}^{T}=\mathbf{V}\mathbf{S}^{T}\mathbf{U}^{T}$ meets all the requirements of an SVD for $\mathbf{A}^{T}$: the first and last matrices are orthogonal, and the middle matrix is diagonal with nonnegative elements. Hence $\mathbf{A}$ and $\mathbf{A}^{T}$ have the same singular values.

**#2 与特征值分解的联系**

SVD 与 EVD 的核心连接之一是：$\mathbf{A}^{*}\mathbf{A}$ 的特征值与奇异值之间有直接关系.

> **Theorem:** Connection between SVD and EVD
> The nonzero eigenvalues of $\mathbf{A}^{*}\mathbf{A}$ are the squares of the singular values of $\mathbf{A}$.

> **Note:** 直观上，$\mathbf{A}^{*}\mathbf{A}$ 是一个把 "输入空间" 里的方向映射到 "输入空间" 的算子. 它的特征值刻画了各方向上长度变化的平方，而奇异值就是这些长度变化本身.

另一个与 EVD 的联系来自一个更大的对称块矩阵：

$$
\mathbf{C}=
\begin{bmatrix}
\mathbf{0} & \mathbf{A}^{*}\\
\mathbf{A} & \mathbf{0}
\end{bmatrix},
$$

其中 $\mathbf{C}$ 的特征向量会同时包含左右奇异向量的信息.

**#3 如何理解 SVD**

把 $\mathbf{A}=\mathbf{U}\mathbf{S}\mathbf{V}^{*}$ 右乘 $\mathbf{V}$，得到

$$
\mathbf{A}\mathbf{V}=\mathbf{U}\mathbf{S}.
$$

按列来看，这意味着

$$
\mathbf{A}\mathbf{v}_k=\sigma_k \mathbf{u}_k,
\qquad k=1,\dots,r=\min\{m,n\}.
$$

用一句话概括：每个右奇异向量 $\mathbf{v}_k$ 经由 $\mathbf{A}$ 映射后，会变成对应左奇异向量 $\mathbf{u}_k$ 的一个缩放版本，缩放倍数就是奇异值 $\sigma_k$.

SVD 与 EVD 都是 "用少量标量 + 特殊向量" 来描述矩阵. 它们的重要差别在于: SVD 允许定义域与值域使用不同的正交基 (毕竟它们维度可能不同)，因此在两个空间里都能得到正交性；代价是它不再使用同一个基同时描述两边.

**#4 Thin SVD**

在 **3-3-QR 分解** 里我们见过 QR 分解的 full form 与 thin form. SVD 也有类似的 thin form.

设 $\mathbf{A}$ 为 $m\times n$，且 $m>n$，并令 $\mathbf{A}=\mathbf{U}\mathbf{S}\mathbf{V}^{*}$ 为一个 full SVD. 因为 $\mathbf{S}$ 是对角形式，所以它最后 $m-n$ 行全为 0. 因此 $\mathbf{U}\mathbf{S}$ 可以写成

$$
\mathbf{U}\mathbf{S}
=
\begin{bmatrix}\mathbf{u}_1 & \cdots & \mathbf{u}_n & \mathbf{u}_{n+1} & \cdots & \mathbf{u}_m\end{bmatrix}
\begin{bmatrix}
\sigma_1\\
&\ddots\\
&&\sigma_n\\
&&&0\\
&&&&\ddots\\
&&&&&0
\end{bmatrix}
=
\underbrace{\begin{bmatrix}\mathbf{u}_1 & \cdots & \mathbf{u}_n\end{bmatrix}}_{\widehat{\mathbf{U}}}
\underbrace{\begin{bmatrix}
\sigma_1\\
&\ddots\\
&&\sigma_n
\end{bmatrix}}_{\widehat{\mathbf{S}}}.
$$

于是我们得到 thin SVD：

$$
\mathbf{A}=\widehat{\mathbf{U}}\widehat{\mathbf{S}}\mathbf{V}^{*},
$$

其中 $\widehat{\mathbf{U}}$ 是 $m\times n$，$\widehat{\mathbf{S}}$ 是 $n\times n$ 的对角矩阵，而 $\widehat{\mathbf{U}}$ 的列向量是正交归一的 (但它本身不是方阵).

thin form 仍然保留了 SVD 的全部信息：它仍然是一个等式而不是近似. 当 $m\gg n$ 时，thin form 的存储与计算代价明显更低. 对于 "列数多于行数" 的情形，可以通过对 $\mathbf{A}^{*}$ 的 thin SVD 取共轭转置来得到相应的 thin form.

**#5 SVD 与 2-范数**

SVD 与 2-范数关系非常紧密.

> **Theorem:** SVD properties
> Let $\mathbf{A}\in\mathbb{C}^{m\times n}$ have an SVD $\mathbf{A}=\mathbf{U}\mathbf{S}\mathbf{V}^{*}$ in which the singular values are ordered as above. Then:
> 1. The 2-norm satisfies
> $$
> \|\mathbf{A}\|_2=\sigma_1.
> $$
> 2. The rank of $\mathbf{A}$ is the number of nonzero singular values.
> 3. Let $r=\min\{m,n\}$. Then
> $$
> \kappa_2(\mathbf{A})=\|\mathbf{A}\|_2\|\mathbf{A}^{+}\|_2=\frac{\sigma_1}{\sigma_r},
> $$
> where a division by zero implies that $\mathbf{A}$ does not have full rank.

> **Note:** 这里的 $\mathbf{A}^{+}$ 表示 Moore-Penrose 伪逆. 若 $\mathbf{A}=\mathbf{U}\mathbf{S}\mathbf{V}^{*}$，则 $\mathbf{A}^{+}=\mathbf{V}\mathbf{S}^{+}\mathbf{U}^{*}$，其中 $\mathbf{S}^{+}$ 把非零奇异值取倒数 (并把矩阵维度转置). 在 Python/NumPy 里可用 `numpy.linalg.pinv` 计算.

当 $\mathbf{A}$ 为方阵且满秩时，$\mathbf{A}$ 可逆与 "满秩" 等价. 在实际计算里，我们通常用 SVD 来计算矩阵的 2-范数与条件数.

> **Demo:** Verifying SVD properties in Python
> We compute singular values and check the norm and condition number identities.
>
> ```Python
> import numpy as np
>
> A = np.array([[i**j for j in range(4)] for i in range(1, 6)], dtype=float)  # 5x4
> U, s, Vt = np.linalg.svd(A, full_matrices=False)  # thin SVD
> print("singular values:", s)
> print("||A||_2:", np.linalg.norm(A, 2))
> print("sigma_1:", s[0])
>
> # 2-norm condition number.
> print("cond_2(A):", np.linalg.cond(A, 2))
> print("sigma_1/sigma_r:", s[0] / s[-1])
>
> # Orthonormality checks.
> print("||U^T U - I||:", np.linalg.norm(U.T @ U - np.eye(U.shape[1])))
> print("||V V^T - I||:", np.linalg.norm(Vt @ Vt.T - np.eye(Vt.shape[0])))
> ```

> **Note:** `numpy.linalg.svd` 默认返回 thin form (`full_matrices=False`) 更接近教材的展示方式.
