# 7-5-降维 (Dimension reduction)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 低秩近似**

SVD 还有一个重要性质，在很多应用里非常有用.

设 $\mathbf{A}$ 是一个实 $m\times n$ 矩阵，SVD 为 $\mathbf{A}=\mathbf{U}\mathbf{S}\mathbf{V}^{T}$. 为了便于写清楚 thin form，我们暂时假设 $m\ge n$. 此时我们可以把 thin SVD 进一步写成

$$
\mathbf{A}
=
\sum_{i=1}^{r}\sigma_i \mathbf{u}_i\mathbf{v}_i^{T},
$$

其中 $r=\operatorname{rank}(\mathbf{A})$. 上式对 $m<n$ 的情形也成立.

每个外积 $\mathbf{u}_i\mathbf{v}_i^{T}$ 都是一个秩为 1 的矩阵，并且它的矩阵 2-范数为 1. 又因为奇异值满足 $\sigma_1\ge \sigma_2\ge \cdots$，所以上式把 $\mathbf{A}$ 表示为一串 "重要性逐步下降" 的贡献之和.

因此我们自然会定义对任意 $1\le k\le r$，

$$
\mathbf{A}_k
=
\sum_{i=1}^{k}\sigma_i \mathbf{u}_i\mathbf{v}_i^{T}
=
\mathbf{U}_k\mathbf{S}_k\mathbf{V}_k^{T},
$$

其中 $\mathbf{U}_k$ 与 $\mathbf{V}_k$ 分别由 $\mathbf{U}$ 与 $\mathbf{V}$ 的前 $k$ 列组成，而 $\mathbf{S}_k$ 是 $\mathbf{S}$ 的左上角 $k\times k$ 子矩阵.

因为矩阵秩满足 $\operatorname{rank}(\mathbf{X}+\mathbf{Y})\le \operatorname{rank}(\mathbf{X})+\operatorname{rank}(\mathbf{Y})$，所以 $\mathbf{A}_k$ 是 $\mathbf{A}$ 的一个秩为 $k$ 的近似. 更强的是：它在矩阵 2-范数意义下是 "最好" 的秩 $k$ 近似.

> **Theorem:** Best rank-$k$ approximation in the 2-norm
> Suppose $\mathbf{A}$ has rank $r$ and let $\mathbf{A}=\mathbf{U}\mathbf{S}\mathbf{V}^{T}$ be an SVD. Let $\mathbf{A}_k$ be as above for $1\le k < r$. Then
> 1. $\|\mathbf{A}-\mathbf{A}_k\|_2=\sigma_{k+1}$, and
> 2. If the rank of $\mathbf{B}$ is $k$ or less, then $\|\mathbf{A}-\mathbf{B}\|_2\ge \sigma_{k+1}$.

**#2 压缩**

如果 $\mathbf{A}$ 的奇异值下降得足够快，那么对相对较小的 $k$，$\mathbf{A}_k$ 就可能已经捕捉到了矩阵最显著的行为.

> **Demo:** Low-rank SVD approximation for compression
> We compute the SVD of a grayscale image matrix, plot the singular values, and visualize low-rank reconstructions.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Load an example image. If scikit-image is unavailable, fall back to a synthetic one.
> try:
>     from skimage import data
>     img = data.astronaut()  # RGB, uint8
> except Exception:
>     rng = np.random.default_rng(0)
>     img = rng.integers(0, 256, size=(256, 256, 3), dtype=np.uint8)
>
> X = img.astype(float) / 255.0
> A = 0.2126 * X[..., 0] + 0.7152 * X[..., 1] + 0.0722 * X[..., 2]  # grayscale in [0,1]
>
> U, s, Vt = np.linalg.svd(A, full_matrices=False)  # thin SVD
> eps = np.finfo(float).eps
> threshold = eps * s[0]  # eps * ||A||_2 since ||A||_2 = s[0]
> mask = (s < threshold)
> k0 = int(np.argmax(mask)) if mask.any() else len(s)
> print("shape =", A.shape, "  first i with sigma_i < eps||A||_2:", k0 + 1 if mask.any() else None)
>
> # Singular values.
> plt.figure()
> plt.semilogy(np.arange(1, len(s) + 1), s, "o", ms=3)
> plt.axhline(threshold, color="k", ls="--", lw=1, label=r"$\\epsilon_{\\rm mach}\\|A\\|_2$")
> plt.xlabel("i")
> plt.ylabel(r"$\\sigma_i$")
> plt.title("Singular values")
> plt.grid(True, alpha=0.3)
> plt.legend()
> plt.show()
>
> # Low-rank reconstructions.
> ks = [3, 6, 9, 12]
> plt.figure(figsize=(8, 8))
> for j, k in enumerate(ks, start=1):
>     Ak = (U[:, :k] * s[:k]) @ Vt[:k, :]
>     plt.subplot(2, 2, j)
>     plt.imshow(Ak, cmap="gray", vmin=0, vmax=1)
>     plt.axis("off")
>     plt.title(f"rank = {k}")
> plt.suptitle("Low-rank SVD approximations")
> plt.tight_layout()
> plt.show()
>
> # Storage comparison: original has m*n numbers; rank-k uses k(m+n+1).
> m, n = A.shape
> k = 9
> compression = (m * n) / (k * (m + n + 1))
> print("compression ratio (rank-9):", compression)
> ```
>
> The singular values typically decrease until they reach the numerical noise floor (about $\epsilon_{\rm mach}\|A\|_2$). A rapid decrease suggests that low-rank approximations can be quite accurate.

**#3 主要趋势与能量占比**

低秩近似带来的 "降维" 不只是减少计算量. 当我们把矩阵中最重要的贡献单独抽出来时，很多被噪声与弱效应遮蔽的结构会变得更清楚，从而揭示更深层的联系与趋势.

刻画奇异值衰减的一种方式是计算

$$
s_k=\sum_{i=1}^{k}\sigma_i^2,
\qquad
\tau_k=\frac{s_k}{s_r},
\qquad
k=1,\dots,r.
$$

显然 $0\le \tau_k\le 1$，并且 $\tau_k$ 随 $k$ 单调不减. 我们可以把 $\tau_k$ 理解为奇异值序列中 "前 $k$ 个奇异值所包含的能量占比" (在统计语境里也常被解释为方差占比).

> **Demo:** A low-dimensional structure hidden in data
> We use a synthetic "voting record" matrix to illustrate that a data set can be primarily low-dimensional, and that the first two singular directions can capture major trends.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> rng = np.random.default_rng(0)
> n_senator = 100
> n_bill = 300
>
> # Party labels: -1 (blue) vs +1 (red).
> party = rng.choice([-1, 1], size=n_senator)
>
> # Each bill has a partisan direction and strength.
> bill_dir = rng.choice([-1, 1], size=n_bill)
> bill_strength = rng.uniform(0.5, 2.0, size=n_bill)
>
> noise = 0.6 * rng.standard_normal((n_senator, n_bill))
> score = (party[:, None] * bill_dir[None, :] * bill_strength[None, :]) + noise
> A = np.where(score >= 0, 1.0, -1.0)  # +1 = "yea", -1 = "nay"
>
> plt.figure(figsize=(7, 3))
> plt.imshow(A, aspect="auto", cmap="viridis")
> plt.title("Synthetic votes matrix")
> plt.xlabel("bill")
> plt.ylabel("senator")
> plt.colorbar(label="vote")
> plt.tight_layout()
> plt.show()
>
> U, s, Vt = np.linalg.svd(A, full_matrices=False)
> tau = np.cumsum(s**2) / np.sum(s**2)
>
> plt.figure()
> plt.scatter(np.arange(1, 17), tau[:16])
> plt.ylim(0, 1.05)
> plt.xlabel("k")
> plt.ylabel(r"$\\tau_k$")
> plt.title("Fraction of singular value energy")
> plt.grid(True, alpha=0.3)
> plt.show()
>
> print("tau_1 =", tau[0], " tau_2 =", tau[1])
>
> # 2D embedding of senators: project senator row-vectors onto first two right singular directions.
> coords = U[:, :2] * s[:2]
> colors = np.where(party == 1, "tab:red", "tab:blue")
> plt.figure()
> plt.scatter(coords[:, 0], coords[:, 1], c=colors, s=30, alpha=0.85)
> plt.xlabel("coord 1")
> plt.ylabel("coord 2")
> plt.title("2D embedding by first two singular directions")
> plt.grid(True, alpha=0.3)
> plt.show()
> ```
>
> If $\tau_1$ and $\tau_2$ already account for most of the energy, it suggests the data are essentially two-dimensional, and the first two singular directions can reveal the dominant trend.

> **Note:** 上面的 Demo 用合成数据代替真实投票数据. 它要展示的是 "能量占比很集中" 时，前几个奇异方向往往对应可解释的主要结构 (例如两极分化的趋势)，而不是复现某个特定数据集本身.

