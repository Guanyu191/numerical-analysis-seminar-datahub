# 7-2-特征值分解 (Eigenvalue decomposition)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 特征值问题**

到目前为止，我们经常处理线性系统

$$
\mathbf{A}\mathbf{x}=\mathbf{b}.
$$

在线性代数里，与它同等重要的另一个问题是特征值问题.

> **Definition:** Eigenvalue and eigenvector
> Given a square matrix $\mathbf{A}$, if
> $$
> \mathbf{A}\mathbf{x}=\lambda \mathbf{x}
> $$
> for a scalar $\lambda$ and a nonzero vector $\mathbf{x}$, then $\lambda$ is an eigenvalue and $\mathbf{x}$ is an associated eigenvector.

**#2 复矩阵**

一个实矩阵也可能有复特征值. 因此接下来默认允许矩阵、向量、标量都取复数值.

回忆：复数可以写成 $a+ib$，其中 $a,b$ 为实数，且 $i^2=-1$. 复数 $x=a+ib$ 的共轭记为 $\bar{x}$，满足 $\bar{x}=a-ib$. 复数 $z$ 的模长 (magnitude 或 modulus) 定义为

$$
|z|=\sqrt{z\,\bar{z}}.
$$

在复数语境里，很多我们在实矩阵里熟悉的概念会更换名字：

- "转置" 常被 "共轭转置" (adjoint) 取代.
- "对称" 常被 "Hermitian" 取代.
- "正交矩阵" 常被 "酉矩阵" (unitary matrix) 取代.

> **Definition:** Terms for complex matrices
> The adjoint or hermitian of a matrix $\mathbf{A}$ is denoted $\mathbf{A}^{*}$ and is given by $\mathbf{A}^{*}=(\overline{\mathbf{A}})^{T}=\overline{\mathbf{A}^{T}}$. The matrix is self-adjoint or hermitian if $\mathbf{A}^{*}=\mathbf{A}$.
>
> The 2-norm of a complex vector $\mathbf{u}$ is $\|\mathbf{u}\|_2=\sqrt{\mathbf{u}^{*}\mathbf{u}}$. Other vector norms, and all matrix norms, are as defined in **2-7-向量与矩阵范数** (definitions of vector norms and matrix norms).
>
> Complex vectors $\mathbf{u}$ and $\mathbf{v}$ of the same dimension are orthogonal if $\mathbf{u}^{*}\mathbf{v}=0$.
>
> Orthonormal vectors are mutually orthogonal and have unit 2-norm. A unitary matrix is a square matrix with orthonormal columns, or, equivalently, a matrix satisfying $\mathbf{A}^{*}=\mathbf{A}^{-1}$.

> **Note:** 在 Python/NumPy 里，复矩阵的共轭转置通常写作 `A.conj().T`.

**#3 特征值分解**

把特征值定义 $\mathbf{A}\mathbf{x}=\lambda\mathbf{x}$ 改写为

$$
(\mathbf{A}-\lambda \mathbf{I})\mathbf{x}=\mathbf{0},
$$

我们看到 $\mathbf{A}-\lambda \mathbf{I}$ 必然是奇异矩阵，因此其行列式为 0. 这也是手算特征值时最常用的出发点.

> **Example:** A 2-by-2 eigenvalue computation
> Given
> $$
> \mathbf{A}=
> \begin{bmatrix}
> 1 & 1\\
> 4 & 1
> \end{bmatrix},
> $$
> we compute
> $$
> \det(\mathbf{A}-\lambda \mathbf{I})
> =
> \begin{vmatrix}
> 1-\lambda & 1\\
> 4 & 1-\lambda
> \end{vmatrix}
> =
> (1-\lambda)^2-4=\lambda^2-2\lambda-3.
> $$
> The eigenvalues are the roots of this quadratic, $\lambda_1=3$ and $\lambda_2=-1$.

行列式 $\det(\mathbf{A}-\lambda\mathbf{I})$ 称为特征多项式 (characteristic polynomial). 它的根就是特征值. 因此，一个 $n\times n$ 矩阵有 $n$ 个特征值 (按代数重数计数).

假设我们找到了一组特征值与特征向量，使得

$$
\mathbf{A}\mathbf{v}_k=\lambda_k\mathbf{v}_k,\qquad k=1,\dots,n.
$$

把这些式子并排写成矩阵形式，就得到

$$
\mathbf{A}\mathbf{V}=\mathbf{V}\mathbf{D},
$$

其中

$$
\mathbf{V}=
\begin{bmatrix}
\mathbf{v}_1&\mathbf{v}_2&\cdots&\mathbf{v}_n
\end{bmatrix},
\qquad
\mathbf{D}=
\begin{bmatrix}
\lambda_1\\
&\lambda_2\\
&&\ddots\\
&&&\lambda_n
\end{bmatrix}.
$$

如果我们进一步发现 $\mathbf{V}$ 可逆，那么就得到一个关键分解.

> **Definition:** Eigenvalue decomposition (EVD)
> An eigenvalue decomposition (EVD) of a square matrix $\mathbf{A}$ is
> $$
> \mathbf{A}=\mathbf{V}\mathbf{D}\mathbf{V}^{-1}.
> $$
> If $\mathbf{A}$ has an EVD, we say that $\mathbf{A}$ is diagonalizable; otherwise $\mathbf{A}$ is nondiagonalizable (or defective).

需要强调两点：

1. 特征向量并不唯一：如果 $\mathbf{A}\mathbf{v}=\lambda\mathbf{v}$，那么对任意非零标量 $c$ 都有 $\mathbf{A}(c\mathbf{v})=\lambda(c\mathbf{v})$.
2. 不是所有方阵都有 EVD. 例如
   $$
   \mathbf{B}=
   \begin{bmatrix}
   1 & 1\\
   0 & 1
   \end{bmatrix}
   $$
   就是一个不可对角化的简单例子.

下面给出一个常见的充分条件.

> **Theorem:** Diagonalizable matrices with distinct eigenvalues
> If the $n\times n$ matrix $\mathbf{A}$ has $n$ distinct eigenvalues, then $\mathbf{A}$ is diagonalizable.

> **Demo:** Eigenvalues and eigenvectors in Python
> We compute eigenvalues and eigenvectors numerically, and we also illustrate that a defective matrix leads to an ill-conditioned eigenvector matrix.
>
> ```Python
> import numpy as np
>
> # Basic eigenvalue/eigenvector computation.
> A = np.pi * np.ones((2, 2))
> w = np.linalg.eigvals(A)
> w2, V = np.linalg.eig(A)
> print("eigvals(A) =", w)
> print("check ||AV - V diag(w)|| =", np.linalg.norm(A @ V - V @ np.diag(w2)))
>
> # Sorting eigenvalues by different criteria.
> lam = np.array([-2.3, -1.3, -0.3, 0.7, 1.7], dtype=float)
> D = np.diag(lam)
> w = np.linalg.eigvals(D)
> print("sort by real:", w[np.argsort(w.real)])
> print("sort by abs :", w[np.argsort(np.abs(w))])
>
> # A defective (nondiagonalizable) matrix: a 2x2 Jordan block.
> J = np.array([[-1.0, 1.0], [0.0, -1.0]])
> wJ, VJ = np.linalg.eig(J)
> print("eigvals(J) =", wJ)
> print("cond(VJ)   =", np.linalg.cond(VJ))
> print("||JV - V diag(w)|| =", np.linalg.norm(J @ VJ - VJ @ np.diag(wJ)))
> ```

**#4 相似变换与矩阵幂**

EVD 里的关系 $\mathbf{A}=\mathbf{V}\mathbf{D}\mathbf{V}^{-1}$ 是一种特殊的相似变换. 更一般地：

> **Definition:** Similar matrices
> If $\mathbf{S}$ is any nonsingular matrix, we say that $\mathbf{B}=\mathbf{S}\mathbf{A}\mathbf{S}^{-1}$ is a similarity transformation of $\mathbf{A}$, and we say that $\mathbf{B}$ is similar to $\mathbf{A}$.

相似变换不会改变特征值.

> **Theorem:** Eigenvalues are invariant under similarity
> If $\mathbf{S}$ is a nonsingular matrix, then $\mathbf{S}\mathbf{A}\mathbf{S}^{-1}$ has the same eigenvalues as $\mathbf{A}$.

EVD 对矩阵幂特别有用. 例如

$$
\mathbf{A}^2
=
(\mathbf{V}\mathbf{D}\mathbf{V}^{-1})(\mathbf{V}\mathbf{D}\mathbf{V}^{-1})
=
\mathbf{V}\mathbf{D}(\mathbf{V}^{-1}\mathbf{V})\mathbf{D}\mathbf{V}^{-1}
=
\mathbf{V}\mathbf{D}^2\mathbf{V}^{-1}.
$$

重复使用同样的运算，我们得到

$$
\mathbf{A}^k=\mathbf{V}\mathbf{D}^k\mathbf{V}^{-1}.
$$

因为 $\mathbf{D}$ 是对角矩阵，所以 $\mathbf{D}^k$ 只需要把每个对角元素做 $k$ 次方：

$$
\mathbf{D}^k=
\begin{bmatrix}
\lambda_1^k\\
&\lambda_2^k\\
&&\ddots\\
&&&\lambda_n^k
\end{bmatrix}.
$$

类似地，给定一个多项式

$$
p(z)=c_0+c_1 z+\cdots + c_m z^m,
$$

我们可以定义矩阵多项式

$$
p(\mathbf{A})=c_0\mathbf{I}+c_1\mathbf{A}+\cdots+c_m\mathbf{A}^m.
$$

代入 $\mathbf{A}^k=\mathbf{V}\mathbf{D}^k\mathbf{V}^{-1}$ 后，就得到

$$
p(\mathbf{A})
=
\mathbf{V}\,p(\mathbf{D})\,\mathbf{V}^{-1}
=
\mathbf{V}
\begin{bmatrix}
p(\lambda_1)\\
&p(\lambda_2)\\
&&\ddots\\
&&&p(\lambda_n)
\end{bmatrix}
\mathbf{V}^{-1}.
$$

进一步地，如果一个函数 $f$ 可以用 Taylor 多项式在某个区域里逼近，那么我们也能用同样的思路定义 $f(\mathbf{A})$ (把 $p$ 换成 $f$ 的级数逼近).

**#5 特征值的条件性: Bauer-Fike 定理**

正如线性系统有条件数来量化有限精度对解的影响，特征值问题也可能是病态的. 这里我们只使用一个核心结果: Bauer-Fike 定理.

> **Theorem:** Bauer-Fike
> Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be diagonalizable, $\mathbf{A}=\mathbf{V}\mathbf{D}\mathbf{V}^{-1}$, with eigenvalues $\lambda_1,\dots,\lambda_n$. If $\mu$ is an eigenvalue of $\mathbf{A}+\mathbf{E}$ for a complex matrix $\mathbf{E}$, then
> $$
> \min_{j=1,\dots,n}|\mu-\lambda_j|\le \kappa(\mathbf{V})\,\|\mathbf{E}\|,
> $$
> where $\|\cdot\|$ and $\kappa$ are in the 2-norm.

Bauer-Fike 告诉我们：特征值的扰动，最多可能比矩阵本身的扰动大 $\kappa(\mathbf{V})$ 倍. 但这个结论还有一个微妙点：特征向量并不唯一，因此 $\mathbf{V}$ 也不唯一，从而 $\kappa(\mathbf{V})$ 也可能有多个取值. 即便如此，该定理仍然提示我们：当特征向量组成的矩阵 $\mathbf{V}$ 病态时，特征值对扰动会非常敏感.

> **Note:** Bauer-Fike 使用的是绝对差值 $|\mu-\lambda_j|$，不是相对误差. 因此当特征值的尺度很大时，"相对变化很小" 仍可能对应不小的绝对扰动.

极端情形 $\kappa(\mathbf{V})=\infty$ 常被理解为矩阵不可对角化的信号. 另一个有趣的极端是 $\kappa(\mathbf{V})=1$，它意味着 $\mathbf{V}$ 是酉矩阵.

> **Definition:** Normal matrix
> If $\mathbf{A}$ has an EVD $\mathbf{A}=\mathbf{V}\mathbf{D}\mathbf{V}^{-1}$ with a unitary eigenvector matrix $\mathbf{V}$, then $\mathbf{A}$ is a normal matrix.

在 **7-4-对称性与正定性** 中会看到，Hermitian 矩阵与实对称矩阵都是 normal 的. 因为酉矩阵的条件数等于 1，Bauer-Fike 也就给出: normal 矩阵的特征值扰动不超过矩阵扰动的量级.

> **Demo:** Bauer-Fike in action
> We compare a normal (Hermitian) matrix with a highly non-normal triangular matrix.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> rng = np.random.default_rng(0)
>
> # A random Hermitian matrix (normal).
> n = 7
> A = rng.standard_normal((n, n)) + 1j * rng.standard_normal((n, n))
> A = (A + A.conj().T) / 2
> lam, V = np.linalg.eigh(A)  # V is unitary
> print("cond(V) =", np.linalg.cond(V))
>
> dA = rng.standard_normal((n, n)) + 1j * rng.standard_normal((n, n))
> dA = 1e-8 * dA / np.linalg.norm(dA, 2)
> lam_tilde = np.linalg.eigvals(A + dA)
> dist = np.min(np.abs(lam_tilde[:, None] - lam[None, :]), axis=1)
> print("max min-distance =", dist.max())
> print("||dA||_2          =", np.linalg.norm(dA, 2))
>
> # A non-normal triangular matrix.
> n = 20
> x = np.arange(1, n + 1, dtype=float)
> A = np.triu(np.outer(x, np.ones(n)))
> lam, V = np.linalg.eig(A)
> print("cond(V) =", np.linalg.cond(V))
>
> dA = rng.standard_normal((n, n)) + 1j * rng.standard_normal((n, n))
> dA = 1e-8 * dA / np.linalg.norm(dA, 2)
> lam_tilde = np.linalg.eigvals(A + dA)
> dist = np.min(np.abs(lam_tilde[:, None] - lam[None, :]), axis=1)
> BF_bound = np.linalg.cond(V) * np.linalg.norm(dA, 2)
> print("max min-distance =", dist.max())
> print("Bauer-Fike bound =", BF_bound)
>
> # Optional: eigenvalue cloud under many perturbations (single-precision scale).
> # Uncomment to visualize.
> # lam = np.linalg.eigvals(A)
> # pts = []
> # for _ in range(200):
> #     dA = rng.standard_normal((n, n)) + 1j * rng.standard_normal((n, n))
> #     dA = np.finfo(np.float32).eps * dA / np.linalg.norm(dA, 2)
> #     pts.append(np.linalg.eigvals(A + dA))
> # pts = np.concatenate(pts)
> # plt.scatter(lam.real, lam.imag, s=40, c="tab:red", marker="D", label="eig(A)")
> # plt.scatter(pts.real, pts.imag, s=2, c="k", alpha=0.35, label="perturbed")
> # plt.gca().set_aspect("equal", adjustable="box")
> # plt.legend()
> # plt.title("Eigenvalue cloud under small perturbations")
> # plt.show()
> ```
>
> The Bauer-Fike theorem gives a uniform bound. In practice, some eigenvalues can be much more sensitive than others, which the theorem does not explain.

**#6 如何计算 EVD**

在数值算法里，我们不会通过求特征多项式的根来计算特征值. 实用的 EVD 算法超出了本笔记的范围. 这里我们只保留主线直觉: EVD 与矩阵幂的关系非常关键.

如果特征值的模长彼此不同，那么当 $k\to\infty$ 时，$\mathbf{D}^k$ 的对角元素会越来越 "分离"，从而让与特征值相关的结构变得更容易识别. 有一种非常巧妙的办法可以利用这种分离，而不需要显式地计算矩阵幂.

> **Demo:** Francis QR iteration (a glimpse)
> We apply repeated QR factorizations with swapped factors. Each step is a similarity transformation, so eigenvalues are unchanged, but the matrix tends to converge toward an upper-triangular (and for this symmetric case, diagonal) form.
>
> ```Python
> import numpy as np
>
> rng = np.random.default_rng(0)
>
> D = np.diag(np.array([-6.0, -1.0, 2.0, 4.0, 5.0]))
> Q, _ = np.linalg.qr(rng.standard_normal((5, 5)))  # Q is orthogonal
> A = Q @ D @ Q.T
>
> for _ in range(40):
>     Qk, Rk = np.linalg.qr(A)
>     A = Rk @ Qk
>
> print(A)
> ```
>
> This process is known as the Francis QR iteration. It can be formulated as an $O(n^3)$ algorithm for finding the EVD, and it is the foundation of practical eigenvalue routines.

> **Note:** 本节只用它来说明 "相似变换 + 迭代 -> 逼近对角形态" 的主线直觉. 具体实现细节与数值稳定性属于更深入的主题.
