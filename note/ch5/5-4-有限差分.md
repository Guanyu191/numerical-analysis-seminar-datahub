# 5-4-有限差分 (Finite differences)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 有限差分公式**

现在我们讨论插值函数的一个最常见、也最重要的应用：用函数值来近似导数. 因为微分是线性算子，我们只考虑对节点函数值线性的近似公式.

> **Definition:** Finite-difference formula
> A finite-difference formula is a list of values $a_{-p},\ldots,a_q$, called weights, such that for all $f$ in some class of functions,
> $$
> f'(x)\approx \frac{1}{h}\sum_{k=-p}^{q} a_k f(x+kh).
> $$
> The weights are independent of $f$ and $h$. The formula is convergent if the approximation becomes equality in the limit $h\to 0$ for a suitable class of functions.

注意上面的公式虽然是在一个点 $x$ 上近似 $f'(x)$，但它可以对不同的 $x$ 重复使用. 最常见的情形是等距网格

$$
a,\ a+h,\ a+2h,\ \ldots,\ b,
$$

此时每个网格点上的函数值会参与多个不同位置的导数近似. 下面的 Example 会展示这一点.

**#2 常见的一阶差分**

在一阶导数近似里，有三个特别常见的子类.

> **Definition:** Forward, backward, and centered FD formulas
> A forward difference formula is characterized by $p=0$ in the finite-difference formula. A backward difference formula has $q=0$. A centered difference formula has $p=q$.

最简单的前向差分来自导数的极限定义：

$$
f'(x)\approx \frac{f(x+h)-f(x)}{h}.
$$

它对应 $p=0,q=1$，权重为 $a_0=-1,a_1=1$. 类似地，后向差分为

$$
f'(x)\approx \frac{f(x)-f(x-h)}{h},
$$

对应 $p=1,q=0$.

> **Example:** Reusing the same differences on a grid.
> Suppose $f(x)=x^2$, and we take $h=1/4$ over the interval $[0,1]$. This results in the nodes $0,1/4,1/2,3/4,1$. We evaluate $f$ at the nodes to get
> $$
> f(0)=0,\quad f(1/4)=1/16,\quad f(1/2)=1/4,\quad f(3/4)=9/16,\quad f(1)=1.
> $$
> This gives four forward difference estimates,
> $$
> \begin{aligned}
> f'(0) &\approx 4\,(1/16-0),\\
> f'(1/4) &\approx 4\,(1/4-1/16),\\
> f'(1/2) &\approx 4\,(9/16-1/4),\\
> f'(3/4) &\approx 4\,(1-9/16),
> \end{aligned}
> $$
> and we also get four backward difference estimates,
> $$
> \begin{aligned}
> f'(1/4) &\approx 4\,(1/16-0),\\
> f'(1/2) &\approx 4\,(1/4-1/16),\\
> f'(3/4) &\approx 4\,(9/16-1/4),\\
> f'(1) &\approx 4\,(1-9/16).
> \end{aligned}
> $$
> Notice that it is the same four differences each time, but we are interpreting them as derivative estimates at different nodes.

从上面的 Example 可以看出，前向差分与后向差分的主要区别，其实只是我们把

$$
\frac{f(x+h)-f(x)}{h}
$$

理解成 "在左端点估计导数" 还是 "在右端点估计导数". 从对称性出发，一个自然想法是：把它理解成 "在中点估计导数". 这就引出了 centered difference formulas.

下面推导最短的 centered 差分公式，取 $p=q=1$. 为了不影响结论，我们把目标点平移到 $x=0$，于是可用的函数值是 $f(-h),f(0),f(h)$. (上面的前向差分，本质上是穿过 $(0,f(0))$ 与 $(h,f(h))$ 的直线斜率.) 我们可以改为：先用这三个点做二次插值，再对插值多项式求导.

如果用 $Q(x)$ 表示插值 $f(-h),f(0),f(h)$ 的二次多项式，那么

$$
Q(x)
=
\frac{x(x-h)}{2h^2}f(-h)
-\frac{x^2-h^2}{h^2}f(0)
+\frac{x(x+h)}{2h^2}f(h).
$$

从而

$$
f'(0)\approx Q'(0)=\frac{f(h)-f(-h)}{2h}.
$$

这个结果对应 $p=q=1$，权重为 $a_{-1}=-\frac{1}{2}$，$a_0=0$，$a_1=\frac{1}{2}$. 注意虽然推导中用到了 $f(0)$，但最终 $f(0)$ 的权重为 0.

对称性之外，centered 差分相比单边差分还有一个重要优势：在后面的 **5-5-有限差分的收敛性** 中会看到，centered 差分更有利于获得更高的精度阶.

原则上，任何有限差分公式都可以用同一套 "插值-求导" 的思路推导出来. 下面两个表格给出了一些常用结果：一个是 centered 差分，另一个是 forward 差分. 两个表格都对应 "在 $x=0$ 处估计 $f'(0)$". backward 差分可以由 forward 差分得到：把系数顺序反过来，并改变符号.

$$
\begin{array}{c c c c c c c c c c}
\hline
\text{order} & -4h & -3h & -2h & -h & 0 & h & 2h & 3h & 4h\\
\hline
2 & & & & -\frac{1}{2} & 0 & \frac{1}{2} & & &\\
4 & & & \frac{1}{12} & -\frac{2}{3} & 0 & \frac{2}{3} & -\frac{1}{12} & &\\
6 & & -\frac{1}{60} & \frac{3}{20} & -\frac{3}{4} & 0 & \frac{3}{4} & -\frac{3}{20} & \frac{1}{60} &\\
8 & \frac{1}{280} & -\frac{4}{105} & \frac{1}{5} & -\frac{4}{5} & 0 & \frac{4}{5} & -\frac{1}{5} & \frac{4}{105} & -\frac{1}{280}\\
\hline
\end{array}
$$

$$
\begin{array}{c c c c c c}
\hline
\text{order} & 0 & h & 2h & 3h & 4h\\
\hline
1 & -1 & 1 & & &\\
2 & -\frac{3}{2} & 2 & -\frac{1}{2} & &\\
3 & -\frac{11}{6} & 3 & -\frac{3}{2} & \frac{1}{3} &\\
4 & -\frac{25}{12} & 4 & -3 & \frac{4}{3} & -\frac{1}{4}\\
\hline
\end{array}
$$

使用更多函数值的主要动机，是提高精度. 精度用 **order of accuracy** (精度阶) 度量，它会在后面的 **5-5-有限差分的收敛性** 里系统讨论.

> **Example:** Three finite-difference formulas for $f'(0)$.
> According to the tables, here are three specific finite-difference formulas:
> $$
> f'(0)\approx \frac{1}{h}\left[\frac{1}{12}f(-2h)-\frac{2}{3}f(-h)+\frac{2}{3}f(h)-\frac{1}{12}f(2h)\right],
> $$
> $$
> f'(0)\approx \frac{1}{h}\left[-\frac{3}{2}f(0)+2f(h)-\frac{1}{2}f(2h)\right],
> $$
> $$
> f'(0)\approx \frac{1}{h}\left[\frac{1}{2}f(-2h)-2f(-h)+\frac{3}{2}f(0)\right].
> $$

> **Demo:** Comparing FD formulas for $f'(0)$.
> If $f(x)=e^{\sin x}$, then $f'(0)=1$.
> ```Python
> import numpy as np
>
> f = lambda x: np.exp(np.sin(x))
> h = 0.05
>
> # Centered differences (order 2 and 4).
> CD2 = (-f(-h) + f(h)) / (2*h)
> CD4 = (f(-2*h) - 8*f(-h) + 8*f(h) - f(2*h)) / (12*h)
> print("CD2, CD4 =", CD2, CD4)
>
> # Forward differences (order 1 and 2).
> FD1 = (-f(0) + f(h)) / h
> FD2 = (-3*f(0) + 4*f(h) - f(2*h)) / (2*h)
> print("FD1, FD2 =", FD1, FD2)
>
> # Backward differences (derived from forward differences).
> BD1 = (-f(-h) + f(0)) / h
> BD2 = (f(-2*h) - 4*f(-h) + 3*f(0)) / (2*h)
> print("BD1, BD2 =", BD1, BD2)
> ```
> Centered differences are much closer to the exact value than one-sided differences at the same $h$.

**#3 高阶导数的差分公式**

很多应用需要二阶导数. 一个诱人的想法是 "差分的差分". 例如，把前面的 centered 一阶差分公式应用到 $f'$ 上，然后再用 centered 差分去近似出现的 $f'$，可以得到

$$
f''(0)\approx \frac{f(-2h)-2f(0)+f(2h)}{4h^2}.
$$

这是一个有效公式，但它用的是 $\pm 2h$ 处的函数值，而不是更近的 $\pm h$.

更好且更容易推广的做法，是回到上面的二次插值多项式 $Q(x)$，用 $Q''(0)$ 近似 $f''(0)$. 这给出最简单的 centered 二阶差分：

$$
f''(0)\approx \frac{f(-h)-2f(0)+f(h)}{h^2}.
$$

就像一阶导数一样，我们也可以在一般形式里取更大的 $p,q$ 来得到更多公式，例如

$$
f''(0)\approx \frac{f(0)-2f(h)+f(2h)}{h^2},
$$

以及

$$
f''(0)\approx \frac{2f(0)-5f(h)+4f(2h)-f(3h)}{h^2}.
$$

对二阶导数而言，把 forward 差分改成 backward 差分时，只需要把权重顺序反过来，而不需要改变符号.

> **Demo:** Comparing FD formulas for $f''(0)$.
> If $f(x)=e^{\sin x}$, then $f''(0)=1$.
> ```Python
> import numpy as np
>
> f = lambda x: np.exp(np.sin(x))
> h = 0.05
>
> # Centered second difference.
> CD2 = (f(-h) - 2*f(0) + f(h)) / (h**2)
> print("CD2 =", CD2)
>
> # Forward second differences.
> FD1 = (f(0) - 2*f(h) + f(2*h)) / (h**2)
> FD2 = (2*f(0) - 5*f(h) + 4*f(2*h) - f(3*h)) / (h**2)
> print("FD1, FD2 =", FD1, FD2)
>
> # Backward second differences (reverse the forward weights).
> BD1 = (f(-2*h) - 2*f(-h) + f(0)) / (h**2)
> BD2 = (-f(-3*h) + 4*f(-2*h) - 5*f(-h) + 2*f(0)) / (h**2)
> print("BD1, BD2 =", BD1, BD2)
> ```

**#4 任意节点与 Fornberg 权重算法**

等距节点很常见，也很方便，但节点位置也可以是任意的. 此时有限差分公式的一般形式是

$$
f^{(m)}(0)\approx \sum_{k=0}^{r} c_{k,m} f(t_k).
$$

我们不再假设节点等距，因此不再出现统一的 "步长 $h$". 和前面一样，只要做一个平移变换，权重就可以用于其他估计点. 权重依然来自 "插值-求导" 的套路，但代数推导会变得复杂. 一个优雅的递推算法是 Fornberg 算法，它可以为任意给定的节点与导数阶数生成权重. 我们把它作为一个函数直接给出.

> **Function:** fdweights
> **Fornberg's algorithm for finite-difference weights**
> ```Python
> import numpy as np
>
> def fdweights(t, m):
>     """
>     Compute finite-difference weights for the m-th derivative at 0,
>     using function values at nodes in the 1D array t.
>
>     Returns w such that f^{(m)}(0) ≈ sum_k w[k] * f(t[k]).
>     """
>     t = np.asarray(t, dtype=float)
>     if t.ndim != 1:
>         raise ValueError("t must be a 1D array.")
>     n = t.size
>     if n == 0:
>         raise ValueError("t must be nonempty.")
>     if m < 0:
>         raise ValueError("m must be nonnegative.")
>     if m >= n:
>         raise ValueError("need at least m+1 nodes.")
>
>     # Fornberg's algorithm for x0=0 (t is assumed already shifted).
>     C = np.zeros((n, m + 1), dtype=float)
>     C[0, 0] = 1.0
>     c1 = 1.0
>     c4 = t[0]
>     for i in range(1, n):
>         mn = min(i, m)
>         c2 = 1.0
>         c5 = c4
>         c4 = t[i]
>         for j in range(i):
>             c3 = t[i] - t[j]
>             c2 *= c3
>
>             # Update the newest row i when j reaches i-1.
>             if j == i - 1:
>                 for k in range(mn, 0, -1):
>                     C[i, k] = c1 * (k * C[i - 1, k - 1] - c5 * C[i - 1, k]) / c2
>                 C[i, 0] = -c1 * c5 * C[i - 1, 0] / c2
>
>             # Update an old row j.
>             for k in range(mn, 0, -1):
>                 C[j, k] = (c4 * C[j, k] - k * C[j, k - 1]) / c3
>             C[j, 0] = c4 * C[j, 0] / c3
>
>         c1 = c2
>
>     return C[:, m]
> ```

为了用任意节点估计导数，我们需要先把节点平移到 "估计点为 0" 的形式. 例如，下面用五个节点估计 $f(x)=\cos(x^2)$ 在 $x=0.5$ 处的导数. 有限差分公式本质上是 "权重向量" 与 "节点函数值向量" 的内积.

> **Demo:** Finite differences on arbitrary nodes.
> We estimate the derivative of $f(x)=\cos(x^2)$ at $x=0.5$ using five nodes.
> ```Python
> import numpy as np
>
> t = np.array([0.35, 0.50, 0.57, 0.60, 0.75])          # nodes
> f = lambda x: np.cos(x**2)
> dfdx = lambda x: -2*x*np.sin(x**2)
>
> exact_value = dfdx(0.5)
> print("exact =", exact_value)
>
> # Shift so the derivative is estimated at 0.
> w = fdweights(t - 0.5, 1)
> fd_value = float(w @ f(t))
> print("FD =", fd_value)
>
> # Reproduce a one-sided formula at four equally spaced nodes.
> print("weights on [0,1,2,3] =", fdweights(np.arange(0, 4, dtype=float), 1))
> ```

> **Note:** 如果把节点与权重计算都放在有理数域中 (例如用 `fractions.Fraction` 或 `sympy.Rational`)，那么可以得到精确的有理数权重. 但一旦 $f$ 包含 `sin`、`exp` 等超越函数，函数值本身就不再是有理数，因此精确权重不一定带来精确的导数近似.
