# 5-5-有限差分的收敛性 (Convergence of finite differences)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 截断误差与收敛**

在 **5-4-有限差分** 里，我们给出了基于等距节点的有限差分公式，并默认节点间距 $h\to 0$ 时这些公式会收敛. 但要注意：如果我们在区间 $[a,b]$ 上离散一个函数，通常取 $h=(b-a)/n$，这意味着

$$
n=\frac{b-a}{h}=O(h^{-1}).
$$

因此当 $h\to 0$ 时，所需的节点总数会无界增长. 这让我们更关心一个实践问题：在仍能达到可接受精度的前提下，$h$ 能否尽量大.

对形式

$$
f'(x)\approx \frac{1}{h}\sum_{k=-p}^{q} a_k f(x+kh)
$$

的有限差分方法，我们定义截断误差.

> **Definition:** Truncation error of a finite-difference formula
> For the finite-difference method with weights $a_{-p},\ldots,a_q$, the truncation error is
> $$
> \tau_f(h)=f'(0)-\frac{1}{h}\sum_{k=-p}^{q} a_k f(kh).
> $$
> The method is convergent if $\tau_f(h)\to 0$ as $h\to 0$.

虽然这里把误差定义在 $x=0$，但对其他 $x$ 也可以类似定义. 对二阶导数的差分公式，定义会自然地改成用 $f''(0)$ 与相应的权重.

**#2 精度阶**

> **Example:** Truncation error of forward difference.
> The forward difference formula given by $(f(h)-f(0))/h$ yields
> $$
> \begin{aligned}
> \tau_f(h)
> &= f'(0)-\frac{f(h)-f(0)}{h}\\
> &= f'(0)-h^{-1}\bigl[(f(0)+hf'(0)+\tfrac{1}{2}h^2f''(0)+\cdots)-f(0)\bigr]\\
> &= -\tfrac{1}{2}h f''(0)+O(h^2).
> \end{aligned}
> $$
> The primary conclusion is that the truncation error is $O(h)$ as $h\to 0$.

在收敛方法里，我们关心 $\tau_f\to 0$ 的速度，也就是误差随 $h$ 变小的衰减速率.

> **Definition:** Order of accuracy of a finite-difference formula
> If the truncation error of a finite-difference formula satisfies $\tau_f(h)=O(h^m)$ for a positive integer $m$, then $m$ is the order of accuracy of the formula.

因此，上面的前向差分是 1 阶精度 (first-order accurate). 在其他条件相同的情况下，更高的精度阶更好，因为 $O(h^m)$ 随 $h\to 0$ 的消失速度更快. 一般来说，在有限差分公式中包含更多函数值 (也就是在 **5-4-有限差分** 的一般形式里增加权重个数) 会提高精度阶，这一点可以从那一节给出的表格中直接看到.

精度阶通常通过 Taylor 展开计算：把 $\tau_f(h)$ 中出现的 $f(kh)$ 都在 $h=0$ 附近展开，然后只保留最低阶的非零项.

> **Example:** Truncation error of centered difference.
> We compute the truncation error of the centered difference formula $(f(h)-f(-h))/(2h)$:
> $$
> \begin{aligned}
> \tau_f(h)
> &= f'(0)-\frac{f(h)-f(-h)}{2h}\\
> &= f'(0)-(2h)^{-1}\Bigl[(f(0)+hf'(0)+\tfrac{1}{2}h^2f''(0)+\tfrac{1}{6}h^3f'''(0)+O(h^4))\\
> &\qquad\qquad\ \ -(f(0)-hf'(0)+\tfrac{1}{2}h^2f''(0)-\tfrac{1}{6}h^3f'''(0)+O(h^4))\Bigr]\\
> &= -(2h)^{-1}\bigl[\tfrac{1}{3}h^3 f'''(0)+O(h^4)\bigr]
> = O(h^2).
> \end{aligned}
> $$
> Thus, this method has order of accuracy equal to 2.

> **Demo:** Observing convergence rates on a log-log plot.
> We apply the forward and centered formulas to $f(x)=\sin(e^{x+1})$ at $x=0$.
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> f = lambda x: np.sin(np.exp(x + 1.0))
> exact_value = np.exp(1.0) * np.cos(np.exp(1.0))
>
> hs = np.array([5 * 10**(-n) for n in range(1, 7)], dtype=float)
> FD1 = np.array([(f(h) - f(0.0)) / h for h in hs])
> FD2 = np.array([(f(h) - f(-h)) / (2*h) for h in hs])
>
> err_FD1 = exact_value - FD1
> err_FD2 = exact_value - FD2
>
> for h, e1, e2 in zip(hs, err_FD1, err_FD2):
>     print(h, e1, e2)
>
> plt.loglog(hs, np.abs(err_FD1), "o-", label="FD1")
> plt.loglog(hs, np.abs(err_FD2), "o-", label="FD2")
> plt.loglog(hs, hs, "--", label=r"$O(h)$")
> plt.loglog(hs, hs**2, "--", label=r"$O(h^2)$")
> plt.gca().invert_xaxis()   # conventional: h decreases left-to-right
> plt.xlabel("h")
> plt.ylabel("error")
> plt.title("Convergence of finite differences")
> plt.grid(True, which="both")
> plt.legend()
> plt.show()
> ```
> On a log-log plot, the error curves are expected to look like straight lines with slopes matching their orders of accuracy.

**#3 稳定性：舍入误差与最优步长**

截断误差的主导项通常是 $O(h^m)$，因此当 $h\to 0$ 时它会下降. 但到目前为止我们还没有考虑舍入误差. 为了尽量简单，我们先看前向差分

$$
\delta(h)=\frac{f(x+h)-f(x)}{h}.
$$

当 $h\to 0$ 时，分子 $f(x+h)-f(x)$ 会趋近于 0，但 $f(x+h)$ 与 $f(x)$ 的数值本身并不一定接近 0. 这正是产生消去误差的典型情形. 实际上，有限差分公式在 $h\to 0$ 时本质上是病态的.

更具体地说，回忆计算差值 $f(x+h)-f(x)$ 的问题，其条件数为

$$
\kappa(h)=\frac{\max\{|f(x+h)|,\ |f(x)|\}}{|f(x+h)-f(x)|},
$$

这意味着差值的计算会带来大约 $\kappa(h)\epsilon_{\rm mach}$ 量级的相对误差. 因此我们实际算到的差分值可以写成

$$
\tilde{\delta}(h)
=
\frac{f(x+h)-f(x)}{h}\bigl(1+\kappa(h)\epsilon_{\rm mach}\bigr).
$$

当 $h\to 0$ 时，有

$$
|\tilde{\delta}(h)-\delta(h)|
=
\frac{\max\{|f(x+h)|,\ |f(x)|\}}{h}\epsilon_{\rm mach}
\sim
|f(x)|\,\epsilon_{\rm mach}\,h^{-1}.
$$

把截断误差与舍入误差合在一起，得到

$$
\bigl|f'(x)-\tilde{\delta}(h)\bigr|
\le
|\tau_f(h)|+|f(x)|\,\epsilon_{\rm mach}\,h^{-1}.
$$

这个不等式揭示了一个核心现象：当 $h$ 变小时，截断误差会下降，但由于消去误差，舍入误差项反而会增长. 因此存在一个 "最优" 的 $h$，使得两类误差贡献大致相当.

对一阶方法来说，把 $|f(x)|\epsilon_{\rm mach}h^{-1}$ 与 $Ch$ 量级相平衡，会得到

$$
h\approx K\sqrt{\epsilon_{\rm mach}},
$$

其中常数 $K$ 依赖于 $x$ 与 $f$，但不依赖于 $h$. 这也解释了 **4-6-拟 Newton 方法** 里有限差分步长 $\delta$ 选择为 $\sqrt{\epsilon_{\rm mach}}$ 量级的原因.

当截断误差阶数为 $m$ 时，结论会推广为下面的 Observation.

> **Observation:** Optimal node spacing with roundoff
> For computing with a finite-difference method of order $m$ in the presence of roundoff, the optimal spacing of nodes satisfies
> $$
> h_{\rm opt}\approx \epsilon_{\rm mach}^{\,1/(m+1)},
> $$
> and the optimum total error is roughly $\epsilon_{\rm mach}^{\,m/(m+1)}$.

换一种说法：对一阶差分公式，我们最多只能期待大约 "半数" 的机器有效数字；随着 $m$ 增大，我们更接近用满机器精度. 更高阶的有限差分方法通常既更高效，也更不容易被舍入误差破坏.

> **Demo:** FD error with roundoff.
> We apply first-, second-, and fourth-order formulas to estimate $f'(0)$ for $f(x)=e^{-1.3x}$.
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> f = lambda x: np.exp(-1.3 * x)
> exact = -1.3
>
> hs = np.array([10.0**(-n) for n in range(1, 13)], dtype=float)
> FD1, FD2, FD4 = [], [], []
> for h in hs:
>     nodes = h * np.arange(-2, 3)
>     vals = f(nodes)
>     FD1.append(np.dot([0, 0, -1, 1, 0] / h, vals))
>     FD2.append(np.dot([0, -0.5, 0, 0.5, 0] / h, vals))
>     FD4.append(np.dot([1/12, -2/3, 0, 2/3, -1/12] / h, vals))
>
> FD1 = np.array(FD1); FD2 = np.array(FD2); FD4 = np.array(FD4)
> err = np.abs(np.vstack([FD1, FD2, FD4]).T - exact)
>
> plt.loglog(hs, err[:, 0], "o-", label="FD1")
> plt.loglog(hs, err[:, 1], "o-", label="FD2")
> plt.loglog(hs, err[:, 2], "o-", label="FD4")
>
> # A reference line for the roundoff-dominated regime.
> plt.loglog(hs, 0.1 * np.finfo(float).eps / hs, "--", color="k", label=r"$O(h^{-1})$")
>
> plt.gca().invert_xaxis()
> plt.xlabel("h")
> plt.ylabel("error")
> plt.title("FD error with roundoff")
> plt.grid(True, which="both")
> plt.legend()
> plt.show()
> ```
> The errors decrease at first (truncation error dominates) and then increase (roundoff dominates) as $h$ becomes extremely small.
