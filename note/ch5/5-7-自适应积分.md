# 5-7-自适应积分 (Adaptive integration)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 为什么需要自适应**

到目前为止，我们只用等距节点来计算积分. 但在一些问题里，显然应该使用非均匀分布的节点. 例如，如果被积函数在某个子区间里变化更剧烈 (更振荡、更尖锐)，那么把更多节点放在该子区间会更合理.

> **Demo:** Why uniform nodes can be inefficient.
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
> import warnings
> from scipy.integrate import quad, IntegrationWarning
>
> warnings.filterwarnings("ignore", category=IntegrationWarning)
>
> def f(x):
>     x = np.asarray(x)
>     return (x + 1.0)**2 * np.cos((2.0*x + 1.0) / (x - 4.3))
>
> def trapezoid(f, a, b, n):
>     x = np.linspace(a, b, n + 1)
>     y = f(x)
>     h = (b - a) / n
>     return h * (0.5*y[0] + y[1:-1].sum() + 0.5*y[-1])
>
> # The integrand becomes increasingly oscillatory as x increases.
> xplot = np.linspace(0.0, 4.0, 3000)
> plt.plot(xplot, f(xplot))
> plt.xlabel("x")
> plt.ylabel("f(x)")
> plt.title("An oscillatory integrand")
> plt.grid(True)
> plt.show()
>
> # Compare trapezoid accuracy on [0,2] and [2,4].
> left_val, _ = quad(lambda x: float(f(x)), 0.0, 2.0, epsabs=1e-14, epsrel=1e-14)
> right_val, _ = quad(lambda x: float(f(x)), 2.0, 4.0, epsabs=1e-14, epsrel=1e-14)
>
> ns = [50 * (2**k) for k in range(4)]
> for n in ns:
>     left_T = trapezoid(f, 0.0, 2.0, n)
>     right_T = trapezoid(f, 2.0, 4.0, n)
>     print("n =", n, "left error =", left_T - left_val, "right error =", right_T - right_val)
> ```
> The trapezoid rule is noticeably more accurate on the left half than on the right half for the same number of nodes.

上面的现象提示：我们希望有一种算法，能够自动检测 "哪里更难" 并据此调整节点分布. 这种特性叫 **adaptivity** (自适应性).

**#2 误差估计**

理想情况下，我们希望根据积分结果的误差来决定是否需要加密节点. 当然，若能精确知道误差，就等价于知道精确积分值. 但我们可以借助外推思想来估计误差.

考虑 Simpson 公式：它可以由两次梯形估计做一次外推得到. 记 $T_f(n)$ 为在区间上用 $n$ 个等长小区间的梯形公式近似值，那么

$$
S_f(2n)=\frac{1}{3}\bigl[4T_f(2n)-T_f(n)\bigr].
$$

我们期望它具有四阶精度，也就是

$$
\int_a^b f(x)\,dx = S_f(2n) + O(n^{-4}).
$$

继续外推一次，可以得到六阶精度的近似

$$
R_f(4n)=\frac{1}{15}\bigl[16S_f(4n)-S_f(2n)\bigr].
$$

由于精度阶更高，$R_f(4n)$ 应当比 $S_f(4n)$ 更准确. 因此，我们可以用它们的差来估计误差. 令

$$
E = R_f(4n)-S_f(4n)=\frac{S_f(4n)-S_f(2n)}{15}.
$$

这会给出一个对 $S_f(4n)$ 误差的合理估计.

**#3 分治思想与停止准则**

如果 $|E|$ 足够小，那么我们就可以停止. 但 "足够小" 的判断需要谨慎.

例如，如果精确积分值是 $10^{20}$，那么要求 $|E|<\delta\ll 1$ 在双精度下没有意义，因为这相当于要求超过 20 位有效数字. 反过来，如果积分值非常小 (可能由消去得到)，严格要求相对误差也并不现实. 实践中更合理的做法是：误差相对 "数据" (也就是被积函数的典型量级) 足够小，但不要求相对答案本身足够小.

因此通常同时使用绝对与相对误差容忍度，当满足其中之一时停止. 一个常用的代数形式是

$$
|E| < \delta_a + \delta_r |S_f(n)|,
$$

其中 $\delta_a$ 与 $\delta_r$ 分别是绝对与相对容忍度.

当误差估计未通过上述测试时，我们把区间 $[a,b]$ 二分，并利用恒等式

$$
\int_a^b f(x)\,dx = \int_a^{(a+b)/2} f(x)\,dx + \int_{(a+b)/2}^b f(x)\,dx
$$

分别计算两个半区间上的积分近似. 每个半区间都会递归地应用 Simpson 公式与误差估计准则，必要时继续二分. 在计算机科学里，这类策略叫 divide and conquer：把问题递归地拆成更容易的子问题，然后把结果拼起来.

**#4 一个实现**

在实现自适应积分时，通常只用最基本的 Simpson 值 $S_f(4)$ 以及它的误差估计 $E$ 来做决策. 计算 $S_f(4)$ 需要三个梯形估计 $T_f(1),T_f(2),T_f(4)$. 对 $T_f(4)$ 来说，被积函数只在 5 个节点上被评估；而这 5 个评估值已经足够复用来计算 $T_f(1)$ 与 $T_f(2)$.

下面的函数展示了如何把误差估计与递归二分结合起来. 它除了返回积分近似值，还返回自适应过程中实际使用到的所有节点，便于可视化节点分布.

> **Function:** intadapt
> **Adaptive integration with error estimation**
> ```Python
> import numpy as np
>
> def intadapt(f, a, b, tol, fa=None, fb=None, m=None, fm=None):
>     """
>     Adaptively integrate f over [a,b] to within target error tolerance tol.
>     Returns (Q, nodes) where nodes are the evaluation points used.
>     """
>     if fa is None:
>         fa = float(f(a))
>     if fb is None:
>         fb = float(f(b))
>     if m is None:
>         m = 0.5 * (a + b)
>     if fm is None:
>         fm = float(f(m))
>
>     # Two new nodes and their function values.
>     xl = 0.5 * (a + m)
>     xr = 0.5 * (m + b)
>     fl = float(f(xl))
>     fr = float(f(xr))
>
>     # Trapezoid values at n=1,2,4 subintervals (reuse evaluations).
>     h = b - a
>     T1 = h * (fa + fb) / 2.0
>     T2 = T1 / 2.0 + (h / 2.0) * fm
>     T4 = T2 / 2.0 + (h / 4.0) * (fl + fr)
>
>     # Simpson values and an error estimate.
>     S2 = (4.0 * T2 - T1) / 3.0
>     S4 = (4.0 * T4 - T2) / 3.0
>     E = (S4 - S2) / 15.0
>
>     # Combined absolute/relative tolerance: |E| < tol * (1 + |S4|).
>     if abs(E) < tol * (1.0 + abs(S4)):
>         Q = S4
>         nodes = [a, xl, m, xr, b]
>     else:
>         QL, tL = intadapt(f, a, m, tol, fa=fa, fb=fm, m=xl, fm=fl)
>         QR, tR = intadapt(f, m, b, tol, fa=fm, fb=fb, m=xr, fm=fr)
>         Q = QL + QR
>         nodes = tL + tR[1:]  # merge without duplicating the midpoint
>
>     return Q, nodes
> ```

> **Demo:** Adaptive node selection.
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
> import warnings
> from scipy.integrate import quad, IntegrationWarning
>
> warnings.filterwarnings("ignore", category=IntegrationWarning)
>
> def f(x):
>     x = np.asarray(x)
>     return (x + 1.0)**2 * np.cos((2.0*x + 1.0) / (x - 4.3))
>
> A, t = intadapt(f, 0.0, 4.0, tol=1e-3)
> t = np.array(t)
> print("num_nodes =", t.size)
>
> Q, _ = quad(lambda x: float(f(x)), 0.0, 4.0, epsabs=1e-14, epsrel=1e-14)
> print("error =", Q - A)
>
> xplot = np.linspace(0.0, 4.0, 3000)
> plt.plot(xplot, f(xplot), color="k", label="integrand")
> plt.stem(t, f(t), linefmt="C3-", markerfmt=" ", basefmt=" ", label="nodes")
> plt.xlabel("x")
> plt.ylabel("f(x)")
> plt.title("Adaptive node selection")
> plt.grid(True)
> plt.legend()
> plt.show()
> ```
> The nodes tend to concentrate where the integrand varies more rapidly.

> **Demo:** Convergence of adaptive integration.
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
> import warnings
> from scipy.integrate import quad, IntegrationWarning
>
> warnings.filterwarnings("ignore", category=IntegrationWarning)
>
> def f(x):
>     x = np.asarray(x)
>     return (x + 1.0)**2 * np.cos((2.0*x + 1.0) / (x - 4.3))
>
> Q, _ = quad(lambda x: float(f(x)), 0.0, 4.0, epsabs=1e-14, epsrel=1e-14)
>
> tols = 10.0 ** (-np.arange(4, 15, dtype=float))  # 1e-4,...,1e-14
> errs = []
> ns = []
> for tol in tols:
>     A, t = intadapt(f, 0.0, 4.0, tol=float(tol))
>     errs.append(A - Q)
>     ns.append(len(t))
>
> errs = np.array(errs)
> ns = np.array(ns)
> for tol, e, n in zip(tols, errs, ns):
>     print(tol, e, n)
>
> plt.loglog(ns, np.abs(errs), "o-", label="results")
> ref = 0.01 * (ns / ns[0]) ** (-4)
> plt.loglog(ns, ref, "--", label=r"$O(n^{-4})$")
> plt.xlabel("number of nodes")
> plt.ylabel("error")
> plt.title("Convergence of adaptive integration")
> plt.grid(True, which="both")
> plt.legend()
> plt.show()
> ```
> The observed convergence in terms of chosen nodes can be close to fourth order, but the error estimates are not guaranteed to be perfect.

自适应与误差估计非常强大，但也有代价. 误差估计不可能在所有情况下都完美：有时结果不如请求精度准确，有时函数评估次数多于必要. 当积分是更大计算过程里的一个步骤时，还可能出现更微妙的问题.
