# 5-6-数值积分 (Numerical integration)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 定积分是一个数值量**

在微积分里，我们常用 "先找原函数，再用微积分基本定理" 的方式计算定积分. 这种联系如此深刻，以至于我们很容易忽略：定积分本身就是一个独立的数值量，并不依赖于我们能否写出可用的原函数.

事实上，大多数函数在熟悉的初等函数范围内都没有原函数表达式. 在这种情形下，数值积分 (也常用旧名 **quadrature**) 就成为默认手段.

> **Demo:** Numerical integration vs antiderivatives
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.integrate import quad
>
> exact = np.e - 1.0
> Q, est = quad(lambda x: np.exp(x), 0.0, 1.0, epsabs=1e-14, epsrel=1e-14)
> print("Integral exp(x) on [0,1]:", exact, Q)
>
> Q2, est2 = quad(lambda x: np.exp(np.sin(x)), 0.0, 1.0, epsabs=1e-14, epsrel=1e-14)
> print("Integral exp(sin(x)) on [0,1]:", Q2)
>
> xs = np.linspace(0.0, 1.0, 600)
> fig, ax = plt.subplots(2, 1, sharex=True)
> ax[0].fill_between(xs, 0.0, np.exp(xs))
> ax[0].set_ylabel(r"$e^x$")
> ax[1].fill_between(xs, 0.0, np.exp(np.sin(xs)))
> ax[1].set_ylabel(r"$e^{\\sin(x)}$")
> ax[1].set_xlabel("x")
> plt.show()
> ```
> Numerically, both integrals are essentially the same kind of problem: approximate the area under a curve.

**#2 数值积分公式：节点与权重**

本节只讨论等距节点情形. 给定区间 $[a,b]$，令

$$
t_i=a+ih,\qquad h=\frac{b-a}{n},\qquad i=0,\dots,n.
$$

一个数值积分公式由一组权重 $w_0,\dots,w_n$ 构成，使得对某类函数 $f$，

> **Definition:** Numerical integration formula
> A numerical integration formula is a list of weights $w_0,\dots,w_n$ chosen so that
> $$
> \int_a^b f(x)\,dx \approx h\sum_{i=0}^n w_i f(t_i).
> $$
> The weights are independent of $f$, $a$, and $b$ (aside from the use of $h$ and the nodes $t_i$).

这种公式也能直接作用在一列数据值上，即使它们不是由某个显式函数生成的. 但在我们的讨论与实现里，默认 $f$ 可以在任意点被评估.

一个常见推导方式是模仿有限差分：先找到某个插值函数，然后对插值函数做 "精确积分"，从而得到近似积分公式.

**#3 梯形公式：积分分段线性插值**

最重要的积分公式之一来自对 **5-2-分段线性插值** 的积分：把 $f$ 用分段线性插值函数近似，然后对折线的面积做精确计算.

对等距节点，梯形公式可以写成

$$
T_f(n)
=
h\left[\frac{1}{2}f(t_0)+f(t_1)+\cdots+f(t_{n-1})+\frac{1}{2}f(t_n)\right].
$$

> **Definition:** Trapezoid formula
> The trapezoid formula is a numerical integration formula with weights
> $$
> w_i=
> \begin{cases}
> \frac{1}{2}, & i=0 \text{ or } i=n,\\
> 1, & 0<i<n.
> \end{cases}
> $$

几何上，它把每个小区间上的曲线下方面积用一个梯形面积替代，然后求和. 由于简单、通用、实现短小，梯形公式常被称为积分公式里的 "Swiss Army knife".

> **Function:** trapezoid
> **Apply the trapezoid formula**
> ```Python
> import numpy as np
>
> def trapezoid(f, a, b, n):
>     """
>     trapezoid(f, a, b, n)
>
>     Apply the trapezoid formula for integrand f over [a,b], broken into n equal pieces.
>     Returns: (T, t, y), where t are the nodes and y=f(t).
>     """
>     a = float(a)
>     b = float(b)
>     n = int(n)
>     h = (b - a) / n
>     t = np.linspace(a, b, n + 1)
>     y = np.asarray(f(t), dtype=float)
>     T = h * (0.5 * y[0] + y[1:-1].sum() + 0.5 * y[-1])
>     return T, t, y
> ```

**#4 截断误差、Euler-Maclaurin 公式与二阶精度**

对一般积分公式

$$
h\sum_{i=0}^n w_i f(t_i),
$$

其截断误差定义为

> **Definition:** Truncation error of a numerical integration formula
> $$
> \tau_f(h)=\int_a^b f(x)\,dx - h\sum_{i=0}^n w_i f(t_i).
> $$

对梯形公式来说，我们可以直接用分段线性插值误差界 (**5-2-分段线性插值** 的收敛定理) 推出它是二阶精度：插值误差是 $O(h^2)$，积分后的误差仍然是 $O(h^2)$.

更精细的误差展开由 Euler-Maclaurin 公式给出：

$$
\int_a^b f(x)\,dx
=
T_f(n)
-\frac{h^2}{12}\bigl[f'(b)-f'(a)\bigr]
+\frac{h^4}{720}\bigl[f^{(3)}(b)-f^{(3)}(a)\bigr]
+O(h^6).
$$

$$
=
T_f(n)-\sum_{k=1}^{\infty}\frac{B_{2k}h^{2k}}{(2k)!}\bigl[f^{(2k-1)}(b)-f^{(2k-1)}(a)\bigr],
$$

其中 $B_{2k}$ 是 Bernoulli numbers.

> **Observation:** Trapezoid accuracy
> Unless we happen to have $f'(b)=f'(a)$ (so that the $h^2$ term cancels), we should expect second-order truncation error and no better. The trapezoid integration formula is second-order accurate.

> **Demo:** Convergence of trapezoidal integration
> We approximate $\int_0^2 e^{\\sin(7x)}\,dx$ and verify the expected second-order behavior.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.integrate import quad
>
> f = lambda x: np.exp(np.sin(7 * x))
> a, b = 0.0, 2.0
>
> Q, _ = quad(lambda x: float(f(x)), a, b, epsabs=1e-14, epsrel=1e-14)
> print("Reference integral =", Q)
>
> ns = np.array([10, 100, 1000, 10000, 100000], dtype=int)
> err = []
> for n in ns:
>     T, _, _ = trapezoid(f, a, b, n)
>     err.append(Q - T)
> err = np.array(err)
>
> plt.loglog(ns, np.abs(err), marker="o", label="results")
> plt.loglog(ns, 3e-3 * (ns / ns[0])**(-2), ls="--", label=r"$O(n^{-2})$")
> plt.xlabel("n")
> plt.ylabel("error")
> plt.title("Convergence of trapezoidal integration")
> plt.grid(True, which="both", alpha=0.3)
> plt.legend()
> plt.show()
> ```
> Each factor of 10 in $n$ typically reduces the error by about $10^2$, consistent with second-order accuracy.

**#5 外推：从梯形到 Simpson，再到更高阶**

当 $f$ 的函数值评估很昂贵时，我们希望用同样的采样信息获得更高精度. 一条典型路径是利用误差结构做 extrapolation.

设某个目标量 $A_0$ 被算法 $A(h)$ 近似，并且误差具有渐近展开

$$
A_0 = A(h) + c_1 h + c_2 h^2 + c_3 h^3 + \cdots,
$$

关键在于：我们不需要知道 $c_i$ 的具体值，只要知道它们存在并且与 $h$ 无关.

对梯形公式，Euler-Maclaurin 说明误差只有偶次幂项. 用 $I$ 表示精确积分，则可以写成 (把 $h$ 改写成 $n$ 也更方便)

$$
I=T_f(n)+c_2 n^{-2}+c_4 n^{-4}+\cdots,
$$

并且

$$
I=T_f(2n)+\frac{1}{4}c_2 n^{-2}+\frac{1}{16}c_4 n^{-4}+\cdots.
$$

因此我们可以线性组合两式来消去 $n^{-2}$ 项，得到 Simpson 公式：

> **Definition:** Simpson's formula
> $$
> S_f(2n)=\frac{1}{3}\bigl[4T_f(2n)-T_f(n)\bigr].
> $$

它满足 $I=S_f(2n)+O(n^{-4})$，即四阶精度. 由于新的误差展开仍然具有相同形式，我们还可以再外推一次得到六阶精度：

> **Definition:** Sixth-order extrapolation
> $$
> R_f(4n)=\frac{1}{15}\bigl[16S_f(4n)-S_f(2n)\bigr].
> $$

继续重复就是 Romberg integration 的基本思想.

**#6 节点加倍：复用函数值**

注意上面的外推依赖于 $T_f(n),T_f(2n),T_f(4n),\dots$ 这样按 2 倍加密节点的序列. 这带来一个直接收益：从 $n$ 加倍到 $2n$ 时，只有一半节点是新的中点，旧节点上的 $f$ 值可以复用.

把 $n=2m$，可以推导出一个更新公式：$T_f(2m)$ 可以由 $T_f(m)$ 加上新中点处的函数值求得.

> **Demo:** Extrapolation with node doubling
> We estimate $\int_0^2 x^2 e^{-2x}\,dx$ using $T$, then Simpson ($S$), then a sixth-order extrapolation ($R$), reusing function values when doubling nodes.
>
> ```Python
> import numpy as np
> from scipy.integrate import quad
>
> f = lambda x: x**2 * np.exp(-2 * x)
> a, b = 0.0, 2.0
> Q, _ = quad(lambda x: float(f(x)), a, b, epsabs=1e-14, epsrel=1e-14)
> print("Reference integral =", Q)
>
> N = 20
> n = N
> h = (b - a) / n
> t = a + h * np.arange(n + 1)
> y = f(t)
>
> T = [h * (0.5 * y[0] + y[1:-1].sum() + 0.5 * y[-1])]
>
> # Double nodes twice, reusing old values: only evaluate at new midpoints.
> for _ in range(2):
>     n = 2 * n
>     h = h / 2
>     t_mid = a + h * np.arange(1, n, 2)    # odd indices
>     T.append(0.5 * T[-1] + h * f(t_mid).sum())
>
> # Simpson extrapolation from T.
> S = [(4 * T[i+1] - T[i]) / 3 for i in range(len(T) - 1)]
> # Sixth-order extrapolation from S.
> R = (16 * S[1] - S[0]) / 15
>
> print("T errors:", [Ti - Q for Ti in T])
> print("S errors:", [Si - Q for Si in S])
> print("R error :", R - Q)
> ```
> The extrapolated results typically achieve much smaller error than the best trapezoid result, at almost no extra cost beyond reusing function evaluations.
