# 1-1-浮点数 (Floating-point numbers)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记比较特殊，主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 实数的 "连续无限" 与浮点数集合**

实数集 $\mathbb{R}$ 的 "无限" 有两种：无界、连续. 在多数实际计算中，连续性带来的挑战通常更关键，因此我们先从连续性开始.

> **Definition:** **Floating-point numbers.**
> The set $\mathbb{F}$ of **floating-point numbers** consists of zero and all numbers of the form
> $$
> \pm(1+f)\times 2^n,
> $$
> where $n$ is an integer called the **exponent**, and $1+f$ is the **mantissa** or **significand**, in which
> $$
> f=\sum_{i=1}^{d} b_i 2^{-i}, \qquad b_i\in\{0,1\},
> $$
> for a fixed integer $d$ called the binary **precision**.

> **Note:** 为了把这个定义写得更具体，我们取二进制精度 $d=2$，指数 $n=4$，并令 $b_1=0$，$b_2=1$. 则
> $$
> f=b_1 2^{-1}+b_2 2^{-2}=0\cdot 2^{-1}+1\cdot 2^{-2}=2^{-2}=0.01_2,
> $$
> 因而 $1+f=1.01_2$，对应的浮点数可以写成 $\pm(1.01_2)\times 2^4$. 改变 $n$ 会整体缩放数值大小 (例如把 $n=4$ 改成 $n=3$ 相当于除以 2)，因此 $n$ 主要控制数量级范围. 增大 $d$ 会让 $f$ 拥有更多二进制位，从而表达更细的间隔，因此 $d$ 主要控制精度.

上面的 $f$ 的表示把有效数 $1+f$ 写成区间 $[1,2)$ 中的二进制形式. 等价地，令

$$
z=\sum_{i=1}^{d} b_i 2^{d-i}\in\{0,1,\dots,2^d-1\},
$$

则

$$
f=2^{-d}z.
$$

因此，从 $2^n$ 开始到 $2^{n+1}$ 之前，$\mathbb{F}$ 在该区间内给出恰好 $2^d$ 个等间距的数.

> **Note:** 我们可以把 $1+f$ 看作区间 $[1,2)$ 里的二进制有效数，而 $2^n$ 负责数量级缩放. 固定 $n$ 时，$[2^n,2^{n+1})$ 内的可表示数是等间距的；改变 $n$ 会把这些点整体放大或缩小，同时也放大或缩小间距.

**#2 机器精度与舍入**

为了把 $\mathbb{F}$ 具体化，我们先看一个 toy case.

> **Example:** A tiny floating-point set.
> Let $d=2$ and $n=0$. The elements of $\mathbb{F}$ in $[1,2)$ are
> $$
> 1+\frac{0}{4},\; 1+\frac{1}{4},\; 1+\frac{2}{4},\; 1+\frac{3}{4}.
> $$
> They are evenly spaced with spacing $1/4$.
> Taking $n=1$ doubles the values (numbers in $[2,4)$) and taking $n=-1$ halves them (numbers in $[1/2,1)$); the spacing scales in the same way.

在任意精度 $d$ 下，1 的下一个更大的浮点数是 $1+2^{-d}$. 我们把这段差值记为机器精度.

> **Definition:** **Machine epsilon.**
> The smallest element of $\mathbb{F}$ that is greater than $1$ is $1+2^{-d}$. The difference
> $$
> \epsilon_{\rm mach}=2^{-d}
> $$
> is called **machine epsilon**.

接下来我们定义舍入函数 $fl(\cdot)$：把实数映射到最近的浮点数.

> **Definition:** Rounding to nearest.
> Let $fl(x)$ map a real number $x$ to the nearest member of $\mathbb{F}$.

在区间 $[2^n,2^{n+1})$ 内，相邻浮点数的间距为 $2^n\epsilon_{\rm mach}$. 因此对任意 $x\in[2^n,2^{n+1})$，$x$ 到最近浮点数的距离至多是半个间距 (即 $\frac{1}{2} 2^n\epsilon_{\rm mach}$)，从而得到一个统一的相对误差界.

> **Observation:** Uniform relative precision.
> For $x\in[2^n,2^{n+1})$,
> $$
> \frac{|fl(x)-x|}{|x|}\le \frac{1}{2}\epsilon_{\rm mach}.
> $$
> Equivalently, there exists an $\epsilon$ such that
> $$
> fl(x)=x(1+\epsilon),\qquad |\epsilon|\le \frac{1}{2}\epsilon_{\rm mach}.
> $$

> **Note:** 上面的 $\epsilon$ 依赖于 $x$，但写公式时我们通常不显式写出这种依赖.

> **Note:** "machine epsilon"、"machine precision"、"unit roundoff" 的精确定义在不同资料中不完全一致，但这些差别对我们的目的通常影响不大.

**#3 Precision 与 accuracy**

如果我们先把前面的表示法暂时换成 10 进制写一遍，可能更直观：

$$
\pm\left(b_0+\sum_{i=1}^{d} b_i 10^{-i}\right)\times 10^n
=\pm(b_0.b_1b_2\cdots b_d)\times 10^n,
$$

其中每个 $b_i\in\{0,1,\dots,9\}$，并且 $b_0\ne 0$. 这就是我们熟悉的科学计数法. 此时我们说这个数有 $d+1$ 位有效数字 (significant digits).

接着我们用普朗克常数举例. 若普朗克常数写作 $6.626068\times 10^{-34}$ (m kg/sec)，我们说它有 7 位十进制 **precision**. 如果只把最后一位从 8 改成 9，相对变化大约为

$$
\frac{0.000001\times 10^{-34}}{6.626068\times 10^{-34}}
\approx 1.51\times 10^{-7}.
$$

因此，"7 位 **precision**" 的说法强调的是相对精度，这与说它精确到小数点后 40 位是两回事. 另一个好处是：相对 **precision** 不依赖于单位. 例如把普朗克常数改写成 eV sec，会得到 $4.135668\times 10^{-15}$，仍然是 7 位有效数字，但只有 21 位小数.

浮点数的 **precision** 也是类似的，只是计算机偏好用 2 进制. 浮点数的 **precision** 总是 $d$ 个二进制位，这对应了我们在前一节得到的统一相对误差界.

在这里我们需要提醒自己：很容易把 **precision** 和 **accuracy** 混淆，尤其是在看计算机输出时. 虽然每个结果都会用固定的 $d$ 个二进制位来表示，但并不是每一位都能准确反映我们真正想要的目标值.

> **Note:** **precision** 描述的是表示格式本身能携带多少位信息 (例如固定的 $d$ 个二进制位)，它更多是 "格式" 层面的. **accuracy** 描述的是近似 $\tilde{x}$ 与目标值 $x$ 的接近程度，它更多是 "结果" 层面的. 一个结果可以有很高的 **precision**，但由于问题本身或算法过程的误差，它的 **accuracy** 仍然可能很差.

设 $x$ 是我们关心的目标值，$\tilde{x}$ 是它的近似.

- **absolute accuracy：** $|\tilde{x}-x|$.
- **relative accuracy：** $\dfrac{|\tilde{x}-x|}{|x|}$.

绝对 **accuracy** 与 $x$ 具有相同量纲，而相对 **accuracy** 是无量纲的.

我们还可以把相对 **accuracy** 换算成 "准确位数"：十进制下可用

$$
-\log_{10}\left(\frac{|\tilde{x}-x|}{|x|}\right)
$$

来估计 **number of accurate digits**. 通常我们会把它向下取整得到整数位数，但也可以说 "almost seven digits" 或 "ten and a half digits" 这类更细的描述.

> **Demo:** Approximating $\pi$ by $22/7$.
> ```Python
> import math
>
> p = 22 / 7
> acc = abs(p - math.pi)
> print("absolute accuracy =", acc)
> print("relative accuracy =", acc / math.pi)
> print("accurate digits =", -math.log10(acc / math.pi))
> ```
> We get about 3.4 accurate decimal digits.

**#4 IEEE 754 双精度 (Double precision)**

现代数值计算通常遵循 **IEEE 754** 标准. 在该标准下，**单精度** 使用 23 位有效数二进制位，**双精度** 使用 52 位.

> **Observation:** **Double precision** machine epsilon.
> In **double precision**, $d=52$ and
> $$
> \epsilon_{\rm mach}=2^{-52}\approx 2.2\times 10^{-16}.
> $$

通常会说双精度大约有 16 位十进制有效数字. 双精度浮点数一共占 64 位：1 位符号位，11 位指数位，以及 52 位有效数位. 这三部分共同对应 $\pm(1+f)\times 2^n$ 这一结构.

> **Demo:** **Machine epsilon** in Python.
> ```Python
> import numpy as np
>
> eps = np.finfo(float).eps
> print(eps)
> ```
> This prints about `2.220446049250313e-16`.

> **Demo:** Types and bits in Python.
> ```Python
> import struct
>
> print(type(1), type(1.0))
>
> x = 1.0
> bits = struct.unpack(">Q", struct.pack(">d", x))[0]
> print(f"{bits:064b}")
> ```
> The first bit is the sign, the next 11 bits encode the exponent, and the remaining 52 bits encode the significand.

**#5 间距、范围、overflow / underflow**

需要强调：相邻浮点数的间距会随数量级缩放，这让相对精度在不同大小的数上保持大致一致.

> **Demo:** Spacing near a number.
> ```Python
> import numpy as np
>
> x = 161.8
> gap = np.nextafter(x, np.inf) - x
> print("spacing =", gap)
> print("nextfloat =", np.nextafter(x, np.inf))
> ```
> The spacing is on the order of `eps * x`.

一个常见误解是：$\epsilon_{\rm mach}$ 不是 "最小的正浮点数". 它只是 1 附近的刻度大小. 可表示数值范围主要由指数的取值范围决定.

> **Note:** 区分两件事：$\epsilon_{\rm mach}$ 关心的是相对精度，"最小正数" 关心的是绝对尺度. 当我们接近 0 时，必须转向绝对误差视角，因为任何有限的绝对误差相对于 0 的相对误差都是无穷大.

理论定义里指数 $n$ 没有限制，但双精度里它有上下界. 这导致过大结果会 **overflow** (得到 `inf`)，过小正数会 **underflow** (趋向于 0).

> **Demo:** Range, overflow, and underflow in Python.
> ```Python
> import numpy as np
>
> f = np.finfo(float)
> print("floatmin ~", f.tiny)
> print("floatmax ~", f.max)
> print("overflow example =", f.max * 2)
> print("underflow example =", f.tiny / 2)
> ```
> You may see `inf` for overflow and `0.0` for underflow.

> **Observation:** **NaN** stands for **Not a Number**. It is the result of an undefined arithmetic operation such as $0/0$.

> **Demo:** **NaN** in Python (via NumPy).
> ```Python
> import numpy as np
>
> print(np.divide(0.0, 0.0))
> ```
> This produces `nan`.

> **Note:** 还存在更小的非正规数 (denormalized / subnormal)，但它们精度更低，且初学阶段通常不需要用到这一层细节.

**#6 浮点运算与结合律**

下面从 "浮点运算" 的视角看：计算机运算以浮点数为输入并输出浮点数. 一个常用的近似模型是：每次基本运算都会引入一个受控的相对误差，其量级与 $\epsilon_{\rm mach}$ 同阶.

> **Observation:** A simple arithmetic error model.
> For an elementary operation, a common model is that the computed result equals the exact result times $(1+\delta)$ with $|\delta|$ bounded on the order of $\epsilon_{\rm mach}$.
> For floating-point addition $\oplus$ with $x,y\in\mathbb{F}$, one writes a bound of the form
> $$
> \frac{|(x\oplus y)-(x+y)|}{|x+y|}\le \epsilon_{\rm mach}.
> $$

下面这个演示说明：即使只涉及加法，结合律也可能失效.

> **Demo:** Loss of associativity in floating point.
> ```Python
> import numpy as np
>
> eps = np.finfo(float).eps
> e = eps / 2
> print((1.0 + e) - 1.0)
> print(1.0 + (e - 1.0))
> ```
> The first line prints `0.0`, while the second prints about `1.1102230246251565e-16`.

> **Observation:** Equivalent expressions may differ.
> We should not expect two mathematically equivalent results to be exactly equal when computed in floating point; we only expect them to be relatively close.

> **Note:** 上面的 Demo 说明：在浮点运算里，即使两个表达式在数学上等价，它们的计算路径也可能因为舍入而产生不同结果. 而 Observation 我们做数值计算的人信奉的：我们通常只期待结果在合理的相对误差范围内接近，而不是逐位相等.
