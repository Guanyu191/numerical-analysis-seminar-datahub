# 4-7-非线性最小二乘 (Nonlinear least squares)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 从线性到非线性：过定约束的最小二乘**

在解完方阵线性系统 $\mathbf{A}\mathbf{x}=\mathbf{b}$ 之后，我们把问题推广到 "约束多于变量" 的线性最小二乘. 现在我们把同样的推广做一次：从非线性方程组 $\mathbf{f}(\mathbf{x})=\mathbf{0}$ 推广到非线性最小二乘.

$$
\begin{array}{ccc}
\hline
 & \text{linear} & \text{nonlinear} \\
\hline
\text{square} & \mathbf{A}\mathbf{x}=\mathbf{b} & \mathbf{f}(\mathbf{x})=\mathbf{0} \\
\text{overdetermined} & \min \|\mathbf{A}\mathbf{x}-\mathbf{b}\|_2 & \min \|\mathbf{f}(\mathbf{x})\|_2 \\
\hline
\end{array}
$$

> **Definition:** Nonlinear least-squares problem
> Given a function $\mathbf{f}(\mathbf{x})$ mapping from $\mathbb{R}^n$ to $\mathbb{R}^m$, the nonlinear least-squares problem is to find $\mathbf{x}\in\mathbb{R}^n$ such that $\|\mathbf{f}(\mathbf{x})\|_2$ is minimized.

与线性最小二乘一样，我们只考虑过定情形 $m>n$. 此外，最小化一个非负量等价于最小化它的平方，因此也常写成最小化标量函数

$$
\phi(\mathbf{x})=\mathbf{f}(\mathbf{x})^{T}\mathbf{f}(\mathbf{x})=\|\mathbf{f}(\mathbf{x})\|_2^2.
$$

**#2 Gauss-Newton 方法：把非线性问题变成一串线性最小二乘**

不动点迭代与 Newton 方法的套路在这里仍然成立：我们用线性模型替代非线性函数.

在当前估计 $\mathbf{x}_k$ 处，定义线性模型

$$
\mathbf{q}(\mathbf{x})
=
\mathbf{f}(\mathbf{x}_k)+\mathbf{A}_k(\mathbf{x}-\mathbf{x}_k),
$$

其中 $\mathbf{A}_k$ 可以取精确 Jacobian $\mathbf{J}(\mathbf{x}_k)$，也可以取近似 Jacobian (见 **4-6-拟 Newton 方法**).

记 $\mathbf{y}_k=\mathbf{f}(\mathbf{x}_k)$.

在方阵情形 $m=n$ 时，我们通过解线性系统 $\mathbf{A}_k\mathbf{s}_k=-\mathbf{y}_k$ 得到步长 $\mathbf{s}_k=\mathbf{x}_{k+1}-\mathbf{x}_k$. 但在 $m>n$ 时，我们通常无法让 $\mathbf{q}=0$ 精确成立，因此转而选择使残差最小的步长：

$$
\mathbf{s}_k=\arg\min_{\mathbf{s}}\ \|\mathbf{A}_k\mathbf{s}+\mathbf{y}_k\|_2.
$$

然后令 $\mathbf{x}_{k+1}=\mathbf{x}_k+\mathbf{s}_k$.

> **Algorithm:** Gauss-Newton method
> Given $\mathbf{f}$ and a starting value $\mathbf{x}_1$, for each $k=1,2,3,\dots$:
> 1. Compute $\mathbf{y}_k=\mathbf{f}(\mathbf{x}_k)$ and $\mathbf{A}_k$, the exact or approximate Jacobian matrix at $\mathbf{x}_k$.
> 2. Solve the linear least squares problem $\arg\min_{\mathbf{s}}\ \|\mathbf{A}_k\mathbf{s}+\mathbf{y}_k\|_2$ for $\mathbf{s}_k$.
> 3. Let $\mathbf{x}_{k+1}=\mathbf{x}_k+\mathbf{s}_k$.

简言之，Gauss-Newton 是用一串线性最小二乘子问题来解非线性最小二乘.

> **Note:** 从实现角度看，如果我们已经写过 **4-5-非线性方程组的 Newton 法** 的 `newtonsys`，只要把每一步的线性系统求解 `np.linalg.solve(J, -y)` 改成最小二乘求解 `np.linalg.lstsq(J, -y)`，就得到了 Gauss-Newton. 同理，**4-6-拟 Newton 方法** 的 Levenberg 线性系统本身就是在做 (阻尼的) Gauss-Newton，因此它在 $m>n$ 时也仍然适用.

**#3 收敛现象：残差不为零时可能从二次转为线性**

在非线性方程组的 Newton 方法里，我们通常期待在典型情形下出现二次收敛. 对 Gauss-Newton 来说，收敛性会更复杂：最小二乘问题的最优点不一定满足 $\mathbf{f}(\mathbf{x})=\mathbf{0}$.

设最小化后得到的最小残差为 $\|\mathbf{f}(\mathbf{x}_*)\|_2=R>0$. 在不少问题中，我们会观察到一种两阶段现象：

- 当迭代点距离真最优解还较远时，看起来有点像二次收敛.
- 当迭代点进入 $\sim R$ 的尺度后，收敛会变成线性，并且若 $R$ 不够小，后期可能相当慢 (甚至出现明显停滞段).

> **Demo:** Convergence of Gauss-Newton under different residual levels
> We perturb a function with a known zero-residual minimizer to create cases with different minimum residuals $R$, and observe the convergence behavior.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def g(x):
>     x1, x2 = x
>     return np.array([
>         np.sin(x1 + x2),
>         np.cos(x1 - x2),
>         np.exp(x1 - x2),
>     ])
>
> def Jg(x):
>     x1, x2 = x
>     return np.array([
>         [np.cos(x1 + x2), np.cos(x1 + x2)],
>         [-np.sin(x1 - x2),  np.sin(x1 - x2)],
>         [np.exp(x1 - x2),  -np.exp(x1 - x2)],
>     ])
>
> def gauss_newton(f, J, x1, *, maxiter=40, ftol=1e-12, xtol=1e-12):
>     x = np.asarray(x1, dtype=float)
>     xs = [x.copy()]
>     for _ in range(maxiter):
>         y = np.asarray(f(x), dtype=float)
>         A = np.asarray(J(x), dtype=float)
>         s, *_ = np.linalg.lstsq(A, -y, rcond=None)
>         x_new = x + s
>         xs.append(x_new.copy())
>         if np.linalg.norm(s) <= xtol or np.linalg.norm(y) <= ftol:
>             break
>         x = x_new
>     return xs
>
> p = np.array([1.0, 1.0])
> v = np.array([-1.0, 1.0, -1.0])
> v = v / np.linalg.norm(v)
>
> plt.figure()
> plt.xlabel("iteration")
> plt.ylabel("error")
> plt.yscale("log")
> plt.title("Convergence of Gauss-Newton")
>
> for R in [1e-3, 1e-2, 1e-1]:
>     def f(x, R=R):
>         return g(x) - g(p) + R * v
>
>     xs = gauss_newton(f, Jg, x1=[0.0, 0.0], maxiter=40, ftol=1e-14, xtol=1e-14)
>     r = xs[-1]
>     err = [np.linalg.norm(x - r) for x in xs[:-1]]
>     normres = np.linalg.norm(f(r))
>     plt.plot(err, marker="o", label=f"R={normres:.2g}")
>
> plt.grid(True, which="both", alpha=0.3)
> plt.legend()
> plt.show()
> ```
> Smaller minimum residuals tend to allow a longer "quadratic-like" phase; larger residuals can lead to slow, nearly linear convergence.

**#4 用非线性最小二乘做数据拟合**

在 **3-1-用函数拟合数据** 里，我们讨论了当模型对未知系数是线性依赖时，如何做最小二乘拟合. 现在我们可以把这个过程推广到参数以任意方式进入模型的情形.

假设给定数据点 $(t_i,y_i)$，$i=1,\dots,m$. 我们用一个依赖参数 $\mathbf{x}\in\mathbb{R}^n$ 的模型函数 $g(t,\mathbf{x})$ 去拟合数据. 标准做法是定义 **misfit** 向量

$$
\mathbf{f}(\mathbf{x})
=
\bigl[g(t_i,\mathbf{x})-y_i\bigr]_{i=1,\dots,m},
$$

然后最小化 $\|\mathbf{f}(\mathbf{x})\|_2^2$.

如果我们想提供显式 Jacobian，那么矩阵元素为

$$
J_{ij}(\mathbf{x})=\frac{\partial}{\partial x_j}g(t_i,\mathbf{x}),\qquad i=1,\dots,m,\ j=1,\dots,n.
$$

当 $g$ 对参数是线性形式 (例如 $g(t,\mathbf{c})=\sum_j c_j g_j(t)$) 时，misfit 也对 $\mathbf{c}$ 线性，从而退化回线性最小二乘.

> **Demo:** Michaelis-Menten data fitting
> We generate synthetic data from a Michaelis-Menten model and fit the parameters by nonlinear least squares.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def gauss_newton(f, J, x1, *, maxiter=50, ftol=1e-12, xtol=1e-12):
>     x = np.asarray(x1, dtype=float)
>     for _ in range(maxiter):
>         y = np.asarray(f(x), dtype=float)
>         A = np.asarray(J(x), dtype=float)
>         s, *_ = np.linalg.lstsq(A, -y, rcond=None)
>         x_new = x + s
>         if np.linalg.norm(s) <= xtol or np.linalg.norm(y) <= ftol:
>             x = x_new
>             break
>         x = x_new
>     return x
>
> # Synthetic data.
> m = 25
> s = np.linspace(0.05, 6.0, m)
> V_true, Km_true = 2.0, 0.5
> w_hat = V_true * s / (Km_true + s)
> w = w_hat + 0.15 * np.cos(2.0 * np.exp(s / 16.0) * s)  # smooth noise
>
> plt.scatter(s, w, label="noisy data")
> plt.plot(s, w_hat, ls="--", label="noise-free curve")
>
> # Nonlinear fit.
> def misfit(x):
>     V, Km = x
>     return V * s / (Km + s) - w
>
> def misfit_jac(x):
>     V, Km = x
>     J = np.zeros((m, 2))
>     J[:, 0] = s / (Km + s)            # d/dV
>     J[:, 1] = -V * s / (Km + s)**2    # d/dKm
>     return J
>
> x0 = np.array([1.0, 0.75])
> V_fit, Km_fit = gauss_newton(misfit, misfit_jac, x0)
> print("nonlinear fit:", V_fit, Km_fit)
>
> model = lambda z: V_fit * z / (Km_fit + z)
> plt.plot(s, model(s), label="nonlinear fit")
>
> # Linearized fit via 1/w = alpha/s + beta.
> A = np.column_stack([1.0 / s, np.ones_like(s)])
> u = 1.0 / w
> alpha, beta = np.linalg.lstsq(A, u, rcond=None)[0]
> linmodel = lambda z: 1.0 / (beta + alpha / z)
> plt.plot(s, linmodel(s), label="linearized fit")
>
> plt.xlabel("s")
> plt.ylabel("v")
> plt.legend(loc="lower right")
> plt.grid(True, alpha=0.3)
> plt.show()
> ```
> The nonlinear fit directly minimizes the residual in the original measured quantity, while the linearized approach optimizes a transformed residual and can produce a different result.
