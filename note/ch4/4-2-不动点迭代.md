# 4-2-不动点迭代 (Fixed-point iteration)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 不动点问题与求根问题的互换**

除了直接解 $f(x)=0$ (见 **4-1-求根问题**)，我们还会用一种等价形式：**fixed-point problem**.

> **Definition:** Fixed-point problem
> Given a function $g$, the fixed-point problem is to find a value $p$, called a fixed point, such that $g(p)=p$.

求根问题与不动点问题可以互相转化. 例如给定求根函数 $f$，我们可以定义

$$
g(x)=x-f(x),
$$

则 $f(r)=0$ 与 $g(r)=r$ 等价. 这个转化并不唯一：对任意常数 $c$，也可以取

$$
g(x)=x+cf(x).
$$

反过来，若我们从 $g$ 出发，也能定义

$$
f(x)=x-g(x),
$$

此时 $g(p)=p$ 与 $f(p)=0$ 仍然等价.

**#2 不动点迭代：一个极简的迭代算法**

对于任意给定的 $g$，我们可以用一种非常直接的方法尝试寻找不动点：从一个初值出发，不断把它代入 $g$.

> **Algorithm:** Fixed-point iteration
> Given a function $g$ and an initial value $x_1$, define
> $$
> x_{k+1}=g(x_k),\qquad k=1,2,\dots.
> $$

这是我们遇到的第一个典型例子：即使在精确算术下，它也不会在有限步内 "到达" 答案. 我们只能生成一个序列，并在某个时刻因为 "看起来已经足够接近极限" 而停止.

**#3 用图像理解收敛与发散**

把不动点问题画成图像，会很直观：不动点满足 $y=g(x)$ 与 $y=x$ 的交点条件. 不动点迭代对应一种常见的 "蛛网图 (cobweb plot)" 过程：在两条曲线之间交替做水平/竖直移动.

> **Demo:** Finding a fixed point
> Let's convert the roots of a quadratic polynomial $f(x)$ to a fixed-point problem. Define $g(x)=x-f(x)$. Intersections of $y=g(x)$ with the line $y=x$ are fixed points of $g$ and thus roots of $f$.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def f(x):
>     return x**2 - 4*x + 3.5
>
> def g(x):
>     return x - f(x)  # = -x**2 + 5*x - 3.5
>
> # True roots of f(x)=0 (also fixed points of g).
> r = np.roots([1.0, -4.0, 3.5])
> rmin, rmax = np.min(r), np.max(r)
> print("roots:", rmin, rmax)
>
> def cobweb(g, x0, n, xmin, xmax, title):
>     xs = np.linspace(xmin, xmax, 600)
>     plt.figure()
>     plt.plot(xs, g(xs), lw=2, label="y=g(x)")
>     plt.plot(xs, xs, lw=2, label="y=x")
>
>     x = float(x0)
>     for _ in range(n):
>         y = float(g(x))
>         # (x, x) -> (x, g(x)) -> (g(x), g(x))
>         plt.plot([x, x], [x, y], color="C3", lw=1)
>         plt.plot([x, y], [y, y], color="C2", lw=1)
>         x = y
>
>     plt.gca().set_aspect("equal", adjustable="box")
>     plt.xlabel("x")
>     plt.ylabel("y")
>     plt.title(title)
>     plt.legend(loc="lower right")
>     plt.grid(True, alpha=0.3)
>     plt.show()
>     return x
>
> x_last = cobweb(g, x0=2.1, n=6, xmin=2.0, xmax=3.0, title="Finding a fixed point")
> print("last iterate:", x_last)
> print("relative error to rmax:", abs(x_last - rmax) / abs(rmax))
>
> x_last = cobweb(g, x0=1.3, n=6, xmin=1.0, xmax=2.0, title="Divergence")
> print("last iterate:", x_last)
> print("relative error to rmin:", abs(x_last - rmin) / abs(rmin))
> ```
> Starting from $x_0=2.1$, the iteration moves rapidly toward the fixed point near $r_{\max}$. Starting from $x_0=1.3$, the iteration is pushed away from the fixed point near $r_{\min}$.

**#4 级数分析：局部收敛判据 $|g'(p)|<1$**

上面的 Demo 里，两次迭代的区别只是初值不同：一个序列收敛到某个不动点，另一个却发散. 最直接的解释方式是对 $g$ 在不动点附近做 Taylor 展开.

假设我们希望迭代 $x_{k+1}=g(x_k)$ 收敛到不动点 $p$. 令误差为

$$
\epsilon_k=x_k-p,
$$

则 $x_k=p+\epsilon_k$. 把它代回迭代式，并在 $p$ 处展开 (假设 $g$ 至少二阶连续可导)：

$$
\epsilon_{k+1}+p
=
g(p+\epsilon_k)
=
g(p)+g'(p)\epsilon_k+\frac{1}{2}g''(p)\epsilon_k^2+\cdots.
$$

由于 $g(p)=p$，我们得到

$$
\epsilon_{k+1}=g'(p)\epsilon_k+O(\epsilon_k^2).
$$

若迭代确实收敛到 $p$，则 $\epsilon_k\to 0$，于是二阶项可以忽略，从而

$$
\epsilon_{k+1}\approx g'(p)\epsilon_k.
$$

这与收敛相容的情形是 $|g'(p)|<1$；若 $|g'(p)|>1$，则误差会被放大，因而不应期待收敛.

> **Observation:** Local convergence of fixed-point iteration
> Fixed point iteration for a differentiable $g(x)$ converges to a fixed point $p$ if the initial error is sufficiently small and $|g'(p)|<1$. The iteration diverges for all initial values if $|g'(p)|>1$.

> **Note:** 这里的 "diverges for all initial values" 更准确地说是 "不会收敛到这个 fixed point p" (除非初值恰好等于 $p$). 它并不意味着序列一定跑到无穷大，也可能跑向其他吸引的不动点，或进入周期.

> **Example:** Interpreting the Demo via $g'(p)$
> In the Demo above, $g(x)=x-f(x)=-x^2+5x-3.5$, so $g'(x)=-2x+5$.
> For the fixed point near $p\approx 2.71$, $g'(p)\approx -0.42$, which predicts convergence.
> For the fixed point near $p\approx 1.29$, $g'(p)\approx 2.42$, which predicts divergence.

**#5 线性收敛：误差按常数比例衰减**

在计算中，我们通常不仅关心 "是否收敛"，还关心 "收敛得有多快". 在不动点迭代的可收敛情形下，上面的级数分析给出一个预测：

$$
|\epsilon_{k+1}|\approx \sigma|\epsilon_k|,\qquad \sigma=|g'(p)|<1.
$$

这对应一种经典收敛类型：线性收敛.

> **Definition:** Linear convergence
> Suppose a sequence $x_k$ approaches limit $x^*$. If the error sequence $\epsilon_k=x_k-x^*$ satisfies
> $$
> \lim_{k\to\infty}\frac{|\epsilon_{k+1}|}{|\epsilon_k|}=\sigma<1,
> $$
> then the sequence displays linear convergence. The number $\sigma$ is called the convergence rate.

如果进一步假设比值恒等于 $\sigma$ (即 "完美线性收敛" )，则 $|\epsilon_k|=C\sigma^k$. 取对数得到

$$
\log|\epsilon_k|=k(\log\sigma)+\log C,
$$

因此在 log-linear 坐标中，误差应接近一条直线，其斜率为 $\log\sigma$.

> **Observation:** Linear convergence in practice
> Linear convergence is marked by an approximate reduction of the error at each iteration by a constant factor, the convergence rate $\sigma$. When graphed on a log-linear scale, the errors lie on a straight line whose slope is the log of the convergence rate. Both phenomena manifest most strongly at the latest iterations.

> **Demo:** Estimating the convergence rate
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def f(x):
>     return x**2 - 4*x + 3.5
>
> def g(x):
>     return x - f(x)
>
> # Fixed point we converge to.
> r = np.roots([1.0, -4.0, 3.5])
> rmax = np.max(r)
>
> # Fixed point iteration.
> x = [2.1]
> for _ in range(12):
>     x.append(g(x[-1]))
> x = np.array(x, dtype=float)
>
> err = np.abs(x - rmax)
>
> plt.figure()
> plt.semilogy(range(len(err)), err, marker="o")
> plt.xlabel("iteration number")
> plt.ylabel("error")
> plt.title("Convergence of fixed point iteration")
> plt.grid(True, which="both", alpha=0.3)
> plt.show()
>
> # Fit a line to log(err) at later iterations to estimate log(sigma).
> k = np.arange(len(err))
> k_fit = k[5:]          # discard early iterations (non-asymptotic)
> y_fit = np.log(err[5:])
> slope, intercept = np.polyfit(k_fit, y_fit, 1)
> sigma = np.exp(slope)
> print("estimated sigma:", sigma)
>
> ratios = err[8:12] / err[7:11]
> print("ratios err[k+1]/err[k] (late):", ratios)
> ```
> The fitted slope approximates $\log\sigma$, hence $\sigma=\exp(\text{slope})$. Late ratios $\mathrm{err}_{k+1}/\mathrm{err}_k$ should be close to $\sigma$.

**#6 压缩映射：从局部判据到全局保证**

级数分析得到的 $|g'(p)|<1$ 是一个局部条件：它以 $p$ 附近的导数为核心. 更一般的形式是 Lipschitz 条件.

> **Definition:** Lipschitz condition
> A function $g$ is said to satisfy a Lipschitz condition with constant $L$ on the interval $S\subset\mathbb{R}$ if, for all $s,t\in S$,
> $$
> |g(s)-g(t)|\le L|s-t|.
> $$

若 $L<1$，则 $g$ 会把距离整体缩小，因此称为 **contraction mapping**. 在这种情形下，不动点问题有一个非常强的结构性结论：不动点存在且唯一，并且迭代误差有显式上界.

> **Theorem:** Contraction mapping
> Suppose that $g$ satisfies a Lipschitz condition with $L<1$ on an interval $S$. Then $S$ contains exactly one fixed point $p$ of $g$.
> If $x_1,x_2,\dots$ are generated by fixed point iteration and all lie in $S$, then
> $$
> |x_k-p|\le L^{k-1}|x_1-p|,\qquad k>1.
> $$
> **Proof:** Since $p$ is a fixed point, $|x_{k+1}-p|=|g(x_k)-g(p)|\le L|x_k-p|$. Repeating gives the bound.
> Uniqueness follows because if $p,q$ are fixed points then $|p-q|=|g(p)-g(q)|\le L|p-q|$, and $L<1$ forces $p=q$.

从微积分基本定理可以推出一种常用的验证方式：若 $g$ 可导且在区间 $S$ 上满足 $|g'(x)|\le L$，那么对任意 $s,t\in S$，

$$
g(s)-g(t)=\int_t^s g'(x)\,dx,
$$

从而得到 $|g(s)-g(t)|\le L|s-t|$，即 Lipschitz 条件成立. 因而我们也有一个更便捷的推论.

> **Corollary:** Derivative test for contraction
> If $|g'(x)|\le L<1$ for all $x$ in an interval $S$, then the conclusions of the contraction mapping theorem apply.

虽然还存在更强、更一般的版本 (例如只要求初值足够接近不动点即可收敛)，但从算法角度看，不动点迭代的核心优点就是实现极其简单. 代价是：它通常不是最快的选择.

我们在计算中只能生成无限序列的一个有限前缀. 从逻辑上说，这个有限样本本身并不能保证收敛或发散的结论. 但在实际数值计算里，我们通常会把序列中已经形成的稳定趋势当作后续行为的依据，并尽可能用理论去支撑这些经验判断.
