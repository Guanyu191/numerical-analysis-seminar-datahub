# 4-4-基于插值的方法 (Interpolation-based methods)

这是一份数值计算学习笔记，参考了 Tobin A. Driscoll and Richard J. Braun 的教材 [*Fundamentals of Numerical Computation* (2023)](https://tobydriscoll.net/fnc-julia/home.html).

> 这份笔记主要是翻译了原文内容，并删改或重新表述部分内容，希望能进一步减少初学者的学习障碍.

**#1 从 Newton 到插值: approximate approximation 原则**

从实用角度看，Newton 方法的一个主要缺点是：它要求我们提供 $f'$ (见 **4-3-Newton-方法** 里的 `newton` 实现). 这既是编程上的不便，也会带来额外的计算开销.

但我们可以通过一个很简单、却很容易被忽略的观察来绕开 $f'$：

> **Observation:** Principle of approximate approximation
> When a step produces an approximate result, you are free to carry it out approximately.

在 Newton 的语境中，$f'$ 的作用是构造切线对应的线性近似 $q(x)$，并把 $q$ 的根作为下一次迭代的 $x_{k+1}$. 如果我们愿意用 "近似的线性模型" 来代替切线，那么就可以避免直接计算 $f'$.

**#2 割线法：用两点确定一条直线**

最自然的替代方式是：不用切线，而用两点连线 (secant line) 来构造线性模型. 给定两个近似点 $(x_1,f(x_1))$ 与 $(x_2,f(x_2))$，过这两点的直线就是割线. 我们仍然用这条直线的根作为新的近似.

> **Demo:** Two initial values and a secant line
> We return to $f(x)=x e^x-2$. From a plot, there is a root in $[0.5,1]$, so we take the endpoints as two initial approximations.
>
> ```Python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def f(x):
>     return x * np.exp(x) - 2.0
>
> xs = np.linspace(0.25, 1.25, 600)
> plt.plot(xs, f(xs), label="function")
> plt.axhline(0.0, color="k", lw=1)
> plt.grid(True, axis="y", alpha=0.3)
> plt.xlabel("x")
> plt.ylabel("y")
> plt.legend(loc="upper left")
> plt.title("Two initial values")
>
> x1, x2 = 1.0, 0.5
> y1, y2 = f(x1), f(x2)
> plt.scatter([x1, x2], [y1, y2], label="initial points")
>
> m2 = (y2 - y1) / (x2 - x1)          # slope of secant line
> secant = lambda x: y2 + m2 * (x - x2)
> plt.plot(xs, secant(xs), ls="--", color="k", label="secant line")
>
> x3 = x2 - y2 / m2                   # root of secant line
> print("x3 =", x3, "f(x3) =", f(x3))
> plt.scatter([x3], [0.0], label="root of secant")
> plt.legend()
> plt.show()
> ```
> For the next step, use the secant line through the two most recent points and repeat.

把过程写成迭代公式：若我们用最近两次近似 $x_{k-1},x_k$ 来做线性插值，则线性模型可以写为

$$
q(x)=f(x_k)+\frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}(x-x_k),
$$

令 $q(x_{k+1})=0$ 得到割线迭代.

> **Algorithm:** Secant iteration
> Given a function $f$ and two initial values $x_1$ and $x_2$, define
> $$
> x_{k+1}=x_k-\frac{f(x_k)(x_k-x_{k-1})}{f(x_k)-f(x_{k-1})},\qquad k=2,3,\dots.
> $$

实现上，割线法与 Newton 一样需要停止准则与最大迭代次数；但它有一个明显优势：每一步只需要计算一个新的 $f(x_k)$，而前一步的函数值可以复用.

> **Function:** secant
> **Secant method for scalar rootfinding**
> ```Python
> import numpy as np
>
> def secant(f, x1, x2, *, maxiter=40, ftol=None, xtol=None):
>     """
>     secant(f, x1, x2, *, maxiter=40, ftol=..., xtol=...)
>
>     Use the secant method to find a root of f starting from x1 and x2.
>     Returns an array of all iterates.
>     """
>     eps = np.finfo(float).eps
>     if ftol is None:
>         ftol = 100 * eps
>     if xtol is None:
>         xtol = 100 * eps
>
>     x = [float(x1), float(x2)]
>     y_prev = float(f(x[0]))
>     dx = np.inf
>     y = np.inf
>
>     k = 1  # current index in x
>     while (abs(dx) > xtol) and (abs(y) > ftol):
>         if k >= maxiter:
>             print("Warning: Maximum number of iterations reached.")
>             break
>
>         y = float(f(x[k]))
>         dx = -y * (x[k] - x[k-1]) / (y - y_prev)   # secant step
>         x.append(x[k] + dx)
>         y_prev = y
>         k += 1
>
>     return np.array(x)
> ```

**#3 收敛性：超线性与黄金分割比**

从几何直觉看，割线通常比切线更不准确，因此我们会担心收敛变慢. 事实确实如此：割线法的收敛阶严格介于线性与二次之间.

设 $r$ 是 simple root，并令误差 $\epsilon_k=x_k-r$. 当初始误差足够小时，可以通过 Taylor 展开得到 (最低阶意义下)

$$
\epsilon_{k+1}\approx \frac{1}{2}\frac{f''(r)}{f'(r)}\epsilon_k\epsilon_{k-1}.
$$

如果我们大胆假设误差满足某种幂律形式 $\epsilon_{k+1}=c(\epsilon_k)^\alpha$，则可以推出幂指数需要满足

$$
\alpha^2=\alpha+1.
$$

它唯一的正根是黄金分割比

$$
\alpha=\frac{1+\sqrt{5}}{2}\approx 1.618,
$$

因此割线法的误差收敛速度大致像 $|\epsilon_{k+1}|\approx c|\epsilon_k|^\alpha$，其中 $1<\alpha<2$.

> **Definition:** Superlinear convergence
> Suppose a sequence $x_k$ approaches limit $x^*$. If the error sequence $\epsilon_k=x_k-x^*$ satisfies
> $$
> \lim_{k\to\infty}\frac{|\epsilon_{k+1}|}{|\epsilon_k|^\alpha}=L
> $$
> for constants $\alpha>1$ and $L>0$, then the sequence has superlinear convergence with rate $\alpha$.

与 **4-3-Newton-方法** 中的二次收敛类似，我们也可以用一个 log 比值来估计 $\alpha$：

$$
\frac{\log|\epsilon_{k+1}|}{\log|\epsilon_k|}\to \alpha,\qquad (k\to\infty).
$$

> **Demo:** Estimating the secant convergence rate
> We use extended precision to generate a longer secant sequence and estimate $\alpha$ from the log ratios.
>
> ```Python
> import mpmath as mp
>
> mp.mp.dps = 80
>
> def f(x):
>     return x * mp.e**x - 2
>
> # Extended-precision secant iteration.
> x_prev = mp.mpf("1.0")
> x_curr = mp.mpf("0.5")
> xs = [x_prev, x_curr]
>
> y_prev = f(x_prev)
> dx = mp.inf
> y = mp.inf
>
> for _ in range(40):
>     y = f(x_curr)
>     dx = -y * (x_curr - x_prev) / (y - y_prev)
>     x_next = x_curr + dx
>     xs.append(x_next)
>     if abs(dx) < mp.mpf("1e-70") or abs(y) < mp.mpf("1e-70"):
>         break
>     x_prev, x_curr = x_curr, x_next
>     y_prev = y
>
> r = xs[-1]  # proxy for the exact root
> err = [abs(r - xk) for xk in xs[:-1]]
> logerr = [mp.log(e) for e in err]
> ratios = [logerr[i+1] / logerr[i] for i in range(len(logerr) - 1)]
>
> print("log(err[k+1])/log(err[k]) ratios:")
> for q in ratios:
>     print(q)
> ```
> The ratios should settle around $\alpha\approx 1.618$.

讨论收敛阶时，把一次函数评估看作主要成本也很常见: Newton 每次迭代需要 $f(x_k)$ 与 $f'(x_k)$ 两次评估；割线法每次迭代只需要一个新的 $f(x_k)$ (另一个沿用旧值). 从这个角度看，割线法不仅更容易用，也可能更高效.

> **Observation:** Convergence per function evaluation
> If function evaluations are used to measure computational work, the secant iteration converges more rapidly than Newton's method.

**#4 逆向二次插值：用三点但避免多根歧义**

割线法每次只用最近两点. 一个自然问题是：迭代进行时，我们为什么不多用一些旧点？如果用三点做前向插值，我们会得到一个二次多项式，但抛物线可能与 $x$ 轴没有交点、或有两个交点，从而让 "下一步取哪个根" 变得含糊.

一个关键技巧是把抛物线 "侧过来"：把 $x$ 看作 $y=f(x)$ 的函数，用点 $(y_i,x_i)$ 去插值. 这样 $y=0$ 通常对应唯一的 $x$，就能自然定义下一次根近似.

设 $y_i=f(x_i)$. 定义 $q(y)$ 为通过三点 $(y_{k-2},x_{k-2})$、$(y_{k-1},x_{k-1})$、$(y_k,x_k)$ 的二次插值多项式，并令

$$
x_{k+1}=q(0).
$$

这个过程称为 **inverse quadratic interpolation** (IQI).

> **Observation:** Inverse quadratic interpolation (IQI)
> Let $y_i=f(x_i)$. Define $q(y)$ as the quadratic interpolant through $(y_{k-2},x_{k-2})$, $(y_{k-1},x_{k-1})$, and $(y_k,x_k)$, and set $x_{k+1}=q(0)$.

> **Demo:** Inverse quadratic interpolation
> We look for a root of $f(x)=x+\cos(10x)$ that is close to 1, starting from three initial values.
>
> ```Python
> import mpmath as mp
>
> mp.mp.dps = 80
>
> def f(x):
>     return x + mp.cos(10 * x)
>
> def iqi_step(x0, x1, x2):
>     y0, y1, y2 = f(x0), f(x1), f(x2)
>     # Lagrange interpolation for x = q(y) evaluated at y=0.
>     L0 = (0 - y1) * (0 - y2) / ((y0 - y1) * (y0 - y2))
>     L1 = (0 - y0) * (0 - y2) / ((y1 - y0) * (y1 - y2))
>     L2 = (0 - y0) * (0 - y1) / ((y2 - y0) * (y2 - y1))
>     return x0 * L0 + x1 * L1 + x2 * L2
>
> xs = [mp.mpf("0.8"), mp.mpf("1.2"), mp.mpf("1.0")]
> ys = [f(x) for x in xs]
>
> for _ in range(10):
>     x_next = iqi_step(xs[-3], xs[-2], xs[-1])
>     xs.append(x_next)
>     ys.append(f(x_next))
>
> print("residual =", ys[-1])
> r = xs[-1]
> err = [abs(r - xk) for xk in xs[:-1]]
> logerr = [mp.log(e) for e in err]
> ratios = [logerr[i+1] / logerr[i] for i in range(len(logerr) - 1)]
> print("log(err[k+1])/log(err[k]) ratios:")
> for q in ratios[-6:]:
>     print(q)
> ```
> The log ratios suggest a superlinear convergence rate, often higher than the secant method.

**#5 夹逼区间与 Brent 方法**

Newton、割线法、IQI 都不能保证收敛. 要得到 (几乎) 万无一失的算法，还需要一个新想法: **bracketing**.

若 $f$ 在区间 $[a,b]$ 上连续且 $f(a)f(b)<0$，则由介值定理可知 $f$ 在区间内至少有一个根. 当我们得到一个新的根估计 $c\in(a,b)$ 时，只要看 $f(c)$ 的符号，就能确定 $[a,c]$ 与 $[c,b]$ 中必有一个子区间仍然满足端点异号，从而仍然包含一个根. 因此我们可以维护一个始终包含根的区间，而不只是维护单个点.

实践中常见的思路是：把 "快速但不保收敛" 的方法与 "保证不丢根的 **bracket**" 结合起来. 例如从一个 **bracketing interval** 开始，每轮同时计算三个候选点: IQI 估计、割线估计、以及区间中点；按这个顺序挑第一个落在当前区间内部的候选点；然后用它更新 **bracketing subinterval**，再开始下一轮. 这就是 Brent 方法背后的核心思想，它在工程上非常成功.

> **Note:** 在 Python 里，如果我们能提供满足 $f(a)f(b)<0$ 的区间 (bracket)，通常可以直接用 `scipy.optimize.root_scalar(f, bracket=(a, b), method="brentq")` 来实现 Brent 思路的求根.
